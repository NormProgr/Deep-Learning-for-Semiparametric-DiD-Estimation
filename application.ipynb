{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\n",
    "    r\"src\\deep_learning_for_semiparametric_did_estimation\\data\\injury.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = data[data[\"ky\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_filtered = df_filtered.rename(\n",
    "    columns={\"durat\": \"duration\", \"ldurat\": \"log_duration\", \"afchnge\": \"after_1980\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df_plot = df_filtered\n",
    "\n",
    "df_plot[\"highearn\"] = df_plot[\"highearn\"].map(\n",
    "    {0: \"Low earner\", 1: \"High earner\"},\n",
    ")\n",
    "df_plot[\"after_1980\"] = df_plot[\"after_1980\"].map(\n",
    "    {0: \"Before 1980\", 1: \"After 1980\"},\n",
    ")\n",
    "\n",
    "# Plotting\n",
    "\n",
    "g = sns.FacetGrid(df_plot, col=\"highearn\", height=4, aspect=1)\n",
    "g.map_dataframe(\n",
    "    sns.histplot,\n",
    "    x=\"log_duration\",\n",
    "    binwidth=0.5,\n",
    "    color=\"grey\",\n",
    "    edgecolor=\"white\",\n",
    ")\n",
    "\n",
    "\n",
    "# Customize the plot\n",
    "g.set_axis_labels(\"Duration\", \"Frequency\")\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.fig.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = (\n",
    "    \"C:/athesis/deep_learning_for_semiparametric_did_estimation/paper/graphs/dist.png\"\n",
    ")\n",
    "g.savefig(plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression exclusion test\n",
    "\n",
    "For different kind of sicknesses, we can see that the regression exclusion test is not significant. This means that the regression is not biased by the exclusion of the variable.\n",
    "If it is significant within the treatment, meaning duration of the sick leave varies significantly between injury, which is a violation of assuming homogenopous treatment effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diag = df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_highearn = df_diag[df_diag[\"highearn\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_highearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table = df_highearn.pivot_table(\n",
    "    index=\"after_1980\",\n",
    "    columns=\"injtype\",\n",
    "    values=\"duration\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0,\n",
    ")\n",
    "pivot_table.loc[\"difference\"] = pivot_table.loc[1] - pivot_table.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming df_filtered is your initial DataFrame\n",
    "# Filter the data for highearn = 1\n",
    "highearn_1 = df_diag[df_diag[\"highearn\"] == 1]\n",
    "highearn_1 = pd.get_dummies(highearn_1, columns=[\"injtype\"], drop_first=True)\n",
    "\n",
    "# Convert 'injtype' boolean columns to numeric\n",
    "for col in highearn_1.columns:\n",
    "    if col.startswith(\"injtype_\"):\n",
    "        highearn_1[col] = highearn_1[col].astype(int)\n",
    "\n",
    "# Define the dependent and independent variables\n",
    "y = highearn_1[\"duration\"]\n",
    "X = highearn_1[\n",
    "    [\"after_1980\"] + [col for col in highearn_1.columns if col.startswith(\"injtype_\")]\n",
    "]\n",
    "X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
    "\n",
    "# Fit the full model\n",
    "full_model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Fit the restricted model (excluding injtype variables)\n",
    "restricted_model = sm.OLS(y, sm.add_constant(highearn_1[[\"after_1980\"]])).fit()\n",
    "\n",
    "# Perform the F-test to compare the models\n",
    "anova_results = sm.stats.anova_lm(restricted_model, full_model)\n",
    "\n",
    "# Drop the first row with NaN values for better readability\n",
    "anova_results = anova_results.dropna()\n",
    "\n",
    "# Display the results\n",
    "anova_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_filtered)\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Convert to categorical for the linear model\n",
    "df[\"after_1980\"] = df[\"after_1980\"].map({0: \"Before_1980\", 1: \"After_1980\"})\n",
    "df[\"highearn\"] = df[\"highearn\"].map({0: \"Low_earner\", 1: \"High_earner\"})\n",
    "\n",
    "# Fit the linear model\n",
    "model = smf.ols(\n",
    "    \"log_duration ~ highearn + after_1980 + highearn * after_1980\",\n",
    "    data=df,\n",
    ").fit()\n",
    "\n",
    "# Display the results in a tidy format\n",
    "results = model.summary2().tables[1]\n",
    "print(results)\n",
    "\n",
    "# Alternatively, display the results in a more \"tidy\" DataFrame format\n",
    "tidy_results = results.reset_index()\n",
    "tidy_results.columns = [\n",
    "    \"term\",\n",
    "    \"estimate\",\n",
    "    \"std_error\",\n",
    "    \"t_value\",\n",
    "    \"p_value\",\n",
    "    \"conf_lower\",\n",
    "    \"conf_upper\",\n",
    "]\n",
    "print(tidy_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coef.\tStd.Err.\tt\tP>|t|\t[0.025\t0.975]\n",
    "Intercept\t1.580352\t0.037248\t42.427637\t0.000000e+00\t1.507332\t1.653373\n",
    "highearn[T.Low_earner]\t-0.447080\t0.049420\t-9.046587\t1.995458e-19\t-0.543961\t-0.350198\n",
    "after_1980[T.Before_1980]\t-0.198259\t0.051902\t-3.819849\t1.349724e-04\t-0.300007\t-0.096510\n",
    "highearn[T.Low_earner]:after_1980[T.Before_1980]\t0.190601\t0.068509\t2.782138\t5.418222e-03\t0.056297\t0.324905"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Analysis + Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_filtered)\n",
    "\n",
    "# Convert to categorical variables\n",
    "df[\"indust\"] = df[\"indust\"].astype(\"category\")\n",
    "df[\"injtype\"] = df[\"injtype\"].astype(\"category\")\n",
    "\n",
    "# Fit the more complex linear model\n",
    "model_2 = smf.ols(\n",
    "    \"log_duration ~ highearn + after_1980 + highearn * after_1980 + male + married + lage + hosp + indust + injtype + prewage + prewage * highearn + construc+ manuf +totmed +head +neck + upextr+trunk+lowback+lowextr+occdis\",\n",
    "    data=df,\n",
    ").fit()\n",
    "\n",
    "# Display the results in a tidy format\n",
    "results_complex = model_2.summary2().tables[1]\n",
    "print(results_complex)\n",
    "\n",
    "# Alternatively, display the results in a more \"tidy\" DataFrame format\n",
    "tidy_results = results_complex.reset_index()\n",
    "tidy_results.columns = [\n",
    "    \"term\",\n",
    "    \"estimate\",\n",
    "    \"std_error\",\n",
    "    \"t_value\",\n",
    "    \"p_value\",\n",
    "    \"conf_lower\",\n",
    "    \"conf_upper\",\n",
    "]\n",
    "print(tidy_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Doubly Robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main model for Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[\"indust\"] = df_filtered[\"indust\"].astype(\"category\")\n",
    "df_filtered[\"injtype\"] = df_filtered[\"injtype\"].astype(\"category\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(df_filtered)\n",
    "df_cleaned = df.dropna()\n",
    "df = df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "\n",
    "# Configure TensorFlow for deterministic operations\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Use CPU for deterministic behavior\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Setting seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    \"\"\"To retrieve the training and validation losses during training.\"\"\"\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.losses.append(logs.get(\"loss\"))\n",
    "        self.val_losses.append(logs.get(\"val_loss\"))\n",
    "\n",
    "    def get_min_loss(self):\n",
    "        return min(self.losses), min(self.val_losses)\n",
    "\n",
    "\n",
    "def create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(units, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    for _ in range(depth - 2):\n",
    "        x = Dense(units, kernel_regularizer=l2(l2_reg))(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with Adam optimizer and specified learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class KerasClassifier(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        depth,\n",
    "        units,\n",
    "        learning_rate,\n",
    "        l2_reg,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        random_state=42,\n",
    "    ):\n",
    "        self.input_dim = input_dim\n",
    "        self.depth = depth\n",
    "        self.units = units\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_reg = l2_reg\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.classes_ = None\n",
    "        self.train_loss = None\n",
    "        self.val_loss = None\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        np.random.seed(self.random_state)\n",
    "        tf.random.set_seed(self.random_state)\n",
    "\n",
    "        self.model = create_deep_ffnn(\n",
    "            self.input_dim,\n",
    "            self.depth,\n",
    "            self.units,\n",
    "            self.learning_rate,\n",
    "            self.l2_reg,\n",
    "        )\n",
    "        history = LossHistory()\n",
    "        self.model.fit(\n",
    "            X,\n",
    "            y,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=0,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[history],\n",
    "        )\n",
    "        self.train_loss = history.losses\n",
    "        self.val_loss = history.val_losses\n",
    "        self.classes_ = np.unique(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred_prob = self.model.predict(X)\n",
    "        return (pred_prob > 0.5).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        pred_prob = self.model.predict(X)\n",
    "        return np.hstack([1 - pred_prob, pred_prob])\n",
    "\n",
    "\n",
    "# Define parameters for the neural network\n",
    "depth = 3\n",
    "units = 32\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.01\n",
    "input_dim = 19  # Adjust according to your feature dimension\n",
    "\n",
    "df.dropna()\n",
    "df[\"D\"] = df[\"highearn\"] * df[\"after_1980\"]\n",
    "y = df[\"log_duration\"].values\n",
    "d = df[\"D\"].values\n",
    "x = df[\n",
    "    [\n",
    "        \"highearn\",\n",
    "        \"after_1980\",\n",
    "        \"male\",\n",
    "        \"married\",\n",
    "        \"lage\",\n",
    "        \"hosp\",\n",
    "        \"indust\",\n",
    "        \"injtype\",\n",
    "        \"prewage\",\n",
    "        \"construc\",\n",
    "        \"manuf\",\n",
    "        \"totmed\",\n",
    "        \"head\",\n",
    "        \"neck\",\n",
    "        \"upextr\",\n",
    "        \"trunk\",\n",
    "        \"lowback\",\n",
    "        \"lowextr\",\n",
    "        \"occdis\",\n",
    "    ]\n",
    "].values\n",
    "\n",
    "# Split the data for the neural network\n",
    "int_cov_train, int_cov_val, D_train, D_val = train_test_split(\n",
    "    x,\n",
    "    d,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Create and train the neural network classifier\n",
    "nn_classifier = KerasClassifier(\n",
    "    input_dim=input_dim,\n",
    "    depth=depth,\n",
    "    units=units,\n",
    "    learning_rate=learning_rate,\n",
    "    l2_reg=l2_reg,\n",
    "    random_state=42,\n",
    ")\n",
    "nn_classifier.fit(int_cov_train, D_train)\n",
    "\n",
    "# Define and fit the DoubleML model\n",
    "from doubleml import DoubleMLData, DoubleMLDID\n",
    "\n",
    "ml_g = LinearRegression()\n",
    "dml_data = DoubleMLData.from_arrays(x=x, y=y, d=d)\n",
    "dml_did = DoubleMLDID(\n",
    "    dml_data,\n",
    "    ml_g=ml_g,\n",
    "    ml_m=nn_classifier,\n",
    "    score=\"observational\",\n",
    "    in_sample_normalization=False,\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_did.fit()\n",
    "\n",
    "min_train_loss = min(nn_classifier.train_loss)\n",
    "min_val_loss = min(nn_classifier.val_loss)\n",
    "print(\"Minimum Training Loss: \", min_train_loss)\n",
    "print(\"Minimum Validation Loss: \", min_val_loss)\n",
    "\n",
    "# Output the results\n",
    "print(dml_did)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_did.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coef\tstd err\tt\tP>|t|\t2.5 %\t97.5 %\n",
    "d\t0.184286\t0.04616\t3.992338\t0.000065\t0.093814\t0.274758"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
