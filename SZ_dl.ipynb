{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP 1 SZ + Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the DRDID function\n",
    "\n",
    "\n",
    "def create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(units, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    for _ in range(depth - 2):\n",
    "        x = Dense(units, kernel_regularizer=l2(l2_reg))(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with Adam optimizer and specified learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def drdid_rc(y, post, D, covariates=None, i_weights=None):\n",
    "    # Ensure D is a vector\n",
    "    D = np.asarray(D)\n",
    "    # Sample size\n",
    "    n = len(D)\n",
    "    # Ensure y is a vector\n",
    "    y = np.asarray(y)\n",
    "    # Ensure post is a vector\n",
    "    post = np.asarray(post)\n",
    "    # Add constant to covariate vector\n",
    "    int_cov = np.ones((n, 1))\n",
    "    if covariates is not None:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "\n",
    "    # Define parameters for the neural network\n",
    "    depth = 3\n",
    "    units = 32\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.01\n",
    "    input_dim = int_cov.shape[1]\n",
    "\n",
    "    # Create and compile the model with the optimal hyperparameters\n",
    "    model = create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(int_cov, D, sample_weight=i_weights, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict the probabilities\n",
    "    ps_fit = model.predict(int_cov).flatten()\n",
    "\n",
    "    # Ensure no values are exactly 0 or 1\n",
    "    ps_fit = np.clip(ps_fit, 1e-8, 1 - 1e-8)\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the pre-treatment period, using OLS\n",
    "    reg_cont_pre = sm.WLS(\n",
    "        y[(D == 0) & (post == 0)],\n",
    "        int_cov[(D == 0) & (post == 0)],\n",
    "        weights=i_weights[(D == 0) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_cont_pre = int_cov @ reg_cont_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the post-treatment period, using OLS\n",
    "    reg_cont_post = sm.WLS(\n",
    "        y[(D == 0) & (post == 1)],\n",
    "        int_cov[(D == 0) & (post == 1)],\n",
    "        weights=i_weights[(D == 0) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_cont_post = int_cov @ reg_cont_post.params\n",
    "\n",
    "    # Combine the ORs for control group\n",
    "    out_y_cont = post * out_y_cont_post + (1 - post) * out_y_cont_pre\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the pre-treatment period, using OLS\n",
    "    reg_treat_pre = sm.WLS(\n",
    "        y[(D == 1) & (post == 0)],\n",
    "        int_cov[(D == 1) & (post == 0)],\n",
    "        weights=i_weights[(D == 1) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_treat_pre = int_cov @ reg_treat_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the post-treatment period, using OLS\n",
    "    reg_treat_post = sm.WLS(\n",
    "        y[(D == 1) & (post == 1)],\n",
    "        int_cov[(D == 1) & (post == 1)],\n",
    "        weights=i_weights[(D == 1) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_treat_post = int_cov @ reg_treat_post.params\n",
    "\n",
    "    # Weights\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    w_d = i_weights * D\n",
    "    w_dt1 = i_weights * D * post\n",
    "    w_dt0 = i_weights * D * (1 - post)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * (y - out_y_cont) / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * (y - out_y_cont) / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * (y - out_y_cont) / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * (y - out_y_cont) / np.mean(w_cont_post)\n",
    "\n",
    "    # Extra elements for the locally efficient DRDID\n",
    "    eta_d_post = w_d * (out_y_treat_post - out_y_cont_post) / np.mean(w_d)\n",
    "    eta_dt1_post = w_dt1 * (out_y_treat_post - out_y_cont_post) / np.mean(w_dt1)\n",
    "    eta_d_pre = w_d * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_d)\n",
    "    eta_dt0_pre = w_dt0 * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_dt0)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    att_d_post = np.mean(eta_d_post)\n",
    "    att_dt1_post = np.mean(eta_dt1_post)\n",
    "    att_d_pre = np.mean(eta_d_pre)\n",
    "    att_dt0_pre = np.mean(eta_dt0_pre)\n",
    "\n",
    "    # ATT estimator\n",
    "    dr_att = (\n",
    "        (att_treat_post - att_treat_pre)\n",
    "        - (att_cont_post - att_cont_pre)\n",
    "        + (att_d_post - att_dt1_post)\n",
    "        - (att_d_pre - att_dt0_pre)\n",
    "    )\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    # Leading term of the influence function: no estimation effect\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M1_post = -np.mean(\n",
    "        w_treat_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_post)\n",
    "    M1_pre = -np.mean(\n",
    "        w_treat_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_treat_or_post = np.dot(reg_cont_post.cov_params(), M1_post)\n",
    "    inf_treat_or_pre = np.dot(reg_cont_pre.cov_params(), M1_pre)\n",
    "    inf_treat_or = inf_treat_or_post + inf_treat_or_pre\n",
    "\n",
    "    # Influence function for the treated component\n",
    "    inf_treat = inf_treat_post - inf_treat_pre + np.sum(inf_treat_or)\n",
    "\n",
    "    # Now, get the influence function of control component\n",
    "    # Leading term of the influence function: no estimation effect from nuisance parameters\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "\n",
    "    # Influence function for the control component\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Get the influence function of the inefficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func1 = inf_treat - inf_cont\n",
    "\n",
    "    # Now, we only need to get the influence function of the adjustment terms\n",
    "    # First, the terms as if all OR parameters were known\n",
    "    inf_eff1 = eta_d_post - w_d * att_d_post / np.mean(w_d)\n",
    "    inf_eff2 = eta_dt1_post - w_dt1 * att_dt1_post / np.mean(w_dt1)\n",
    "    inf_eff3 = eta_d_pre - w_d * att_d_pre / np.mean(w_d)\n",
    "    inf_eff4 = eta_dt0_pre - w_dt0 * att_dt0_pre / np.mean(w_dt0)\n",
    "    inf_eff = (inf_eff1 - inf_eff2) - (inf_eff3 - inf_eff4)\n",
    "\n",
    "    # Now the estimation effect of the OR coefficients\n",
    "    mom_post = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt1 / np.mean(w_dt1))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    mom_pre = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt0 / np.mean(w_dt0))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    inf_or_post = np.dot(\n",
    "        (reg_treat_post.cov_params() - reg_cont_post.cov_params()),\n",
    "        mom_post,\n",
    "    )\n",
    "    inf_or_pre = np.dot(\n",
    "        (reg_treat_pre.cov_params() - reg_cont_pre.cov_params()),\n",
    "        mom_pre,\n",
    "    )\n",
    "    inf_or = inf_or_post - inf_or_pre\n",
    "    inf_or = np.sum(inf_or)\n",
    "\n",
    "    # Get the influence function of the locally efficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func = dr_att_inf_func1 + inf_eff + inf_or\n",
    "\n",
    "    # Estimate of standard error\n",
    "    se_dr_att = np.std(dr_att_inf_func) / np.sqrt(n)\n",
    "\n",
    "    return {\n",
    "        \"ATT\": dr_att,\n",
    "        \"se\": se_dr_att,\n",
    "    }\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def sz_dl_dgp1(dgp_type):\n",
    "    # Define parameters\n",
    "    n = 1000  # Sample size\n",
    "    Xsi_ps = 0.75  # pscore index\n",
    "\n",
    "    # Define means and standard deviations\n",
    "    mean_z1 = np.exp(0.25 / 2)\n",
    "    sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "    mean_z2 = 10\n",
    "    sd_z2 = 0.54164\n",
    "    mean_z3 = 0.21887\n",
    "    sd_z3 = 0.04453\n",
    "    mean_z4 = 402\n",
    "    sd_z4 = 56.63891\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    ATTE_estimates = []\n",
    "    asymptotic_variance = []\n",
    "\n",
    "    # Loop for 1000 runs\n",
    "    for _i in range(10):\n",
    "        # Generate covariates\n",
    "        x1 = np.random.normal(0, 1, n)\n",
    "        x2 = np.random.normal(0, 1, n)\n",
    "        x3 = np.random.normal(0, 1, n)\n",
    "        x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "        z1 = np.exp(x1 / 2)\n",
    "        z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "        z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "        z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "        z1 = (z1 - mean_z1) / sd_z1\n",
    "        z2 = (z2 - mean_z2) / sd_z2\n",
    "        z3 = (z3 - mean_z3) / sd_z3\n",
    "        z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "        # Propensity score\n",
    "        pi = logistic_cdf(Xsi_ps * (-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4))\n",
    "        d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "        # Generate aux indexes for the potential outcomes\n",
    "        index_lin = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "        index_unobs_het = d * index_lin\n",
    "        index_att = 0\n",
    "        index_trend = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "        # Generate unobserved heterogeneity\n",
    "        v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "        # Generate outcomes at time 0 and time 1\n",
    "        y00 = index_lin + v + np.random.normal(size=n)\n",
    "        y10 = index_lin + v + np.random.normal(size=n)\n",
    "        y01 = index_lin + v + np.random.normal(scale=1, size=n) + index_trend\n",
    "        y11 = (\n",
    "            index_lin + v + np.random.normal(scale=1, size=n) + index_trend + index_att\n",
    "        )\n",
    "\n",
    "        # Generate \"T\"\n",
    "        ti_nt = 0.5\n",
    "        ti_t = 0.5\n",
    "        ti = d * ti_t + (1 - d) * ti_nt\n",
    "        post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "        y = np.where(\n",
    "            d & post,\n",
    "            y11,\n",
    "            np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "        )\n",
    "\n",
    "        # Generate id\n",
    "        id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "        time = np.tile([0, 1], n)\n",
    "\n",
    "        # Put in a long data frame\n",
    "        dta_long = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": id_,\n",
    "                \"time\": time,\n",
    "                \"y\": np.tile(y, 2),\n",
    "                \"post\": np.tile(post.astype(int), 2),\n",
    "                \"d\": np.tile(d.astype(int), 2),\n",
    "                \"x1\": np.tile(z1, 2),\n",
    "                \"x2\": np.tile(z2, 2),\n",
    "                \"x3\": np.tile(z3, 2),\n",
    "                \"x4\": np.tile(z4, 2),\n",
    "            },\n",
    "        )\n",
    "        dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "        dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "        # Run the IPW-DID estimator\n",
    "        covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "        y = dta_long[\"y\"].values\n",
    "        post = dta_long[\"post\"].values\n",
    "        D = dta_long[\"d\"].values\n",
    "\n",
    "        result = drdid_rc(y, post, D, covariates)\n",
    "\n",
    "        ATTE_estimates.append(result[\"ATT\"])\n",
    "        asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "\n",
    "    # Calculate average bias, median bias, and RMSE\n",
    "    true_ATT = 0\n",
    "    average_bias = np.mean(ATTE_estimates) - true_ATT\n",
    "    median_bias = np.median(ATTE_estimates) - true_ATT\n",
    "    rmse = np.sqrt(np.mean((np.array(ATTE_estimates) - true_ATT) ** 2))\n",
    "\n",
    "    # Calculate average of the variance\n",
    "    average_variance = np.mean(asymptotic_variance)\n",
    "    results = {\n",
    "        \"avg_bias\": average_bias,\n",
    "        \"med_bias\": median_bias,\n",
    "        \"rmse\": rmse,\n",
    "        \"average_variance\": average_variance,\n",
    "    }\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(\"bld/sz_dl_results\", exist_ok=True)\n",
    "    latex_filename = f\"bld/sz_dl_results/sz_dl_{dgp_type}.tex\"\n",
    "\n",
    "    # Writing the results to a LaTeX file\n",
    "    with open(latex_filename, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[ht]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{|l|r|}\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"Metric & Value \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key.replace('_', ' ').title()} & {value:.4f} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\n",
    "            f\"\\\\caption{{Simulation Results for double robust deep learning with DGP Type {dgp_type}}}\\n\",\n",
    "        )\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "    # Save ATTE estimates as a pickle file\n",
    "    os.makedirs(\"bld/sz_dl_results\", exist_ok=True)\n",
    "    pickle_filename = f\"bld/sz_dl_results/sz_dl_atte_estimates_dgp_{dgp_type}.pkl\"\n",
    "    with open(pickle_filename, \"wb\") as f:\n",
    "        pickle.dump(ATTE_estimates, f)\n",
    "\n",
    "    return {\n",
    "        \"Average Bias\": average_bias,\n",
    "        \"Median Bias\": median_bias,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Average Variance of ATT\": average_variance,\n",
    "    }\n",
    "\n",
    "\n",
    "sz_dl_dgp1(dgp_type=\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP 2 SZ + Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the DRDID function\n",
    "\n",
    "\n",
    "def create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(units, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    for _ in range(depth - 2):\n",
    "        x = Dense(units, kernel_regularizer=l2(l2_reg))(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with Adam optimizer and specified learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def drdid_rc(y, post, D, covariates=None, i_weights=None):\n",
    "    # Ensure D is a vector\n",
    "    D = np.asarray(D)\n",
    "    # Sample size\n",
    "    n = len(D)\n",
    "    # Ensure y is a vector\n",
    "    y = np.asarray(y)\n",
    "    # Ensure post is a vector\n",
    "    post = np.asarray(post)\n",
    "    # Add constant to covariate vector\n",
    "    int_cov = np.ones((n, 1))\n",
    "    if covariates is not None:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "\n",
    "    # Define parameters for the neural network\n",
    "    depth = 3\n",
    "    units = 32\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.01\n",
    "    input_dim = int_cov.shape[1]\n",
    "\n",
    "    # Create and compile the model with the optimal hyperparameters\n",
    "    model = create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(int_cov, D, sample_weight=i_weights, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict the probabilities\n",
    "    ps_fit = model.predict(int_cov).flatten()\n",
    "\n",
    "    # Ensure no values are exactly 0 or 1\n",
    "    ps_fit = np.clip(ps_fit, 1e-8, 1 - 1e-8)\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the pre-treatment period, using OLS\n",
    "    reg_cont_pre = sm.WLS(\n",
    "        y[(D == 0) & (post == 0)],\n",
    "        int_cov[(D == 0) & (post == 0)],\n",
    "        weights=i_weights[(D == 0) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_cont_pre = int_cov @ reg_cont_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the post-treatment period, using OLS\n",
    "    reg_cont_post = sm.WLS(\n",
    "        y[(D == 0) & (post == 1)],\n",
    "        int_cov[(D == 0) & (post == 1)],\n",
    "        weights=i_weights[(D == 0) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_cont_post = int_cov @ reg_cont_post.params\n",
    "\n",
    "    # Combine the ORs for control group\n",
    "    out_y_cont = post * out_y_cont_post + (1 - post) * out_y_cont_pre\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the pre-treatment period, using OLS\n",
    "    reg_treat_pre = sm.WLS(\n",
    "        y[(D == 1) & (post == 0)],\n",
    "        int_cov[(D == 1) & (post == 0)],\n",
    "        weights=i_weights[(D == 1) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_treat_pre = int_cov @ reg_treat_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the post-treatment period, using OLS\n",
    "    reg_treat_post = sm.WLS(\n",
    "        y[(D == 1) & (post == 1)],\n",
    "        int_cov[(D == 1) & (post == 1)],\n",
    "        weights=i_weights[(D == 1) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_treat_post = int_cov @ reg_treat_post.params\n",
    "\n",
    "    # Weights\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    w_d = i_weights * D\n",
    "    w_dt1 = i_weights * D * post\n",
    "    w_dt0 = i_weights * D * (1 - post)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * (y - out_y_cont) / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * (y - out_y_cont) / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * (y - out_y_cont) / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * (y - out_y_cont) / np.mean(w_cont_post)\n",
    "\n",
    "    # Extra elements for the locally efficient DRDID\n",
    "    eta_d_post = w_d * (out_y_treat_post - out_y_cont_post) / np.mean(w_d)\n",
    "    eta_dt1_post = w_dt1 * (out_y_treat_post - out_y_cont_post) / np.mean(w_dt1)\n",
    "    eta_d_pre = w_d * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_d)\n",
    "    eta_dt0_pre = w_dt0 * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_dt0)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    att_d_post = np.mean(eta_d_post)\n",
    "    att_dt1_post = np.mean(eta_dt1_post)\n",
    "    att_d_pre = np.mean(eta_d_pre)\n",
    "    att_dt0_pre = np.mean(eta_dt0_pre)\n",
    "\n",
    "    # ATT estimator\n",
    "    dr_att = (\n",
    "        (att_treat_post - att_treat_pre)\n",
    "        - (att_cont_post - att_cont_pre)\n",
    "        + (att_d_post - att_dt1_post)\n",
    "        - (att_d_pre - att_dt0_pre)\n",
    "    )\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    # Leading term of the influence function: no estimation effect\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M1_post = -np.mean(\n",
    "        w_treat_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_post)\n",
    "    M1_pre = -np.mean(\n",
    "        w_treat_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_treat_or_post = np.dot(reg_cont_post.cov_params(), M1_post)\n",
    "    inf_treat_or_pre = np.dot(reg_cont_pre.cov_params(), M1_pre)\n",
    "    inf_treat_or = inf_treat_or_post + inf_treat_or_pre\n",
    "\n",
    "    # Influence function for the treated component\n",
    "    inf_treat = inf_treat_post - inf_treat_pre + np.sum(inf_treat_or)\n",
    "\n",
    "    # Now, get the influence function of control component\n",
    "    # Leading term of the influence function: no estimation effect from nuisance parameters\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "\n",
    "    # Influence function for the control component\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Get the influence function of the inefficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func1 = inf_treat - inf_cont\n",
    "\n",
    "    # Now, we only need to get the influence function of the adjustment terms\n",
    "    # First, the terms as if all OR parameters were known\n",
    "    inf_eff1 = eta_d_post - w_d * att_d_post / np.mean(w_d)\n",
    "    inf_eff2 = eta_dt1_post - w_dt1 * att_dt1_post / np.mean(w_dt1)\n",
    "    inf_eff3 = eta_d_pre - w_d * att_d_pre / np.mean(w_d)\n",
    "    inf_eff4 = eta_dt0_pre - w_dt0 * att_dt0_pre / np.mean(w_dt0)\n",
    "    inf_eff = (inf_eff1 - inf_eff2) - (inf_eff3 - inf_eff4)\n",
    "\n",
    "    # Now the estimation effect of the OR coefficients\n",
    "    mom_post = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt1 / np.mean(w_dt1))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    mom_pre = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt0 / np.mean(w_dt0))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    inf_or_post = np.dot(\n",
    "        (reg_treat_post.cov_params() - reg_cont_post.cov_params()),\n",
    "        mom_post,\n",
    "    )\n",
    "    inf_or_pre = np.dot(\n",
    "        (reg_treat_pre.cov_params() - reg_cont_pre.cov_params()),\n",
    "        mom_pre,\n",
    "    )\n",
    "    inf_or = inf_or_post - inf_or_pre\n",
    "    inf_or = np.sum(inf_or)\n",
    "\n",
    "    # Get the influence function of the locally efficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func = dr_att_inf_func1 + inf_eff + inf_or\n",
    "\n",
    "    # Estimate of standard error\n",
    "    se_dr_att = np.std(dr_att_inf_func) / np.sqrt(n)\n",
    "\n",
    "    return {\n",
    "        \"ATT\": dr_att,\n",
    "        \"se\": se_dr_att,\n",
    "    }\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "# Define parameters\n",
    "\n",
    "\n",
    "def sz_dl_dgp2(dgp_type):\n",
    "    n = 1000\n",
    "    # pscore index (strength of common support)\n",
    "    Xsi_ps = 0.75\n",
    "    # Proportion in each period\n",
    "    _lambda = 0.5\n",
    "    # Number of bootstrapped draws\n",
    "\n",
    "    # Mean and Std deviation of Z's without truncation\n",
    "    mean_z1 = np.exp(0.25 / 2)\n",
    "    sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "    mean_z2 = 10\n",
    "    sd_z2 = 0.54164\n",
    "    mean_z3 = 0.21887\n",
    "    sd_z3 = 0.04453\n",
    "    mean_z4 = 402\n",
    "    sd_z4 = 56.63891\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    ATTE_estimates = []\n",
    "    asymptotic_variance = []\n",
    "\n",
    "    for _i in range(10):\n",
    "        # Generate covariates\n",
    "        x1 = np.random.normal(0, 1, n)\n",
    "        x2 = np.random.normal(0, 1, n)\n",
    "        x3 = np.random.normal(0, 1, n)\n",
    "        x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "        z1 = np.exp(x1 / 2)\n",
    "        z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "        z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "        z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "        z1 = (z1 - mean_z1) / sd_z1\n",
    "        z2 = (z2 - mean_z2) / sd_z2\n",
    "        z3 = (z3 - mean_z3) / sd_z3\n",
    "        z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "        np.column_stack((x1, x2, x3, x4))\n",
    "        np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "        # Generate treatment groups\n",
    "        # Propensity score\n",
    "        pi = logistic_cdf(Xsi_ps * (-x1 + 0.5 * x2 - 0.25 * x3 - 0.1 * x4))\n",
    "        d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "        # Generate aux indexes for the potential outcomes\n",
    "        index_lin = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "        # Create heterogenenous effects for the ATT, which is set approximately equal to zero\n",
    "        index_unobs_het = d * (index_lin)\n",
    "        index_att = 0\n",
    "\n",
    "        # This is the key for consistency of outcome regression\n",
    "        index_trend = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "        # v is the unobserved heterogeneity\n",
    "        v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "        # Gen realized outcome at time 0\n",
    "        y00 = index_lin + v + np.random.normal(size=n)\n",
    "        y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "        # Gen outcomes at time 1\n",
    "        # First let's generate potential outcomes: y_1_potential\n",
    "        y01 = (\n",
    "            index_lin + v + np.random.normal(scale=1, size=n) + index_trend\n",
    "        )  # This is the baseline\n",
    "        y11 = (\n",
    "            index_lin + v + np.random.normal(scale=1, size=n) + index_trend + index_att\n",
    "        )  # This is the baseline\n",
    "\n",
    "        # Generate \"T\"\n",
    "        ti_nt = 0.5\n",
    "        ti_t = 0.5\n",
    "        ti = d * ti_t + (1 - d) * ti_nt\n",
    "        post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "        y = np.where(\n",
    "            d & post,\n",
    "            y11,\n",
    "            np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "        )\n",
    "\n",
    "        # Gen id\n",
    "        id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "        time = np.tile([0, 1], n)\n",
    "\n",
    "        # Put in a long data frame\n",
    "        dta_long = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": id_,\n",
    "                \"time\": time,\n",
    "                \"y\": np.tile(y, 2),\n",
    "                \"post\": np.tile(post.astype(int), 2),\n",
    "                \"d\": np.tile(d.astype(int), 2),\n",
    "                \"x1\": np.tile(z1, 2),\n",
    "                \"x2\": np.tile(z2, 2),\n",
    "                \"x3\": np.tile(z3, 2),\n",
    "                \"x4\": np.tile(z4, 2),\n",
    "            },\n",
    "        )\n",
    "        dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "        dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "        # Run the IPW-DID estimator\n",
    "        covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "        y = dta_long[\"y\"].values\n",
    "        post = dta_long[\"post\"].values\n",
    "        D = dta_long[\"d\"].values\n",
    "\n",
    "        result = drdid_rc(y, post, D, covariates)\n",
    "\n",
    "        ATTE_estimates.append(result[\"ATT\"])\n",
    "        asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "\n",
    "    # Calculate average bias, median bias, and RMSE\n",
    "    true_ATT = 0\n",
    "    average_bias = np.mean(ATTE_estimates) - true_ATT\n",
    "    median_bias = np.median(ATTE_estimates) - true_ATT\n",
    "    rmse = np.sqrt(np.mean((np.array(ATTE_estimates) - true_ATT) ** 2))\n",
    "\n",
    "    # Calculate average of the variance\n",
    "    average_variance = np.mean(asymptotic_variance)\n",
    "\n",
    "    results = {\n",
    "        \"avg_bias\": average_bias,\n",
    "        \"med_bias\": median_bias,\n",
    "        \"rmse\": rmse,\n",
    "        \"average_variance\": average_variance,\n",
    "    }\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(\"bld/sz_dl_results\", exist_ok=True)\n",
    "    latex_filename = f\"bld/sz_dl_results/sz_dl_{dgp_type}.tex\"\n",
    "\n",
    "    # Writing the results to a LaTeX file\n",
    "    with open(latex_filename, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[ht]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{|l|r|}\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"Metric & Value \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key.replace('_', ' ').title()} & {value:.4f} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\n",
    "            f\"\\\\caption{{Simulation Results for double robust deep learning with DGP Type {dgp_type}}}\\n\",\n",
    "        )\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "    # Save ATTE estimates as a pickle file\n",
    "    os.makedirs(\"bld/sz_dl_results\", exist_ok=True)\n",
    "    pickle_filename = f\"bld/sz_dl_results/sz_dl_atte_estimates_dgp_{dgp_type}.pkl\"\n",
    "    with open(pickle_filename, \"wb\") as f:\n",
    "        pickle.dump(ATTE_estimates, f)\n",
    "\n",
    "    return {\n",
    "        \"Average Bias\": average_bias,\n",
    "        \"Median Bias\": median_bias,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Average Variance of ATT\": average_variance,\n",
    "    }\n",
    "\n",
    "\n",
    "sz_dl_dgp2(dgp_type=\"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP 3 SZ + Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the DRDID function\n",
    "\n",
    "\n",
    "def create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(units, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    for _ in range(depth - 2):\n",
    "        x = Dense(units, kernel_regularizer=l2(l2_reg))(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with Adam optimizer and specified learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def drdid_rc(y, post, D, covariates=None, i_weights=None):\n",
    "    # Ensure D is a vector\n",
    "    D = np.asarray(D)\n",
    "    # Sample size\n",
    "    n = len(D)\n",
    "    # Ensure y is a vector\n",
    "    y = np.asarray(y)\n",
    "    # Ensure post is a vector\n",
    "    post = np.asarray(post)\n",
    "    # Add constant to covariate vector\n",
    "    int_cov = np.ones((n, 1))\n",
    "    if covariates is not None:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "\n",
    "    # Define parameters for the neural network\n",
    "    depth = 3\n",
    "    units = 32\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.01\n",
    "    input_dim = int_cov.shape[1]\n",
    "\n",
    "    # Create and compile the model with the optimal hyperparameters\n",
    "    model = create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(int_cov, D, sample_weight=i_weights, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict the probabilities\n",
    "    ps_fit = model.predict(int_cov).flatten()\n",
    "\n",
    "    # Ensure no values are exactly 0 or 1\n",
    "    ps_fit = np.clip(ps_fit, 1e-8, 1 - 1e-8)\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the pre-treatment period, using OLS\n",
    "    reg_cont_pre = sm.WLS(\n",
    "        y[(D == 0) & (post == 0)],\n",
    "        int_cov[(D == 0) & (post == 0)],\n",
    "        weights=i_weights[(D == 0) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_cont_pre = int_cov @ reg_cont_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the post-treatment period, using OLS\n",
    "    reg_cont_post = sm.WLS(\n",
    "        y[(D == 0) & (post == 1)],\n",
    "        int_cov[(D == 0) & (post == 1)],\n",
    "        weights=i_weights[(D == 0) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_cont_post = int_cov @ reg_cont_post.params\n",
    "\n",
    "    # Combine the ORs for control group\n",
    "    out_y_cont = post * out_y_cont_post + (1 - post) * out_y_cont_pre\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the pre-treatment period, using OLS\n",
    "    reg_treat_pre = sm.WLS(\n",
    "        y[(D == 1) & (post == 0)],\n",
    "        int_cov[(D == 1) & (post == 0)],\n",
    "        weights=i_weights[(D == 1) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_treat_pre = int_cov @ reg_treat_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the post-treatment period, using OLS\n",
    "    reg_treat_post = sm.WLS(\n",
    "        y[(D == 1) & (post == 1)],\n",
    "        int_cov[(D == 1) & (post == 1)],\n",
    "        weights=i_weights[(D == 1) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_treat_post = int_cov @ reg_treat_post.params\n",
    "\n",
    "    # Weights\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    w_d = i_weights * D\n",
    "    w_dt1 = i_weights * D * post\n",
    "    w_dt0 = i_weights * D * (1 - post)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * (y - out_y_cont) / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * (y - out_y_cont) / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * (y - out_y_cont) / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * (y - out_y_cont) / np.mean(w_cont_post)\n",
    "\n",
    "    # Extra elements for the locally efficient DRDID\n",
    "    eta_d_post = w_d * (out_y_treat_post - out_y_cont_post) / np.mean(w_d)\n",
    "    eta_dt1_post = w_dt1 * (out_y_treat_post - out_y_cont_post) / np.mean(w_dt1)\n",
    "    eta_d_pre = w_d * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_d)\n",
    "    eta_dt0_pre = w_dt0 * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_dt0)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    att_d_post = np.mean(eta_d_post)\n",
    "    att_dt1_post = np.mean(eta_dt1_post)\n",
    "    att_d_pre = np.mean(eta_d_pre)\n",
    "    att_dt0_pre = np.mean(eta_dt0_pre)\n",
    "\n",
    "    # ATT estimator\n",
    "    dr_att = (\n",
    "        (att_treat_post - att_treat_pre)\n",
    "        - (att_cont_post - att_cont_pre)\n",
    "        + (att_d_post - att_dt1_post)\n",
    "        - (att_d_pre - att_dt0_pre)\n",
    "    )\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    # Leading term of the influence function: no estimation effect\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M1_post = -np.mean(\n",
    "        w_treat_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_post)\n",
    "    M1_pre = -np.mean(\n",
    "        w_treat_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_treat_or_post = np.dot(reg_cont_post.cov_params(), M1_post)\n",
    "    inf_treat_or_pre = np.dot(reg_cont_pre.cov_params(), M1_pre)\n",
    "    inf_treat_or = inf_treat_or_post + inf_treat_or_pre\n",
    "\n",
    "    # Influence function for the treated component\n",
    "    inf_treat = inf_treat_post - inf_treat_pre + np.sum(inf_treat_or)\n",
    "\n",
    "    # Now, get the influence function of control component\n",
    "    # Leading term of the influence function: no estimation effect from nuisance parameters\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "\n",
    "    # Influence function for the control component\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Get the influence function of the inefficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func1 = inf_treat - inf_cont\n",
    "\n",
    "    # Now, we only need to get the influence function of the adjustment terms\n",
    "    # First, the terms as if all OR parameters were known\n",
    "    inf_eff1 = eta_d_post - w_d * att_d_post / np.mean(w_d)\n",
    "    inf_eff2 = eta_dt1_post - w_dt1 * att_dt1_post / np.mean(w_dt1)\n",
    "    inf_eff3 = eta_d_pre - w_d * att_d_pre / np.mean(w_d)\n",
    "    inf_eff4 = eta_dt0_pre - w_dt0 * att_dt0_pre / np.mean(w_dt0)\n",
    "    inf_eff = (inf_eff1 - inf_eff2) - (inf_eff3 - inf_eff4)\n",
    "\n",
    "    # Now the estimation effect of the OR coefficients\n",
    "    mom_post = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt1 / np.mean(w_dt1))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    mom_pre = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt0 / np.mean(w_dt0))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    inf_or_post = np.dot(\n",
    "        (reg_treat_post.cov_params() - reg_cont_post.cov_params()),\n",
    "        mom_post,\n",
    "    )\n",
    "    inf_or_pre = np.dot(\n",
    "        (reg_treat_pre.cov_params() - reg_cont_pre.cov_params()),\n",
    "        mom_pre,\n",
    "    )\n",
    "    inf_or = inf_or_post - inf_or_pre\n",
    "    inf_or = np.sum(inf_or)\n",
    "\n",
    "    # Get the influence function of the locally efficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func = dr_att_inf_func1 + inf_eff + inf_or\n",
    "\n",
    "    # Estimate of standard error\n",
    "    se_dr_att = np.std(dr_att_inf_func) / np.sqrt(n)\n",
    "\n",
    "    return {\n",
    "        \"ATT\": dr_att,\n",
    "        \"se\": se_dr_att,\n",
    "    }\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "\n",
    "def sz_dl_dgp3(dgp_type):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Sample size\n",
    "    n = 1000\n",
    "    # pscore index (strength of common support)\n",
    "    Xsi_ps = 0.75\n",
    "    # Proportion in each period\n",
    "    # Number of bootstrapped draws\n",
    "\n",
    "    # Mean and Std deviation of Z's without truncation\n",
    "    mean_z1 = np.exp(0.25 / 2)\n",
    "    sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "    mean_z2 = 10\n",
    "    sd_z2 = 0.54164\n",
    "    mean_z3 = 0.21887\n",
    "    sd_z3 = 0.04453\n",
    "    mean_z4 = 402\n",
    "    sd_z4 = 56.63891\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    ATTE_estimates = []\n",
    "    asymptotic_variance = []\n",
    "\n",
    "    for _i in range(10):\n",
    "        # Generate covariates\n",
    "        x1 = np.random.normal(0, 1, n)\n",
    "        x2 = np.random.normal(0, 1, n)\n",
    "        x3 = np.random.normal(0, 1, n)\n",
    "        x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "        z1 = np.exp(x1 / 2)\n",
    "        z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "        z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "        z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "        z1 = (z1 - mean_z1) / sd_z1\n",
    "        z2 = (z2 - mean_z2) / sd_z2\n",
    "        z3 = (z3 - mean_z3) / sd_z3\n",
    "        z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "        np.column_stack((x1, x2, x3, x4))\n",
    "        np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "        # Propensity score\n",
    "        pi = logistic_cdf(Xsi_ps * (-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4))\n",
    "        d = (np.random.uniform(size=n) <= pi).astype(int)\n",
    "\n",
    "        # Generate aux indexes for the potential outcomes\n",
    "        index_lin = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "        # Create heterogeneous effects for the ATT, which is set approximately equal to zero\n",
    "        index_unobs_het = d * index_lin\n",
    "        index_att = 0\n",
    "\n",
    "        # This is the key for consistency of outcome regression\n",
    "        index_trend = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "        # v is the unobserved heterogeneity\n",
    "        v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "        # Gen realized outcome at time 0\n",
    "        y00 = index_lin + v + np.random.normal(size=n)\n",
    "        y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "        # Gen outcomes at time 1\n",
    "        y01 = (\n",
    "            index_lin + v + np.random.normal(size=n) + index_trend\n",
    "        )  # This is the baseline\n",
    "        y11 = (\n",
    "            index_lin + v + np.random.normal(size=n) + index_trend + index_att\n",
    "        )  # This is the baseline\n",
    "\n",
    "        # Generate \"T\"\n",
    "        ti_nt = 0.5\n",
    "        ti_t = 0.5\n",
    "        ti = d * ti_t + (1 - d) * ti_nt\n",
    "        post = (np.random.uniform(size=n) <= ti).astype(int)\n",
    "\n",
    "        y = np.where(\n",
    "            d & post,\n",
    "            y11,\n",
    "            np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "        )\n",
    "\n",
    "        # Gen id\n",
    "        id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "        time = np.tile([0, 1], n)\n",
    "\n",
    "        # Put in a long data frame\n",
    "        dta_long = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": id_,\n",
    "                \"time\": time,\n",
    "                \"y\": np.tile(y, 2),\n",
    "                \"post\": np.tile(post.astype(int), 2),\n",
    "                \"d\": np.tile(d.astype(int), 2),\n",
    "                \"x1\": np.tile(z1, 2),\n",
    "                \"x2\": np.tile(z2, 2),\n",
    "                \"x3\": np.tile(z3, 2),\n",
    "                \"x4\": np.tile(z4, 2),\n",
    "            },\n",
    "        )\n",
    "        dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "        dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "        # Run the IPW-DID estimator\n",
    "        covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "        y = dta_long[\"y\"].values\n",
    "        post = dta_long[\"post\"].values\n",
    "        D = dta_long[\"d\"].values\n",
    "\n",
    "        result = drdid_rc(y, post, D, covariates)\n",
    "\n",
    "        ATTE_estimates.append(result[\"ATT\"])\n",
    "        asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "\n",
    "    # Calculate average bias, median bias, and RMSE\n",
    "    true_ATT = 0\n",
    "    average_bias = np.mean(ATTE_estimates) - true_ATT\n",
    "    median_bias = np.median(ATTE_estimates) - true_ATT\n",
    "    rmse = np.sqrt(np.mean((np.array(ATTE_estimates) - true_ATT) ** 2))\n",
    "\n",
    "    # Calculate average of the variance\n",
    "    average_variance = np.mean(asymptotic_variance)\n",
    "    results = {\n",
    "        \"avg_bias\": average_bias,\n",
    "        \"med_bias\": median_bias,\n",
    "        \"rmse\": rmse,\n",
    "        \"average_variance\": average_variance,\n",
    "    }\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(\"bld/sz_dl_results\", exist_ok=True)\n",
    "    latex_filename = f\"bld/sz_dl_results/sz_dl_{dgp_type}.tex\"\n",
    "\n",
    "    # Writing the results to a LaTeX file\n",
    "    with open(latex_filename, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[ht]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{|l|r|}\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"Metric & Value \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key.replace('_', ' ').title()} & {value:.4f} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\n",
    "            f\"\\\\caption{{Simulation Results for double robust deep learning with DGP Type {dgp_type}}}\\n\",\n",
    "        )\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "    # Save ATTE estimates as a pickle file\n",
    "    os.makedirs(\"bld/sz_dl_results\", exist_ok=True)\n",
    "    pickle_filename = f\"bld/sz_dl_results/sz_dl_atte_estimates_dgp_{dgp_type}.pkl\"\n",
    "    with open(pickle_filename, \"wb\") as f:\n",
    "        pickle.dump(ATTE_estimates, f)\n",
    "\n",
    "    return {\n",
    "        \"Average Bias\": average_bias,\n",
    "        \"Median Bias\": median_bias,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Average Variance of ATT\": average_variance,\n",
    "    }\n",
    "\n",
    "\n",
    "sz_dl_dgp3(dgp_type=\"3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP 4 SZ + Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the DRDID function\n",
    "\n",
    "\n",
    "def create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(units, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    for _ in range(depth - 2):\n",
    "        x = Dense(units, kernel_regularizer=l2(l2_reg))(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with Adam optimizer and specified learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def drdid_rc(y, post, D, covariates=None, i_weights=None):\n",
    "    # Ensure D is a vector\n",
    "    D = np.asarray(D)\n",
    "    # Sample size\n",
    "    n = len(D)\n",
    "    # Ensure y is a vector\n",
    "    y = np.asarray(y)\n",
    "    # Ensure post is a vector\n",
    "    post = np.asarray(post)\n",
    "    # Add constant to covariate vector\n",
    "    int_cov = np.ones((n, 1))\n",
    "    if covariates is not None:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "\n",
    "    # Define parameters for the neural network\n",
    "    depth = 3\n",
    "    units = 32\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.01\n",
    "    input_dim = int_cov.shape[1]\n",
    "\n",
    "    # Create and compile the model with the optimal hyperparameters\n",
    "    model = create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(int_cov, D, sample_weight=i_weights, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict the probabilities\n",
    "    ps_fit = model.predict(int_cov).flatten()\n",
    "\n",
    "    # Ensure no values are exactly 0 or 1\n",
    "    ps_fit = np.clip(ps_fit, 1e-8, 1 - 1e-8)\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the pre-treatment period, using OLS\n",
    "    reg_cont_pre = sm.WLS(\n",
    "        y[(D == 0) & (post == 0)],\n",
    "        int_cov[(D == 0) & (post == 0)],\n",
    "        weights=i_weights[(D == 0) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_cont_pre = int_cov @ reg_cont_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the post-treatment period, using OLS\n",
    "    reg_cont_post = sm.WLS(\n",
    "        y[(D == 0) & (post == 1)],\n",
    "        int_cov[(D == 0) & (post == 1)],\n",
    "        weights=i_weights[(D == 0) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_cont_post = int_cov @ reg_cont_post.params\n",
    "\n",
    "    # Combine the ORs for control group\n",
    "    out_y_cont = post * out_y_cont_post + (1 - post) * out_y_cont_pre\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the pre-treatment period, using OLS\n",
    "    reg_treat_pre = sm.WLS(\n",
    "        y[(D == 1) & (post == 0)],\n",
    "        int_cov[(D == 1) & (post == 0)],\n",
    "        weights=i_weights[(D == 1) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_treat_pre = int_cov @ reg_treat_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the post-treatment period, using OLS\n",
    "    reg_treat_post = sm.WLS(\n",
    "        y[(D == 1) & (post == 1)],\n",
    "        int_cov[(D == 1) & (post == 1)],\n",
    "        weights=i_weights[(D == 1) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_treat_post = int_cov @ reg_treat_post.params\n",
    "\n",
    "    # Weights\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    w_d = i_weights * D\n",
    "    w_dt1 = i_weights * D * post\n",
    "    w_dt0 = i_weights * D * (1 - post)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * (y - out_y_cont) / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * (y - out_y_cont) / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * (y - out_y_cont) / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * (y - out_y_cont) / np.mean(w_cont_post)\n",
    "\n",
    "    # Extra elements for the locally efficient DRDID\n",
    "    eta_d_post = w_d * (out_y_treat_post - out_y_cont_post) / np.mean(w_d)\n",
    "    eta_dt1_post = w_dt1 * (out_y_treat_post - out_y_cont_post) / np.mean(w_dt1)\n",
    "    eta_d_pre = w_d * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_d)\n",
    "    eta_dt0_pre = w_dt0 * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_dt0)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    att_d_post = np.mean(eta_d_post)\n",
    "    att_dt1_post = np.mean(eta_dt1_post)\n",
    "    att_d_pre = np.mean(eta_d_pre)\n",
    "    att_dt0_pre = np.mean(eta_dt0_pre)\n",
    "\n",
    "    # ATT estimator\n",
    "    dr_att = (\n",
    "        (att_treat_post - att_treat_pre)\n",
    "        - (att_cont_post - att_cont_pre)\n",
    "        + (att_d_post - att_dt1_post)\n",
    "        - (att_d_pre - att_dt0_pre)\n",
    "    )\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    # Leading term of the influence function: no estimation effect\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M1_post = -np.mean(\n",
    "        w_treat_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_post)\n",
    "    M1_pre = -np.mean(\n",
    "        w_treat_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_treat_or_post = np.dot(reg_cont_post.cov_params(), M1_post)\n",
    "    inf_treat_or_pre = np.dot(reg_cont_pre.cov_params(), M1_pre)\n",
    "    inf_treat_or = inf_treat_or_post + inf_treat_or_pre\n",
    "\n",
    "    # Influence function for the treated component\n",
    "    inf_treat = inf_treat_post - inf_treat_pre + np.sum(inf_treat_or)\n",
    "\n",
    "    # Now, get the influence function of control component\n",
    "    # Leading term of the influence function: no estimation effect from nuisance parameters\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "\n",
    "    # Influence function for the control component\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Get the influence function of the inefficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func1 = inf_treat - inf_cont\n",
    "\n",
    "    # Now, we only need to get the influence function of the adjustment terms\n",
    "    # First, the terms as if all OR parameters were known\n",
    "    inf_eff1 = eta_d_post - w_d * att_d_post / np.mean(w_d)\n",
    "    inf_eff2 = eta_dt1_post - w_dt1 * att_dt1_post / np.mean(w_dt1)\n",
    "    inf_eff3 = eta_d_pre - w_d * att_d_pre / np.mean(w_d)\n",
    "    inf_eff4 = eta_dt0_pre - w_dt0 * att_dt0_pre / np.mean(w_dt0)\n",
    "    inf_eff = (inf_eff1 - inf_eff2) - (inf_eff3 - inf_eff4)\n",
    "\n",
    "    # Now the estimation effect of the OR coefficients\n",
    "    mom_post = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt1 / np.mean(w_dt1))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    mom_pre = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt0 / np.mean(w_dt0))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    inf_or_post = np.dot(\n",
    "        (reg_treat_post.cov_params() - reg_cont_post.cov_params()),\n",
    "        mom_post,\n",
    "    )\n",
    "    inf_or_pre = np.dot(\n",
    "        (reg_treat_pre.cov_params() - reg_cont_pre.cov_params()),\n",
    "        mom_pre,\n",
    "    )\n",
    "    inf_or = inf_or_post - inf_or_pre\n",
    "    inf_or = np.sum(inf_or)\n",
    "\n",
    "    # Get the influence function of the locally efficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func = dr_att_inf_func1 + inf_eff + inf_or\n",
    "\n",
    "    # Estimate of standard error\n",
    "    se_dr_att = np.std(dr_att_inf_func) / np.sqrt(n)\n",
    "\n",
    "    return {\n",
    "        \"ATT\": dr_att,\n",
    "        \"se\": se_dr_att,\n",
    "    }\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "\n",
    "def sz_dl_dgp4(dgp_type):\n",
    "    np.random.seed(42)  # You can use any integer value as the seed\n",
    "    # Sample size\n",
    "    n = 1000\n",
    "\n",
    "    # pscore index (strength of common support)\n",
    "    Xsi_ps = 0.75\n",
    "\n",
    "    # Proportion in each period\n",
    "\n",
    "    # Number of bootstrapped draws\n",
    "\n",
    "    # Mean and Std deviation of Z's without truncation\n",
    "    mean_z1 = np.exp(0.25 / 2)\n",
    "    sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "    mean_z2 = 10\n",
    "    sd_z2 = 0.54164\n",
    "    mean_z3 = 0.21887\n",
    "    sd_z3 = 0.04453\n",
    "    mean_z4 = 402\n",
    "    sd_z4 = 56.63891\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    ATTE_estimates = []\n",
    "    asymptotic_variance = []\n",
    "    for _i in range(10):\n",
    "        # Gen covariates\n",
    "        x1 = np.random.normal(0, 1, n)\n",
    "        x2 = np.random.normal(0, 1, n)\n",
    "        x3 = np.random.normal(0, 1, n)\n",
    "        x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "        z1 = np.exp(x1 / 2)\n",
    "        z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "        z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "        z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "        z1 = (z1 - mean_z1) / sd_z1\n",
    "        z2 = (z2 - mean_z2) / sd_z2\n",
    "        z3 = (z3 - mean_z3) / sd_z3\n",
    "        z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "        np.column_stack((x1, x2, x3, x4))\n",
    "        np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "        # Gen treatment groups\n",
    "        # Propensity score\n",
    "        pi = 1 / (1 + np.exp(-Xsi_ps * (-x1 + 0.5 * x2 - 0.25 * x3 - 0.1 * x4)))\n",
    "        d = np.random.rand(n) <= pi\n",
    "\n",
    "        # Generate aux indexes for the potential outcomes\n",
    "        index_lin = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "        # Create heterogeneous effects for the ATT, which is set approximately equal to zero\n",
    "        index_unobs_het = d * index_lin\n",
    "        index_att = 0\n",
    "\n",
    "        # This is the key for consistency of outcome regression\n",
    "        index_trend = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "        # v is the unobserved heterogeneity\n",
    "        v = np.random.normal(index_unobs_het, 1, n)\n",
    "\n",
    "        # Gen realized outcome at time 0\n",
    "        y00 = index_lin + v + np.random.normal(size=n)\n",
    "        y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "        # Gen outcomes at time 1\n",
    "        # First let's generate potential outcomes: y_1_potential\n",
    "        y01 = index_lin + v + np.random.normal(size=n) + index_trend\n",
    "        y11 = index_lin + v + np.random.normal(size=n) + index_trend + index_att\n",
    "\n",
    "        # Generate \"T\"\n",
    "        ti_nt = 0.5\n",
    "        ti_t = 0.5\n",
    "        ti = d * ti_t + (1 - d) * ti_nt\n",
    "        post = np.random.rand(n) <= ti\n",
    "\n",
    "        # Combine outcomes into panel data format\n",
    "        y = np.where(\n",
    "            d & post,\n",
    "            y11,\n",
    "            np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "        )\n",
    "\n",
    "        # Gen id\n",
    "        id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "        time = np.tile([0, 1], n)\n",
    "\n",
    "        # Put in a long data frame\n",
    "        dta_long = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": id_,\n",
    "                \"time\": time,\n",
    "                \"y\": np.tile(y, 2),\n",
    "                \"post\": np.tile(post.astype(int), 2),\n",
    "                \"d\": np.tile(d.astype(int), 2),\n",
    "                \"x1\": np.tile(z1, 2),\n",
    "                \"x2\": np.tile(z2, 2),\n",
    "                \"x3\": np.tile(z3, 2),\n",
    "                \"x4\": np.tile(z4, 2),\n",
    "            },\n",
    "        )\n",
    "        dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "        dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "        # Run the IPW-DID estimator\n",
    "        covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "        y = dta_long[\"y\"].values\n",
    "        post = dta_long[\"post\"].values\n",
    "        D = dta_long[\"d\"].values\n",
    "\n",
    "        result = drdid_rc(y, post, D, covariates)\n",
    "\n",
    "        ATTE_estimates.append(result[\"ATT\"])\n",
    "        asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "\n",
    "    # Calculate average bias, median bias, and RMSE\n",
    "    true_ATT = 0\n",
    "    average_bias = np.mean(ATTE_estimates) - true_ATT\n",
    "    median_bias = np.median(ATTE_estimates) - true_ATT\n",
    "    rmse = np.sqrt(np.mean((np.array(ATTE_estimates) - true_ATT) ** 2))\n",
    "\n",
    "    # Calculate average of the variance\n",
    "    average_variance = np.mean(asymptotic_variance)\n",
    "    results = {\n",
    "        \"avg_bias\": average_bias,\n",
    "        \"med_bias\": median_bias,\n",
    "        \"rmse\": rmse,\n",
    "        \"average_variance\": average_variance,\n",
    "    }\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(\"bld/sz_dl_results\", exist_ok=True)\n",
    "    latex_filename = f\"bld/sz_dl_results/sz_dl_{dgp_type}.tex\"\n",
    "\n",
    "    # Writing the results to a LaTeX file\n",
    "    with open(latex_filename, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[ht]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{|l|r|}\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"Metric & Value \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key.replace('_', ' ').title()} & {value:.4f} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\n",
    "            f\"\\\\caption{{Simulation Results for double robust deep learning with DGP Type {dgp_type}}}\\n\",\n",
    "        )\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "    # Save ATTE estimates as a pickle file\n",
    "    os.makedirs(\"bld/sz_dl_results\", exist_ok=True)\n",
    "    pickle_filename = f\"bld/sz_dl_results/sz_dl_atte_estimates_dgp_{dgp_type}.pkl\"\n",
    "    with open(pickle_filename, \"wb\") as f:\n",
    "        pickle.dump(ATTE_estimates, f)\n",
    "\n",
    "    return {\n",
    "        \"Average Bias\": average_bias,\n",
    "        \"Median Bias\": median_bias,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Average Variance of ATT\": average_variance,\n",
    "    }\n",
    "\n",
    "\n",
    "sz_dl_dgp4(dgp_type=\"4\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
