\section{Monte Carlo Simulations}


\subsection{Data Generating Process}

In this section, I introduce the \ac{dgp} for the Monte Carlo simulations.
The \ac{dgp} is based on the simulation study by \citet{kang2007demystifying} and \citet{santannaDoublyRobustDifferenceindifferences2020}.
The advantage of this setup is to ensures comparability with previous studies and allow to validate novel approaches as the use of deep learning for \ac{did} estimation.
For all simulations, the \ac{dgp} has a total sample size of $n=1000$.
There are two time periods $t=0,1$ and two groups $i=0,1$, such that it allows to apply the classical $2\times2$-\ac{did} estimator.
Since individuals are tracked over time, the data is panel data.
\citet{kang2007demystifying} created the \ac{dgp} to include covariate specific trends and homogenous treatment effects.
The true \ac{att} is $\tau = 0$.
In the first simulation shown in Table \ref{tab:table1}, I adhere to this specification. In the second simulation in Table \ref{tab:table2}, I extend the \ac{dgp} to allow for heterogeneous treatment effects.

To introduce the outcome generation of the \ac{dgp}, consider the arbitrary input vector $M = (M_1, M_2, M_3, M_4)'$ and let the true \ac{or} and propensity score-based \ac{ipw} model be defined as follows:
\begin{align}
    f_{\text{or}}(M) &= 210 + 27.4 \cdot M_1 + 13.7 \cdot (M_2 + M_3 + M_4), \\
    f_{\text{ps}}(M) &= 0.75 \cdot (-M_1 + 0.5 \cdot M_2 - 0.25 \cdot M_3 - 0.1 \cdot M_4).
\end{align}
Note a selection bias is constructed within this data \citep{kang2007demystifying} such that naive estimators are likely to be biased.
As $M$ is arbitrary, \citet{kang2007demystifying} introduces two variations of covariates $Z$ and $X$ that are in use in the simulations.
$Z$ is a set of observable variables, while $X$ is a set of unobservable variables.
In this simulation study, $f_{\text{or}}(M)$ and $f_{\text{ps}}(M)$ are constructed only by $Z$, only by $X$, or a combination of both.
Thus there are four different \ac{dgp} setups labeled as DGP1, DGP2, DGP3, and DGP4.
These four setups differ because $Z$ is a non-linear transformation of $X$.

Consider $\mathbf{X} = (X_1, X_2, X_3, X_4)'$ distributed as $N(0, I_4)$, where $I_4$ is the $4 \times 4$ identity matrix.
For $j = 1, 2, 3, 4$, \citet{kang2007demystifying} define the following variations of $Z_j = \frac{\tilde{Z}_j - \mathbb{E}[\tilde{Z}_j]}{\sqrt{\text{Var}(\tilde{Z}_j)}}$ where:
\begin{align} \nonumber
\tilde{Z}_1 &= \exp(0.5X_1), \\ \nonumber
\tilde{Z}_2 &= 10 + \frac{X_2}{1 + \exp(X_1)}, \\
\tilde{Z}_3 &= (0.6 + \frac{X_1 X_3}{25})^3, \quad \text{and} \\ \nonumber
\tilde{Z}_4 &= (20 + X_2 + X_4)^2.  \nonumber
\label{eq:13}
\end{align}
Each variation of $Z$ differs by their functional form as they are quadratic, exponential, and cubic.
They also include variations of interactions of $X$.
This complexity in the functional form of $Z$ is added to invoke potential biases when estimating the \ac{att}.
For example, when the true \ac{dgp} are based on $X$ but the model estimates based on $Z$, then the estimates are likely to be biased.
As we can construct either the \ac{or} or \ac{ipw} model based on $Z$, $X$, or a combination of both, we have four different setups.
The setup for each \ac{dgp} is presented below, indicating which model is correctly specified and which is not.
\begin{multicols}{2}

\textbf{DGP1} \\
\small (IPW and OR models correct)
\begin{align*}
    Y_0(0) &= f_{\text{or}}(Z) + \nu(Z, D) + \epsilon_0, \\
    Y_1(d) &= 2 \cdot f_{\text{or}}(Z) + \nu(Z, D) + \epsilon_1(d) \\
    p(Z) &= \frac{\exp \left( f_{\text{ps}}(Z) \right)}{1 + \exp \left( f_{\text{ps}}(Z) \right)}, \\
    D &= 1\{ p(Z) \geq U \};
\end{align*}

\textbf{DGP2}\\
\small (IPW model incorrect, OR correct)
\begin{align*}
    Y_0(0) &= f_{\text{or}}(Z) + \nu(Z, D) + \epsilon_0, \\
    Y_1(d) &= 2 \cdot f_{\text{or}}(Z) + \nu(Z, D) + \epsilon_1(d) \\
    p(X) &= \frac{\exp \left( f_{\text{ps}}(X) \right)}{1 + \exp \left( f_{\text{ps}}(X) \right)}, \\
    D &= 1\{ p(X) \geq U \};
\end{align*}

\columnbreak

\textbf{DGP3}\\
\small (IPW model correct, OR incorrect)
\begin{align*}
    Y_0(0) &= f_{\text{or}}(X) + \nu(X, D) + \epsilon_0, \\
    Y_1(d) &= 2 \cdot f_{\text{or}}(X) + \nu(X, D) + \epsilon_1(d) \\
    p(Z) &= \frac{\exp \left( f_{\text{ps}}(Z) \right)}{1 + \exp \left( f_{\text{ps}}(Z) \right)}, \\
    D &= 1\{ p(X) \geq U \};
\end{align*}

\textbf{DGP4 } \\
\small (IPW and OR models incorrect)
\begin{align*}
    Y_0(0) &= f_{\text{or}}(X) + \nu(X, D) + \epsilon_0, \\
    Y_1(d) &= 2 \cdot f_{\text{or}}(X) + \nu(X, D) + \epsilon_1(d) \\
    p(X) &= \frac{\exp \left( f_{\text{ps}}(X) \right)}{1 + \exp \left( f_{\text{ps}}(X) \right)}, \\
    D &= 1\{ p(X) \geq U \};
\end{align*}

\end{multicols}

\subsection{Homogenous Treatment Effects Simulation}

\input{./graphs/table_low_dim}

In this section, I present the results of the Monte Carlo simulations for the homogenous treatment effects case.
Table \ref{tab:table1} and Table \ref{tab:table2} report the average bias, median bias, root mean squared error, and variance of the estimators.
$\hat{\tau}^{corr}$ are the results of the correctly specified \ac{twfe} from Equation \ref{eq:twfecorr}, which can be interpreted as a baseline for the other estimators.
$\hat{\tau}^{fe}$ is the naive \ac{twfe} estimator from Equation \ref{eq:twfe}, as argued, the estimator is highly biased because the naive selection of controls does not reflect the underlying function of the data.
Note that the coverage probabilities of the \ac{twfe} estimator across \ac{dgp}s is almost zero.
$\hat{\tau}^{ipw}$ and $\hat{\tau}^{dr}$ are the results of the \ac{ipw} and \ac{drdid} estimators, respectively.
In both cases are the propensity scores estimated with a logistic regression.
The results of $\hat{\tau}^{fe}$, $\hat{\tau}^{ipw}$, and $\hat{\tau}^{dr}$ are directly comparable to the results of \citet{santannaDoublyRobustDifferenceindifferences2020} panel data case.
$\hat{\tau}^{ipw,dl}$ and $\hat{\tau}^{dr,dl}$ are the results of the \ac{ipw} and \ac{drdid} estimators, respectively, where the propensity scores are estimated with a neural network.
Note that the \ac{or} part of $\hat{\tau}^{dr,dl}$ is estimated as a linear model.

In DGP1, all estimators exhibit relatively small biases, except for the naive \ac{twfe} estimator.
Additionally, the coverage is quite high, with the deep learning applications even reaching 1.
This is most likely due to inflated confidence interval length \citep{farrellDeepNeuralNetworks2021}.
The results are consistent with the theory, as DGP1 marks the case where the \ac{ipw} and \ac{or} models are correctly specified.
In DGP2, the propensity score approach is misspecified such that $\hat{\tau}^{ipw}$ and $\hat{\tau}^{ipw,dl}$ are biased but the bias for the  $\hat{\tau}^{ipw,dl}$ is substantial.
Possible reasons could be overfitting or the prediction of extreme propensity scores.
The \ac{ipw} model approach generally produces high variance, which is consistent with \citet{santannaDoublyRobustDifferenceindifferences2020}.
This high variance also appears in other data structures, such as repeated cross-sections \citep{santannaDoublyRobustDifferenceindifferences2020, manfeDifferenceInDifferenceDesignRepeated}.
On the other hand, in DGP3 one can see that all estimators are relatively unbiased, except for the naive \ac{twfe} estimator as before.
These results are consistent as DGP3 marks the case where the \ac{ipw} model is correctly specified.
In DGP1-3 are both \ac{drdid} estimators $\hat{\tau}^{dr}$ and $\hat{\tau}^{dr,dl}$ relatively unbiased and produce low variance.
Notably, the classic $\hat{\tau}^{dr}$ of \citet{santannaDoublyRobustDifferenceindifferences2020} does perform slightly better.

DGP4 is the most challenging but probably most realistic case as both the \ac{ipw} and \ac{or} models are misspecified.
One can see clearly that all estimators are now more biased and have higher variance.
Surprisingly, the $\hat{\tau}^{dr,dl}$ estimator reports the smallest bias and relatively low variance compared to the other estimators.
This result is consistent with the findings of \citet{belloni2017program,chernozhukovDoubleDebiasedMachine2018,farrellDeepNeuralNetworks2021} that deep learning is useful to recover the true treatment effect if there is a nuisance in the data.



Overall, the results seem to be consistent with the findings of the literature on deep learning and \ac{did} estimation.
It should be noted that the biases of the deep learning estimators are relatively similar distributed within each \ac{dgp}.
This indicates that the deep learning model results across Monte Carlo runs are consistent and not heavily driven by outliers.
The results also mirror a structural aspect of deep learning that especially when using regularization methods, they are prone to produce symmetrically distributed errors around zero \citep{koh2017understanding}.

To evaluate if the deep learning model is robust, I report the minimum loss of the training and validation set in Table \ref{tab:table3}.
Note that I implemented one model and applied it on all \ac{dgp} setups, such that the results are comparable.
Across all \ac{dgp} setups the deep learning model reports similar losses, indicating that the model is robust across the different \ac{dgp}s.
Importantly, across all setups is the validation loss smaller than the training loss, indicating that the model is not overfitting \citep[see][]{Goodfellow-et-al-2016,farrellDeepNeuralNetworks2021}.


%discuss consistency of results with other papers sant anna
%bad performance of abadie dl. Discuss!
%dl performs worse except in the case of dgp4. Discuss!
%why are the biases of dl so similiar distributed? Discuss!
\input{./graphs/table_loss_report}


\subsection{Heterogeneous Treatment Effects Simulation}
In the previous section, I outlined the advantage of deep learning, or machine learning in general, when dealing with heterogeneous treatment effects.
The problem of heterogeneous treatment effect arises when the treatment effect $\theta(X)$ varies across groups \citep{hansen2022econometrics}.
To validate how the aforementioned estimators perform under heterogeneous treatment effects, I introduce heterogeneity to the DGP4.
DGP4 is the most general and possibly the most realistic case of the observed \ac{dgp}s.
The main difference to the DGP4 with homogeneous treatment effects is the introduction of $\theta(X)$, which directly influences the outcome $Y_1(d)$ depending on the value of $X$.
DGP4 can therefore be rewritten as follows:
\\
\textbf{DGP4 with Heterogeneous Treatment Effects}
\begin{align*}
    Y_0(0) &= f_{\text{or}}(X) + \nu(X) + \epsilon_0, \\
    Y_1(d) &= 2 \cdot f_{\text{or}}(X) + \nu(X) + \theta(X) \cdot d + \epsilon_1(d), \\
    p(X) &= \frac{\exp \left( f_{\text{ps}}(X) \right)}{1 + \exp \left( f_{\text{ps}}(X) \right)}, \\
    D &= 1\{ p(X) \geq U \},
\end{align*}
where: $\theta(X) = 10 \cdot (Z_1 + Z_2 - Z_3 + Z_4)$.\\
\input{./graphs/table_het_dgp4}
The results of the Monte Carlo simulations with heterogeneous treatment effects are presented in Table \ref{tab:table2}.
The estimators $\hat{\tau}^{fe}$,$\hat{\tau}^{corr}$,$\hat{\tau}^{ipw}$, and $\hat{\tau}^{dr}$ are now more biased and have higher variance compared to the homogenous treatment effects case.
These results are consistent with the literature as these methods do not account for heterogeneity \citep{hansen2022econometrics}.
\citet{manfeDifferenceInDifferenceDesignRepeated} reports similar more biased results for the \ac{ipw} and \ac{drdid} estimator in the case of repeated cross-sectional data.
The $\hat{\tau}^{ipw,dl}$ also reports higher bias and variance compared to the homogenous treatment effects case.
But the $\hat{\tau}^{ipw,dl}$ is now overall less biased and has lower variance than the comparable $\hat{\tau}^{ipw}$.
The same is true for the $\hat{\tau}^{dr,dl}$, which reports the smallest bias and variance across all estimators.
These results are interesting as they indicate that neural networks are more robust towards covariate-specific trends and heterogeneous treatment effects than comparable estimators viewed in this thesis.
This is consistent with the findings of \citet{farrellDeepNeuralNetworks2021} and \citet{chernozhukovDoubleDebiasedMachine2018}.

\subsection{Comparison of Deep Learning Architectures}

The choice of the correct neural network is generally arbitrary as discussed in Section 3.
This is due to the chosen activation function, neural network class, or hyperparameters.
Table \ref{tab:nn} shows the influence of different hyperparameters on the \ac{dgp} used in Section 4.3.
Note that the activation function is \ac{relu} and the neural network is a feedforward neural network across all architectures.

In table \ref{tab:nn} one can see, that there is no strictly better alternative architecture when comparing these neural network classes.
Every change in the hyperparameters comes to the cost of either higher bias or higher variance, which is a common trade-off in machine learning.
For example, the architecture used throughout the thesis has a higher bias compared to the first variation, which has more hidden layers and units.
This comes at the cost of higher variance.
Interestingly, there is no clear sign that deeper neural networks (with more units and layers) impose better results than shallower networks.
Generally, deeper neural networks come with higher computational costs, which can be extensive, especially in the case of complex and large data.

Additionally, one can see that the first variation and the last variation have a bigger validation loss than training loss.
The neural networks 4 and 5 report very small differences between the losses.
As argued, a higher validation loss compared to the training loss indicates that the model is overfitting.
For practitioners, it might be useful to use the results of the loss to evaluate which neural networks to select.
\input{./graphs/table_nn}


%here
Observing the coverage probability in Table \ref{tab:nn}, one can see again the extreme coverage of 1 in all but two cases.
The exemptions are variations 4 and 6 which report a coverage probability of 0.84 and 0.874, respectively.
Variation 4 reports the lowest learning rate and variation 6 is equal to the architecture used throughout the thesis but with no regularization imposed.
The learning rate of variation 4 is comparable to \citet{farrellDeepNeuralNetworks2021} which also report very high coverage probabilities but not as extreme as 1.
\citet{farrellDeepNeuralNetworks2021} argue that regularization can lead to extreme coverage probabilities but it is not clear how large the impact is.
The results of the thesis application and variation 6 suggest that imposing regularization might lead to extreme coverage probabilities.
Variation 4 has a similar regularization as the other variations but a smaller learning rate, which could hint that learning rate and regularization affect the distribution of estimates and thus the coverage probabilities.
The results emphasize the effect of hyperparameter selection on the model outcome and imply being cautious when imposing regularization within neural networks as \citet{farrellDeepNeuralNetworks2021} suggest.

\begin{figure}[h]
\centering
\caption{Conditional Average Treatment on the Treated Effekt without Regularization}
\includegraphics[width=\textwidth]{atte_bounds_noreg}
\label{fig:atte_bounds_noreg}
\end{figure}
%regularization preach cautios
%say something theoretically about learning rates here
To better understand the extreme coverage probabilities reported in this thesis, one can observe Figure \ref{fig:atte_bounds_noreg}.
It shows for 1000 runs of the \ac{mcs} the estimated conditional \ac{att} of variation 6 without regularization.
The highest density of the distribution is around 0, which is the "true" effect.
In Appendix Figure \ref{fig:atte_bounds} one can see the results of the application used throughout the thesis.
The estimated conditional \ac{att} never reaches the average upper or lower bound, thus indicating the extreme coverage probabilities of 1.
The distribution of the estimates is very dense and narrow as comparable to the results of \citet{farrellDeepNeuralNetworks2021}.
Note that the application used throughout the thesis does not recover the true treatment effect even though reporting much lower bias and variance than variation 6.

These results show that there is still no clear understanding of how to select optimal deep learning models for inference.
The comparison of the neural network architectures even implies a strong sensitivity towards selected hyperparameters.
Especially the choice of regularization has a substantial effect on the simulation results.
For selecting suitable hyperparameters frameworks like TensorFlow\textsuperscript{\textregistered} offer grid search approaches that can help selecting hyperparameters for simulations but for observational data, the choice of optimal neural networks remains unclear.
