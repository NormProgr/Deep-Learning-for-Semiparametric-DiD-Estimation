\begin{table}[ht]
\centering

\begin{threeparttable}
\caption{Simulation Results across Neural Network Architectures}
\label{tab:nn}
\begin{tabular}{lrrrrrr}
    \toprule
\hline
\addlinespace
Metric\textbackslash Architecture & Classic &  1 &  2 &  3 &  4 &  5 \\
\addlinespace
\hline
\addlinespace
Avg Bias & -1.593 & -0.409 & -2.731 & -6.011 & -5.501 &-4.369 \\
Med Bias & -1.591 & -0.557 & -2.669 & -6.002 & -5.524 & -4.483\\
RMSE & 1.632 & 1.192 & 2.7739 &  6.012 &5.573 & 4.389 \\
Variance & 16.579 & 18.920 & 15.793 & 11.059 & 11.592 & 11.811 \\
Training Loss & 0.632 & 0.608 &0.627 &0.658 & 0.723 & 0.657 \\
Validation Loss & 0.625 & 0.612 &  0.618 & 0.656 & 0.722 & 0.651 \\
Avg Coverage Prob & 1.0 & 1.0 & 1.0 & 1.0 & 0.84 & 1.0 \\
\addlinespace
\hline
\addlinespace
Depth & 3 & 5 & 2 & 3 & 3 & 6 \\
Units & 32 & 64 & 16 & 128 & 16 & 128 \\
Learning Rate & 0.01 & 0.001 & 0.01 & 0.001 & 0.0001 & 0.001 \\
L2 Regularization & 0.01 & 0.001 & 0.01 & 0.1 & 0.01 & 0.01 \\
\hline
\end{tabular}
\begin{tablenotes}
    \item Notes: All networks use the \ac{relu} activation function. The classic architecture is the one used throughout the thesis. The other architectures are variations of the classic architecture with different hyperparameters. For example shows architecture 1 the following: the depth is 5 such there are 5 hidden layers, each hidden layer has 64 units (or \textit{neurons}). Each hidden layer applies a L2 regularization with value 0.001.
\end{tablenotes}
\end{threeparttable}
\end{table}
