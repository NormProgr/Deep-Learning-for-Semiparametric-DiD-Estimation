\begin{table}[ht]
\centering

\begin{threeparttable}
\caption{Simulation Results across Neural Network Architectures}
\label{tab:nn}
\begin{tabular}{lrrrrrr}
    \toprule
\hline
\addlinespace
Metric\textbackslash Architecture & Classic &  1 &  2 &  3 &  4 &  5 \\
\addlinespace
\hline
\addlinespace
Avg Bias & -1.5726 & -0.0419 & -2.7408 & -6.0963 & -6.1169 & -4.4494 \\
Med Bias & -1.6320 & -0.4206 & -2.6777 & -6.1525 & -6.2021 & -4.5586 \\
RMSE & 1.6130 & 1.4939 & 2.7739 & 6.0976 & 6.2239 & 4.4667 \\
Variance & 16.5906 & 20.9828 & 15.8893 & 10.9668 & 11.0447 & 11.7120 \\
Training Loss & 0.6316 & 0.6083 & 0.6270 & 0.6585 & 0.7217 & 0.6573 \\
Validation Loss & 0.6252 & 0.6209 & 0.6193 & 0.6554 & 0.7203 & 0.6513 \\
Avg Coverage Prob & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5000 & 1.0000 \\
\addlinespace
\hline
\addlinespace
Depth & 3 & 5 & 2 & 3 & 3 & 6 \\
Units & 32 & 64 & 16 & 128 & 16 & 128 \\
Learning Rate & 0.01 & 0.001 & 0.01 & 0.001 & 0.0001 & 0.001 \\
L2 Regularization & 0.01 & 0.001 & 0.01 & 0.1 & 0.01 & 0.01 \\
\hline
\end{tabular}
\begin{tablenotes}
    \item Notes: All networks use the \ac{relu} activation function. The classic architecture is the one used throughout the thesis. The other architectures are variations of the classic architecture with different hyperparameters. For example shows architecture 1 the following: the depth is 5 such there are 5 hidden layers, each hidden layer has 64 units (or \textit{neurons}). Each hidden layer applies a L2 regularization with value 0.001.
\end{tablenotes}
\end{threeparttable}
\end{table}
