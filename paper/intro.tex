\section{Introduction}



% Issues and importance of Inference and DIfference in Difference and how Deep Learning could tackle that

\ac{did} is a widely used econometric method to estimate the effect of a policy change on a group of individuals called treatment group.
To achieve this, the method requires to compare treatment group to a control group before and after the policy change.
The important underlying assumption is the \ac{pta}, which states that the treatment and control group would have developed similarly in the absence of the policy change.
This assumption is key to identify the effect on the treatment group, the \ac{att}, to be causal.

However in practice one does not know if the \ac{pta} holds as it is by design untestable.
If individuals are selected into treatment based on characteristics that also influence the outcome, the \ac{pta} is violated.
To overcome this issue researchers condition on these characteristics such that they assume conditional \ac{pta} \citep[see][]{santannaDoublyRobustDifferenceindifferences2020,manfeDifferenceInDifferenceDesignRepeated}

In this thesis I want to adress how researchers can use a more flexible semiparametric approach to achieve robust \ac{did} estimation under conditional \ac{pta}.
For this I consider a variety of machine and deep learning models to interchange the first-step in the \ac{did} estimation.
Especially the use of deep learning marks a novelty and I follow \citet{farrellDeepNeuralNetworks2021} to contribute to the young but growing literature.
% Importance of Deep Learning for Inference and for Economics



% say what the exact finding is in the following parts and keep the structure with First, Second, ... .
%
This thesis wants to contribute in four ways to the literature.
First, I want to discuss the current state of classical and machine learning techniques used for \ac{did} estimation.
For this I revise the classic \ac{did} estimation with \ac{twfe}, then I revise the semiparametric approaches such as \ac{or} \citep[see][]{heckmanMatchingEconometricEvaluation1998}, \ac{ipw} \citep[see][]{abadieSemiparametricDifferenceinDifferencesEstimators2005}, and \ac{drdid} \citep[see][]{santannaDoublyRobustDifferenceindifferences2020}.

Second, I introduce a new approach using deep learning for first step \ac{did} estimation.
As the literature is rather new I want to revise how deep neural networks work and why it can be valid for inference following the results of \citet{farrellDeepNeuralNetworks2021}.

Third, I want to provide a \ac{mcs} to compare the performance of all techniques mentioned earlier with deep learning variations of the \ac{ipw} and \ac{drdid} approach.
My simulation is close to the design of \citet{santannaDoublyRobustDifferenceindifferences2020} where we use the same data generating process for panel data that introduces four variations.
The variations differ whether the outcome was generated correctly by \ac{or}, \ac{ipw}, both or none.
I can show that deep learning is nearly as good as the best performing technique in the first three variations.
In the variation where the outcome was not generated correctly by any technique, \ac{drdid} deep learning outperforms the other techniques.
Additionally, I introduce heterogeneity in treatment for the incorrectly specified case and show that \ac{drdid} deep learning case is the best performing technique.
%explain more
Lastly, I want to apply these techniques to a real-world dataset of \citet{favaraCreditSupplyPrice2015} to show the potential of the \ac{drdid} deep learning estimator. %explain more
The rest of the paper is structured as follows. In Section I introduces the methodology of all techniques used in this thesis.
Section II introduces deep learning in general and its use for inference.
Section III introduces the \ac{mcs} and its results.
Section V applies the techniques to a real-world dataset.
Section VI concludes the paper.

% Organization of the paper
