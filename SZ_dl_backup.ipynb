{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.special import expit as logistic_cdf\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define the DRDID function\n",
    "\n",
    "\n",
    "def create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(units, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    for _ in range(depth - 2):\n",
    "        x = Dense(units, kernel_regularizer=l2(l2_reg))(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with Adam optimizer and specified learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def drdid_rc(\n",
    "    y,\n",
    "    post,\n",
    "    D,\n",
    "    covariates=None,\n",
    "    i_weights=None,\n",
    "    boot=False,\n",
    "    boot_type=\"weighted\",\n",
    "    nboot=None,\n",
    "    inffunc=False,\n",
    "):\n",
    "    # Ensure D is a vector\n",
    "    D = np.asarray(D)\n",
    "    # Sample size\n",
    "    n = len(D)\n",
    "    # Ensure y is a vector\n",
    "    y = np.asarray(y)\n",
    "    # Ensure post is a vector\n",
    "    post = np.asarray(post)\n",
    "    # Add constant to covariate vector\n",
    "    int_cov = np.ones((n, 1))\n",
    "    if covariates is not None:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "    elif np.min(i_weights) < 0:\n",
    "        msg = \"i.weights must be non-negative\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # Define parameters for the neural network\n",
    "    depth = 3\n",
    "    units = 32\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.01\n",
    "    input_dim = int_cov.shape[1]\n",
    "\n",
    "    # Create and compile the model with the optimal hyperparameters\n",
    "    model = create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(int_cov, D, sample_weight=i_weights, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "    # Predict the probabilities\n",
    "    ps_fit = model.predict(int_cov).flatten()\n",
    "\n",
    "    # Ensure no values are exactly 0 or 1\n",
    "    ps_fit = np.clip(ps_fit, 1e-8, 1 - 1e-8)\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the pre-treatment period, using OLS\n",
    "    reg_cont_pre = sm.WLS(\n",
    "        y[(D == 0) & (post == 0)],\n",
    "        int_cov[(D == 0) & (post == 0)],\n",
    "        weights=i_weights[(D == 0) & (post == 0)],\n",
    "    ).fit()\n",
    "    if np.any(np.isnan(reg_cont_pre.params)):\n",
    "        msg = \"Outcome regression model coefficients have NA components. Multicollinearity (or lack of variation) of covariates is a likely reason.\"\n",
    "        raise ValueError(\n",
    "            msg,\n",
    "        )\n",
    "    out_y_cont_pre = int_cov @ reg_cont_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the post-treatment period, using OLS\n",
    "    reg_cont_post = sm.WLS(\n",
    "        y[(D == 0) & (post == 1)],\n",
    "        int_cov[(D == 0) & (post == 1)],\n",
    "        weights=i_weights[(D == 0) & (post == 1)],\n",
    "    ).fit()\n",
    "    if np.any(np.isnan(reg_cont_post.params)):\n",
    "        msg = \"Outcome regression model coefficients have NA components. Multicollinearity (or lack of variation) of covariates is a likely reason.\"\n",
    "        raise ValueError(\n",
    "            msg,\n",
    "        )\n",
    "    out_y_cont_post = int_cov @ reg_cont_post.params\n",
    "\n",
    "    # Combine the ORs for control group\n",
    "    out_y_cont = post * out_y_cont_post + (1 - post) * out_y_cont_pre\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the pre-treatment period, using OLS\n",
    "    reg_treat_pre = sm.WLS(\n",
    "        y[(D == 1) & (post == 0)],\n",
    "        int_cov[(D == 1) & (post == 0)],\n",
    "        weights=i_weights[(D == 1) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_treat_pre = int_cov @ reg_treat_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the post-treatment period, using OLS\n",
    "    reg_treat_post = sm.WLS(\n",
    "        y[(D == 1) & (post == 1)],\n",
    "        int_cov[(D == 1) & (post == 1)],\n",
    "        weights=i_weights[(D == 1) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_treat_post = int_cov @ reg_treat_post.params\n",
    "\n",
    "    # Weights\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    w_d = i_weights * D\n",
    "    w_dt1 = i_weights * D * post\n",
    "    w_dt0 = i_weights * D * (1 - post)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * (y - out_y_cont) / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * (y - out_y_cont) / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * (y - out_y_cont) / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * (y - out_y_cont) / np.mean(w_cont_post)\n",
    "\n",
    "    # Extra elements for the locally efficient DRDID\n",
    "    eta_d_post = w_d * (out_y_treat_post - out_y_cont_post) / np.mean(w_d)\n",
    "    eta_dt1_post = w_dt1 * (out_y_treat_post - out_y_cont_post) / np.mean(w_dt1)\n",
    "    eta_d_pre = w_d * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_d)\n",
    "    eta_dt0_pre = w_dt0 * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_dt0)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    att_d_post = np.mean(eta_d_post)\n",
    "    att_dt1_post = np.mean(eta_dt1_post)\n",
    "    att_d_pre = np.mean(eta_d_pre)\n",
    "    att_dt0_pre = np.mean(eta_dt0_pre)\n",
    "\n",
    "    # ATT estimator\n",
    "    dr_att = (\n",
    "        (att_treat_post - att_treat_pre)\n",
    "        - (att_cont_post - att_cont_pre)\n",
    "        + (att_d_post - att_dt1_post)\n",
    "        - (att_d_pre - att_dt0_pre)\n",
    "    )\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    # Leading term of the influence function: no estimation effect\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M1_post = -np.mean(\n",
    "        w_treat_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_post)\n",
    "    M1_pre = -np.mean(\n",
    "        w_treat_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_treat_or_post = np.dot(reg_cont_post.cov_params(), M1_post)\n",
    "    inf_treat_or_pre = np.dot(reg_cont_pre.cov_params(), M1_pre)\n",
    "    inf_treat_or = inf_treat_or_post + inf_treat_or_pre\n",
    "\n",
    "    # Influence function for the treated component\n",
    "    inf_treat = inf_treat_post - inf_treat_pre + np.sum(inf_treat_or)\n",
    "\n",
    "    # Now, get the influence function of control component\n",
    "    # Leading term of the influence function: no estimation effect from nuisance parameters\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "\n",
    "    # Estimation effect from gamma hat (pscore)\n",
    "    np.mean(\n",
    "        w_cont_pre[:, np.newaxis]\n",
    "        * (y[:, np.newaxis] - out_y_cont[:, np.newaxis] - att_cont_pre)\n",
    "        * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_pre)\n",
    "    np.mean(\n",
    "        w_cont_post[:, np.newaxis]\n",
    "        * (y[:, np.newaxis] - out_y_cont[:, np.newaxis] - att_cont_post)\n",
    "        * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_post)\n",
    "\n",
    "    # Influence function for the control component\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Get the influence function of the inefficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func1 = inf_treat - inf_cont\n",
    "\n",
    "    # Now, we only need to get the influence function of the adjustment terms\n",
    "    # First, the terms as if all OR parameters were known\n",
    "    inf_eff1 = eta_d_post - w_d * att_d_post / np.mean(w_d)\n",
    "    inf_eff2 = eta_dt1_post - w_dt1 * att_dt1_post / np.mean(w_dt1)\n",
    "    inf_eff3 = eta_d_pre - w_d * att_d_pre / np.mean(w_d)\n",
    "    inf_eff4 = eta_dt0_pre - w_dt0 * att_dt0_pre / np.mean(w_dt0)\n",
    "    inf_eff = (inf_eff1 - inf_eff2) - (inf_eff3 - inf_eff4)\n",
    "\n",
    "    # Now the estimation effect of the OR coefficients\n",
    "    mom_post = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt1 / np.mean(w_dt1))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    mom_pre = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt0 / np.mean(w_dt0))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    inf_or_post = np.dot(\n",
    "        (reg_treat_post.cov_params() - reg_cont_post.cov_params()),\n",
    "        mom_post,\n",
    "    )\n",
    "    inf_or_pre = np.dot(\n",
    "        (reg_treat_pre.cov_params() - reg_cont_pre.cov_params()),\n",
    "        mom_pre,\n",
    "    )\n",
    "    inf_or = inf_or_post - inf_or_pre\n",
    "    inf_or = np.sum(inf_or)\n",
    "\n",
    "    # Get the influence function of the locally efficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func = dr_att_inf_func1 + inf_eff + inf_or\n",
    "\n",
    "    if not boot:\n",
    "        # Estimate of standard error\n",
    "        se_dr_att = np.std(dr_att_inf_func) / np.sqrt(n)\n",
    "        # Estimate of upper boundary of 95% CI\n",
    "        uci = dr_att + 1.96 * se_dr_att\n",
    "        # Estimate of lower boundary of 95% CI\n",
    "        lci = dr_att - 1.96 * se_dr_att\n",
    "        # Create this null vector so we can export the bootstrap draws too.\n",
    "        dr_boot = None\n",
    "    else:\n",
    "        if nboot is None:\n",
    "            nboot = 999\n",
    "        if boot_type == \"multiplier\":\n",
    "            # Do multiplier bootstrap\n",
    "            dr_boot = mboot_did(dr_att_inf_func, nboot)\n",
    "            # Get bootstrap std errors based on IQR\n",
    "            se_dr_att = np.percentile(dr_boot, 75) - np.percentile(dr_boot, 25)\n",
    "            # Get symmetric critical values\n",
    "            cv = np.percentile(np.abs(dr_boot / se_dr_att), 95)\n",
    "            # Estimate of upper boundary of 95% CI\n",
    "            uci = dr_att + cv * se_dr_att\n",
    "            # Estimate of lower boundary of 95% CI\n",
    "            lci = dr_att - cv * se_dr_att\n",
    "        else:\n",
    "            # Do weighted bootstrap\n",
    "            dr_boot = [\n",
    "                wboot_drdid_rc(n, y, post, D, int_cov, i_weights) for _ in range(nboot)\n",
    "            ]\n",
    "            # Get bootstrap std errors based on IQR\n",
    "            se_dr_att = np.percentile(dr_boot - dr_att, 75) - np.percentile(\n",
    "                dr_boot - dr_att,\n",
    "                25,\n",
    "            )\n",
    "            # Get symmetric critical values\n",
    "            cv = np.percentile(np.abs((dr_boot - dr_att) / se_dr_att), 95)\n",
    "            # Estimate of upper boundary of 95% CI\n",
    "            uci = dr_att + cv * se_dr_att\n",
    "            # Estimate of lower boundary of 95% CI\n",
    "            lci = dr_att - cv * se_dr_att\n",
    "\n",
    "    if not inffunc:\n",
    "        dr_att_inf_func = None\n",
    "\n",
    "    return {\n",
    "        \"ATT\": dr_att,\n",
    "        \"se\": se_dr_att,\n",
    "        \"uci\": uci,\n",
    "        \"lci\": lci,\n",
    "        \"boots\": dr_boot,\n",
    "        \"att_inf_func\": dr_att_inf_func,\n",
    "        \"call_param\": None,\n",
    "        \"argu\": {\n",
    "            \"panel\": False,\n",
    "            \"estMethod\": \"trad\",\n",
    "            \"boot\": boot,\n",
    "            \"boot_type\": boot_type,\n",
    "            \"nboot\": nboot,\n",
    "            \"type\": \"dr\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "n = 1000  # Sample size\n",
    "Xsi_ps = 0.75  # pscore index\n",
    "_lambda = 0.5  # Proportion in each period\n",
    "\n",
    "# Define means and standard deviations\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "\n",
    "# Loop for 1000 runs\n",
    "for _i in range(10):\n",
    "    # Generate covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    # Propensity score\n",
    "    pi = logistic_cdf(Xsi_ps * (-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4))\n",
    "    d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "    index_unobs_het = d * index_lin\n",
    "    index_att = 0\n",
    "    index_trend = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "    # Generate unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "    # Generate outcomes at time 0 and time 1\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "    y01 = index_lin + v + np.random.normal(scale=1, size=n) + index_trend\n",
    "    y11 = index_lin + v + np.random.normal(scale=1, size=n) + index_trend + index_att\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "    y = np.where(\n",
    "        d & post,\n",
    "        y11,\n",
    "        np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "    )\n",
    "\n",
    "    # Generate id\n",
    "    id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "    time = np.tile([0, 1], n)\n",
    "\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"time\": time,\n",
    "            \"y\": np.tile(y, 2),\n",
    "            \"post\": np.tile(post.astype(int), 2),\n",
    "            \"d\": np.tile(d.astype(int), 2),\n",
    "            \"x1\": np.tile(z1, 2),\n",
    "            \"x2\": np.tile(z2, 2),\n",
    "            \"x3\": np.tile(z3, 2),\n",
    "            \"x4\": np.tile(z4, 2),\n",
    "        },\n",
    "    )\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "    dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "    # Run the IPW-DID estimator\n",
    "    covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "    y = dta_long[\"y\"].values\n",
    "    post = dta_long[\"post\"].values\n",
    "    D = dta_long[\"d\"].values\n",
    "\n",
    "    result = drdid_rc(y, post, D, covariates)\n",
    "\n",
    "    ATTE_estimates.append(result[\"ATT\"])\n",
    "    asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "\n",
    "# Calculate average bias, median bias, and RMSE\n",
    "true_ATT = 0\n",
    "average_bias = np.mean(ATTE_estimates) - true_ATT\n",
    "median_bias = np.median(ATTE_estimates) - true_ATT\n",
    "rmse = np.sqrt(np.mean((np.array(ATTE_estimates) - true_ATT) ** 2))\n",
    "\n",
    "# Calculate average of the variance\n",
    "average_variance = np.mean(asymptotic_variance)\n",
    "\n",
    "print(\"Average Bias:\", average_bias)\n",
    "print(\"Median Bias:\", median_bias)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"Average Variance:\", average_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the DRDID function\n",
    "\n",
    "\n",
    "def create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(units, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    for _ in range(depth - 2):\n",
    "        x = Dense(units, kernel_regularizer=l2(l2_reg))(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with Adam optimizer and specified learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def drdid_rc(y, post, D, covariates=None, i_weights=None):\n",
    "    # Ensure D is a vector\n",
    "    D = np.asarray(D)\n",
    "    # Sample size\n",
    "    n = len(D)\n",
    "    # Ensure y is a vector\n",
    "    y = np.asarray(y)\n",
    "    # Ensure post is a vector\n",
    "    post = np.asarray(post)\n",
    "    # Add constant to covariate vector\n",
    "    int_cov = np.ones((n, 1))\n",
    "    if covariates is not None:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "\n",
    "    # Define parameters for the neural network\n",
    "    depth = 3\n",
    "    units = 32\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.01\n",
    "    input_dim = int_cov.shape[1]\n",
    "\n",
    "    # Create and compile the model with the optimal hyperparameters\n",
    "    model = create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(int_cov, D, sample_weight=i_weights, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict the probabilities\n",
    "    ps_fit = model.predict(int_cov).flatten()\n",
    "\n",
    "    # Ensure no values are exactly 0 or 1\n",
    "    ps_fit = np.clip(ps_fit, 1e-8, 1 - 1e-8)\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the pre-treatment period, using OLS\n",
    "    reg_cont_pre = sm.WLS(\n",
    "        y[(D == 0) & (post == 0)],\n",
    "        int_cov[(D == 0) & (post == 0)],\n",
    "        weights=i_weights[(D == 0) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_cont_pre = int_cov @ reg_cont_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the post-treatment period, using OLS\n",
    "    reg_cont_post = sm.WLS(\n",
    "        y[(D == 0) & (post == 1)],\n",
    "        int_cov[(D == 0) & (post == 1)],\n",
    "        weights=i_weights[(D == 0) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_cont_post = int_cov @ reg_cont_post.params\n",
    "\n",
    "    # Combine the ORs for control group\n",
    "    out_y_cont = post * out_y_cont_post + (1 - post) * out_y_cont_pre\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the pre-treatment period, using OLS\n",
    "    reg_treat_pre = sm.WLS(\n",
    "        y[(D == 1) & (post == 0)],\n",
    "        int_cov[(D == 1) & (post == 0)],\n",
    "        weights=i_weights[(D == 1) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_treat_pre = int_cov @ reg_treat_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the post-treatment period, using OLS\n",
    "    reg_treat_post = sm.WLS(\n",
    "        y[(D == 1) & (post == 1)],\n",
    "        int_cov[(D == 1) & (post == 1)],\n",
    "        weights=i_weights[(D == 1) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_treat_post = int_cov @ reg_treat_post.params\n",
    "\n",
    "    # Weights\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    w_d = i_weights * D\n",
    "    w_dt1 = i_weights * D * post\n",
    "    w_dt0 = i_weights * D * (1 - post)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * (y - out_y_cont) / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * (y - out_y_cont) / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * (y - out_y_cont) / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * (y - out_y_cont) / np.mean(w_cont_post)\n",
    "\n",
    "    # Extra elements for the locally efficient DRDID\n",
    "    eta_d_post = w_d * (out_y_treat_post - out_y_cont_post) / np.mean(w_d)\n",
    "    eta_dt1_post = w_dt1 * (out_y_treat_post - out_y_cont_post) / np.mean(w_dt1)\n",
    "    eta_d_pre = w_d * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_d)\n",
    "    eta_dt0_pre = w_dt0 * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_dt0)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    att_d_post = np.mean(eta_d_post)\n",
    "    att_dt1_post = np.mean(eta_dt1_post)\n",
    "    att_d_pre = np.mean(eta_d_pre)\n",
    "    att_dt0_pre = np.mean(eta_dt0_pre)\n",
    "\n",
    "    # ATT estimator\n",
    "    dr_att = (\n",
    "        (att_treat_post - att_treat_pre)\n",
    "        - (att_cont_post - att_cont_pre)\n",
    "        + (att_d_post - att_dt1_post)\n",
    "        - (att_d_pre - att_dt0_pre)\n",
    "    )\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    # Leading term of the influence function: no estimation effect\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M1_post = -np.mean(\n",
    "        w_treat_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_post)\n",
    "    M1_pre = -np.mean(\n",
    "        w_treat_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_treat_or_post = np.dot(reg_cont_post.cov_params(), M1_post)\n",
    "    inf_treat_or_pre = np.dot(reg_cont_pre.cov_params(), M1_pre)\n",
    "    inf_treat_or = inf_treat_or_post + inf_treat_or_pre\n",
    "\n",
    "    # Influence function for the treated component\n",
    "    inf_treat = inf_treat_post - inf_treat_pre + np.sum(inf_treat_or)\n",
    "\n",
    "    # Now, get the influence function of control component\n",
    "    # Leading term of the influence function: no estimation effect from nuisance parameters\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "\n",
    "    # Influence function for the control component\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Get the influence function of the inefficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func1 = inf_treat - inf_cont\n",
    "\n",
    "    # Now, we only need to get the influence function of the adjustment terms\n",
    "    # First, the terms as if all OR parameters were known\n",
    "    inf_eff1 = eta_d_post - w_d * att_d_post / np.mean(w_d)\n",
    "    inf_eff2 = eta_dt1_post - w_dt1 * att_dt1_post / np.mean(w_dt1)\n",
    "    inf_eff3 = eta_d_pre - w_d * att_d_pre / np.mean(w_d)\n",
    "    inf_eff4 = eta_dt0_pre - w_dt0 * att_dt0_pre / np.mean(w_dt0)\n",
    "    inf_eff = (inf_eff1 - inf_eff2) - (inf_eff3 - inf_eff4)\n",
    "\n",
    "    # Now the estimation effect of the OR coefficients\n",
    "    mom_post = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt1 / np.mean(w_dt1))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    mom_pre = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt0 / np.mean(w_dt0))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    inf_or_post = np.dot(\n",
    "        (reg_treat_post.cov_params() - reg_cont_post.cov_params()),\n",
    "        mom_post,\n",
    "    )\n",
    "    inf_or_pre = np.dot(\n",
    "        (reg_treat_pre.cov_params() - reg_cont_pre.cov_params()),\n",
    "        mom_pre,\n",
    "    )\n",
    "    inf_or = inf_or_post - inf_or_pre\n",
    "    inf_or = np.sum(inf_or)\n",
    "\n",
    "    # Get the influence function of the locally efficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func = dr_att_inf_func1 + inf_eff + inf_or\n",
    "\n",
    "    # Estimate of standard error\n",
    "    se_dr_att = np.std(dr_att_inf_func) / np.sqrt(n)\n",
    "\n",
    "    return {\n",
    "        \"ATT\": dr_att,\n",
    "        \"se\": se_dr_att,\n",
    "    }\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "n = 1000  # Sample size\n",
    "Xsi_ps = 0.75  # pscore index\n",
    "\n",
    "# Define means and standard deviations\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "\n",
    "# Loop for 1000 runs\n",
    "for _i in range(10):\n",
    "    # Generate covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    # Propensity score\n",
    "    pi = logistic_cdf(Xsi_ps * (-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4))\n",
    "    d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "    index_unobs_het = d * index_lin\n",
    "    index_att = 0\n",
    "    index_trend = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "    # Generate unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "    # Generate outcomes at time 0 and time 1\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "    y01 = index_lin + v + np.random.normal(scale=1, size=n) + index_trend\n",
    "    y11 = index_lin + v + np.random.normal(scale=1, size=n) + index_trend + index_att\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "    y = np.where(\n",
    "        d & post,\n",
    "        y11,\n",
    "        np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "    )\n",
    "\n",
    "    # Generate id\n",
    "    id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "    time = np.tile([0, 1], n)\n",
    "\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"time\": time,\n",
    "            \"y\": np.tile(y, 2),\n",
    "            \"post\": np.tile(post.astype(int), 2),\n",
    "            \"d\": np.tile(d.astype(int), 2),\n",
    "            \"x1\": np.tile(z1, 2),\n",
    "            \"x2\": np.tile(z2, 2),\n",
    "            \"x3\": np.tile(z3, 2),\n",
    "            \"x4\": np.tile(z4, 2),\n",
    "        },\n",
    "    )\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "    dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "    # Run the IPW-DID estimator\n",
    "    covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "    y = dta_long[\"y\"].values\n",
    "    post = dta_long[\"post\"].values\n",
    "    D = dta_long[\"d\"].values\n",
    "\n",
    "    result = drdid_rc(y, post, D, covariates)\n",
    "\n",
    "    ATTE_estimates.append(result[\"ATT\"])\n",
    "    asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "\n",
    "# Calculate average bias, median bias, and RMSE\n",
    "true_ATT = 0\n",
    "average_bias = np.mean(ATTE_estimates) - true_ATT\n",
    "median_bias = np.median(ATTE_estimates) - true_ATT\n",
    "rmse = np.sqrt(np.mean((np.array(ATTE_estimates) - true_ATT) ** 2))\n",
    "\n",
    "# Calculate average of the variance\n",
    "average_variance = np.mean(asymptotic_variance)\n",
    "\n",
    "print(\"Average Bias:\", average_bias)\n",
    "print(\"Median Bias:\", median_bias)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"Average Variance:\", average_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP 1 SZ - Deep Learning\n",
    "The normal version not the improved one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.special import expit as logistic_cdf\n",
    "\n",
    "# Define the DRDID function\n",
    "\n",
    "\n",
    "def drdid_rc(\n",
    "    y,\n",
    "    post,\n",
    "    D,\n",
    "    covariates=None,\n",
    "    i_weights=None,\n",
    "    boot=False,\n",
    "    boot_type=\"weighted\",\n",
    "    nboot=None,\n",
    "    inffunc=False,\n",
    "):\n",
    "    # Ensure D is a vector\n",
    "    D = np.asarray(D)\n",
    "    # Sample size\n",
    "    n = len(D)\n",
    "    # Ensure y is a vector\n",
    "    y = np.asarray(y)\n",
    "    # Ensure post is a vector\n",
    "    post = np.asarray(post)\n",
    "    # Add constant to covariate vector\n",
    "    int_cov = np.ones((n, 1))\n",
    "    if covariates is not None:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "    elif np.min(i_weights) < 0:\n",
    "        msg = \"i.weights must be non-negative\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # Compute the Pscore by MLE\n",
    "    pscore_tr = sm.GLM(\n",
    "        D,\n",
    "        int_cov,\n",
    "        family=sm.families.Binomial(),\n",
    "        freq_weights=i_weights,\n",
    "    ).fit()\n",
    "    if not pscore_tr.converged:\n",
    "        warnings.warn(\"GLM algorithm did not converge\")\n",
    "    if np.any(np.isnan(pscore_tr.params)):\n",
    "        msg = \"Propensity score model coefficients have NA components. Multicollinearity (or lack of variation) of covariates is a likely reason.\"\n",
    "        raise ValueError(\n",
    "            msg,\n",
    "        )\n",
    "    ps_fit = pscore_tr.fittedvalues\n",
    "    # Avoid divide by zero\n",
    "    ps_fit = np.clip(ps_fit, 1e-16, 1 - 1e-16)\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the pre-treatment period, using OLS\n",
    "    reg_cont_pre = sm.WLS(\n",
    "        y[(D == 0) & (post == 0)],\n",
    "        int_cov[(D == 0) & (post == 0)],\n",
    "        weights=i_weights[(D == 0) & (post == 0)],\n",
    "    ).fit()\n",
    "    if np.any(np.isnan(reg_cont_pre.params)):\n",
    "        msg = \"Outcome regression model coefficients have NA components. Multicollinearity (or lack of variation) of covariates is a likely reason.\"\n",
    "        raise ValueError(\n",
    "            msg,\n",
    "        )\n",
    "    out_y_cont_pre = int_cov @ reg_cont_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the post-treatment period, using OLS\n",
    "    reg_cont_post = sm.WLS(\n",
    "        y[(D == 0) & (post == 1)],\n",
    "        int_cov[(D == 0) & (post == 1)],\n",
    "        weights=i_weights[(D == 0) & (post == 1)],\n",
    "    ).fit()\n",
    "    if np.any(np.isnan(reg_cont_post.params)):\n",
    "        msg = \"Outcome regression model coefficients have NA components. Multicollinearity (or lack of variation) of covariates is a likely reason.\"\n",
    "        raise ValueError(\n",
    "            msg,\n",
    "        )\n",
    "    out_y_cont_post = int_cov @ reg_cont_post.params\n",
    "\n",
    "    # Combine the ORs for control group\n",
    "    out_y_cont = post * out_y_cont_post + (1 - post) * out_y_cont_pre\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the pre-treatment period, using OLS\n",
    "    reg_treat_pre = sm.WLS(\n",
    "        y[(D == 1) & (post == 0)],\n",
    "        int_cov[(D == 1) & (post == 0)],\n",
    "        weights=i_weights[(D == 1) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_treat_pre = int_cov @ reg_treat_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the post-treatment period, using OLS\n",
    "    reg_treat_post = sm.WLS(\n",
    "        y[(D == 1) & (post == 1)],\n",
    "        int_cov[(D == 1) & (post == 1)],\n",
    "        weights=i_weights[(D == 1) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_treat_post = int_cov @ reg_treat_post.params\n",
    "\n",
    "    # Weights\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    w_d = i_weights * D\n",
    "    w_dt1 = i_weights * D * post\n",
    "    w_dt0 = i_weights * D * (1 - post)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * (y - out_y_cont) / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * (y - out_y_cont) / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * (y - out_y_cont) / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * (y - out_y_cont) / np.mean(w_cont_post)\n",
    "\n",
    "    # Extra elements for the locally efficient DRDID\n",
    "    eta_d_post = w_d * (out_y_treat_post - out_y_cont_post) / np.mean(w_d)\n",
    "    eta_dt1_post = w_dt1 * (out_y_treat_post - out_y_cont_post) / np.mean(w_dt1)\n",
    "    eta_d_pre = w_d * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_d)\n",
    "    eta_dt0_pre = w_dt0 * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_dt0)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    att_d_post = np.mean(eta_d_post)\n",
    "    att_dt1_post = np.mean(eta_dt1_post)\n",
    "    att_d_pre = np.mean(eta_d_pre)\n",
    "    att_dt0_pre = np.mean(eta_dt0_pre)\n",
    "\n",
    "    # ATT estimator\n",
    "    dr_att = (\n",
    "        (att_treat_post - att_treat_pre)\n",
    "        - (att_cont_post - att_cont_pre)\n",
    "        + (att_d_post - att_dt1_post)\n",
    "        - (att_d_pre - att_dt0_pre)\n",
    "    )\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    # Leading term of the influence function: no estimation effect\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M1_post = -np.mean(\n",
    "        w_treat_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_post)\n",
    "    M1_pre = -np.mean(\n",
    "        w_treat_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_treat_or_post = np.dot(reg_cont_post.cov_params(), M1_post)\n",
    "    inf_treat_or_pre = np.dot(reg_cont_pre.cov_params(), M1_pre)\n",
    "    inf_treat_or = inf_treat_or_post + inf_treat_or_pre\n",
    "\n",
    "    # Influence function for the treated component\n",
    "    inf_treat = inf_treat_post - inf_treat_pre + np.sum(inf_treat_or)\n",
    "\n",
    "    # Now, get the influence function of control component\n",
    "    # Leading term of the influence function: no estimation effect from nuisance parameters\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "\n",
    "    # Estimation effect from gamma hat (pscore)\n",
    "    M2_pre = np.mean(\n",
    "        w_cont_pre[:, np.newaxis]\n",
    "        * (y[:, np.newaxis] - out_y_cont[:, np.newaxis] - att_cont_pre)\n",
    "        * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_pre)\n",
    "    M2_post = np.mean(\n",
    "        w_cont_post[:, np.newaxis]\n",
    "        * (y[:, np.newaxis] - out_y_cont[:, np.newaxis] - att_cont_post)\n",
    "        * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_post)\n",
    "\n",
    "    # Now the influence function related to estimation effect of pscores\n",
    "    inf_cont_ps = np.dot(pscore_tr.cov_params(), (M2_post - M2_pre))\n",
    "    inf_cont_ps = np.sum(inf_cont_ps)\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M3_post = -np.mean(\n",
    "        w_cont_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_post)\n",
    "    M3_pre = -np.mean(\n",
    "        w_cont_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_cont_or_post = np.dot(reg_cont_post.cov_params(), M3_post)\n",
    "    inf_cont_or_pre = np.dot(reg_cont_pre.cov_params(), M3_pre)\n",
    "    inf_cont_or = inf_cont_or_post + inf_cont_or_pre\n",
    "    inf_cont_or = np.sum(inf_cont_or)\n",
    "\n",
    "    # Influence function for the control component\n",
    "    inf_cont = inf_cont_post - inf_cont_pre + inf_cont_ps + inf_cont_or\n",
    "\n",
    "    # Get the influence function of the inefficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func1 = inf_treat - inf_cont\n",
    "\n",
    "    # Now, we only need to get the influence function of the adjustment terms\n",
    "    # First, the terms as if all OR parameters were known\n",
    "    inf_eff1 = eta_d_post - w_d * att_d_post / np.mean(w_d)\n",
    "    inf_eff2 = eta_dt1_post - w_dt1 * att_dt1_post / np.mean(w_dt1)\n",
    "    inf_eff3 = eta_d_pre - w_d * att_d_pre / np.mean(w_d)\n",
    "    inf_eff4 = eta_dt0_pre - w_dt0 * att_dt0_pre / np.mean(w_dt0)\n",
    "    inf_eff = (inf_eff1 - inf_eff2) - (inf_eff3 - inf_eff4)\n",
    "\n",
    "    # Now the estimation effect of the OR coefficients\n",
    "    mom_post = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt1 / np.mean(w_dt1))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    mom_pre = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt0 / np.mean(w_dt0))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    inf_or_post = np.dot(\n",
    "        (reg_treat_post.cov_params() - reg_cont_post.cov_params()),\n",
    "        mom_post,\n",
    "    )\n",
    "    inf_or_pre = np.dot(\n",
    "        (reg_treat_pre.cov_params() - reg_cont_pre.cov_params()),\n",
    "        mom_pre,\n",
    "    )\n",
    "    inf_or = inf_or_post - inf_or_pre\n",
    "    inf_or = np.sum(inf_or)\n",
    "\n",
    "    # Get the influence function of the locally efficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func = dr_att_inf_func1 + inf_eff + inf_or\n",
    "\n",
    "    if not boot:\n",
    "        # Estimate of standard error\n",
    "        se_dr_att = np.std(dr_att_inf_func) / np.sqrt(n)\n",
    "        # Estimate of upper boundary of 95% CI\n",
    "        uci = dr_att + 1.96 * se_dr_att\n",
    "        # Estimate of lower boundary of 95% CI\n",
    "        lci = dr_att - 1.96 * se_dr_att\n",
    "        # Create this null vector so we can export the bootstrap draws too.\n",
    "        dr_boot = None\n",
    "    else:\n",
    "        if nboot is None:\n",
    "            nboot = 999\n",
    "        if boot_type == \"multiplier\":\n",
    "            # Do multiplier bootstrap\n",
    "            dr_boot = mboot_did(dr_att_inf_func, nboot)\n",
    "            # Get bootstrap std errors based on IQR\n",
    "            se_dr_att = np.percentile(dr_boot, 75) - np.percentile(dr_boot, 25)\n",
    "            # Get symmetric critical values\n",
    "            cv = np.percentile(np.abs(dr_boot / se_dr_att), 95)\n",
    "            # Estimate of upper boundary of 95% CI\n",
    "            uci = dr_att + cv * se_dr_att\n",
    "            # Estimate of lower boundary of 95% CI\n",
    "            lci = dr_att - cv * se_dr_att\n",
    "        else:\n",
    "            # Do weighted bootstrap\n",
    "            dr_boot = [\n",
    "                wboot_drdid_rc(n, y, post, D, int_cov, i_weights) for _ in range(nboot)\n",
    "            ]\n",
    "            # Get bootstrap std errors based on IQR\n",
    "            se_dr_att = np.percentile(dr_boot - dr_att, 75) - np.percentile(\n",
    "                dr_boot - dr_att,\n",
    "                25,\n",
    "            )\n",
    "            # Get symmetric critical values\n",
    "            cv = np.percentile(np.abs((dr_boot - dr_att) / se_dr_att), 95)\n",
    "            # Estimate of upper boundary of 95% CI\n",
    "            uci = dr_att + cv * se_dr_att\n",
    "            # Estimate of lower boundary of 95% CI\n",
    "            lci = dr_att - cv * se_dr_att\n",
    "\n",
    "    if not inffunc:\n",
    "        dr_att_inf_func = None\n",
    "\n",
    "    return {\n",
    "        \"ATT\": dr_att,\n",
    "        \"se\": se_dr_att,\n",
    "        \"uci\": uci,\n",
    "        \"lci\": lci,\n",
    "        \"boots\": dr_boot,\n",
    "        \"att_inf_func\": dr_att_inf_func,\n",
    "        \"call_param\": None,\n",
    "        \"argu\": {\n",
    "            \"panel\": False,\n",
    "            \"estMethod\": \"trad\",\n",
    "            \"boot\": boot,\n",
    "            \"boot_type\": boot_type,\n",
    "            \"nboot\": nboot,\n",
    "            \"type\": \"dr\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "n = 1000  # Sample size\n",
    "Xsi_ps = 0.75  # pscore index\n",
    "_lambda = 0.5  # Proportion in each period\n",
    "\n",
    "# Define means and standard deviations\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "\n",
    "# Loop for 1000 runs\n",
    "for _i in range(1000):\n",
    "    # Generate covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    # Propensity score\n",
    "    pi = logistic_cdf(Xsi_ps * (-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4))\n",
    "    d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "    index_unobs_het = d * index_lin\n",
    "    index_att = 0\n",
    "    index_trend = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "    # Generate unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "    # Generate outcomes at time 0 and time 1\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "    y01 = index_lin + v + np.random.normal(scale=1, size=n) + index_trend\n",
    "    y11 = index_lin + v + np.random.normal(scale=1, size=n) + index_trend + index_att\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "    y = np.where(\n",
    "        d & post,\n",
    "        y11,\n",
    "        np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "    )\n",
    "\n",
    "    # Generate id\n",
    "    id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "    time = np.tile([0, 1], n)\n",
    "\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"time\": time,\n",
    "            \"y\": np.tile(y, 2),\n",
    "            \"post\": np.tile(post.astype(int), 2),\n",
    "            \"d\": np.tile(d.astype(int), 2),\n",
    "            \"x1\": np.tile(z1, 2),\n",
    "            \"x2\": np.tile(z2, 2),\n",
    "            \"x3\": np.tile(z3, 2),\n",
    "            \"x4\": np.tile(z4, 2),\n",
    "        },\n",
    "    )\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "    dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "    # Run the IPW-DID estimator\n",
    "    covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "    y = dta_long[\"y\"].values\n",
    "    post = dta_long[\"post\"].values\n",
    "    D = dta_long[\"d\"].values\n",
    "\n",
    "    result = drdid_rc(y, post, D, covariates)\n",
    "\n",
    "    ATTE_estimates.append(result[\"ATT\"])\n",
    "    asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "\n",
    "# Calculate average bias, median bias, and RMSE\n",
    "true_ATT = 0\n",
    "average_bias = np.mean(ATTE_estimates) - true_ATT\n",
    "median_bias = np.median(ATTE_estimates) - true_ATT\n",
    "rmse = np.sqrt(np.mean((np.array(ATTE_estimates) - true_ATT) ** 2))\n",
    "\n",
    "# Calculate average of the variance\n",
    "average_variance = np.mean(asymptotic_variance)\n",
    "\n",
    "print(\"Average Bias:\", average_bias)\n",
    "print(\"Median Bias:\", median_bias)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"Average Variance:\", average_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP 2 SZ - Deep Learning\n",
    "The normal version not the improved one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.special import expit as logistic_cdf\n",
    "\n",
    "# Define the DRDID function\n",
    "\n",
    "\n",
    "def drdid_rc(\n",
    "    y,\n",
    "    post,\n",
    "    D,\n",
    "    covariates=None,\n",
    "    i_weights=None,\n",
    "    boot=False,\n",
    "    boot_type=\"weighted\",\n",
    "    nboot=None,\n",
    "    inffunc=False,\n",
    "):\n",
    "    # Ensure D is a vector\n",
    "    D = np.asarray(D)\n",
    "    # Sample size\n",
    "    n = len(D)\n",
    "    # Ensure y is a vector\n",
    "    y = np.asarray(y)\n",
    "    # Ensure post is a vector\n",
    "    post = np.asarray(post)\n",
    "    # Add constant to covariate vector\n",
    "    int_cov = np.ones((n, 1))\n",
    "    if covariates is not None:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "    elif np.min(i_weights) < 0:\n",
    "        msg = \"i.weights must be non-negative\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # Compute the Pscore by MLE\n",
    "    pscore_tr = sm.GLM(\n",
    "        D,\n",
    "        int_cov,\n",
    "        family=sm.families.Binomial(),\n",
    "        freq_weights=i_weights,\n",
    "    ).fit()\n",
    "    if not pscore_tr.converged:\n",
    "        warnings.warn(\"GLM algorithm did not converge\")\n",
    "    if np.any(np.isnan(pscore_tr.params)):\n",
    "        msg = \"Propensity score model coefficients have NA components. Multicollinearity (or lack of variation) of covariates is a likely reason.\"\n",
    "        raise ValueError(\n",
    "            msg,\n",
    "        )\n",
    "    ps_fit = pscore_tr.fittedvalues\n",
    "    # Avoid divide by zero\n",
    "    ps_fit = np.clip(ps_fit, 1e-16, 1 - 1e-16)\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the pre-treatment period, using OLS\n",
    "    reg_cont_pre = sm.WLS(\n",
    "        y[(D == 0) & (post == 0)],\n",
    "        int_cov[(D == 0) & (post == 0)],\n",
    "        weights=i_weights[(D == 0) & (post == 0)],\n",
    "    ).fit()\n",
    "    if np.any(np.isnan(reg_cont_pre.params)):\n",
    "        msg = \"Outcome regression model coefficients have NA components. Multicollinearity (or lack of variation) of covariates is a likely reason.\"\n",
    "        raise ValueError(\n",
    "            msg,\n",
    "        )\n",
    "    out_y_cont_pre = int_cov @ reg_cont_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the post-treatment period, using OLS\n",
    "    reg_cont_post = sm.WLS(\n",
    "        y[(D == 0) & (post == 1)],\n",
    "        int_cov[(D == 0) & (post == 1)],\n",
    "        weights=i_weights[(D == 0) & (post == 1)],\n",
    "    ).fit()\n",
    "    if np.any(np.isnan(reg_cont_post.params)):\n",
    "        msg = \"Outcome regression model coefficients have NA components. Multicollinearity (or lack of variation) of covariates is a likely reason.\"\n",
    "        raise ValueError(\n",
    "            msg,\n",
    "        )\n",
    "    out_y_cont_post = int_cov @ reg_cont_post.params\n",
    "\n",
    "    # Combine the ORs for control group\n",
    "    out_y_cont = post * out_y_cont_post + (1 - post) * out_y_cont_pre\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the pre-treatment period, using OLS\n",
    "    reg_treat_pre = sm.WLS(\n",
    "        y[(D == 1) & (post == 0)],\n",
    "        int_cov[(D == 1) & (post == 0)],\n",
    "        weights=i_weights[(D == 1) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_treat_pre = int_cov @ reg_treat_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the post-treatment period, using OLS\n",
    "    reg_treat_post = sm.WLS(\n",
    "        y[(D == 1) & (post == 1)],\n",
    "        int_cov[(D == 1) & (post == 1)],\n",
    "        weights=i_weights[(D == 1) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_treat_post = int_cov @ reg_treat_post.params\n",
    "\n",
    "    # Weights\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    w_d = i_weights * D\n",
    "    w_dt1 = i_weights * D * post\n",
    "    w_dt0 = i_weights * D * (1 - post)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * (y - out_y_cont) / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * (y - out_y_cont) / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * (y - out_y_cont) / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * (y - out_y_cont) / np.mean(w_cont_post)\n",
    "\n",
    "    # Extra elements for the locally efficient DRDID\n",
    "    eta_d_post = w_d * (out_y_treat_post - out_y_cont_post) / np.mean(w_d)\n",
    "    eta_dt1_post = w_dt1 * (out_y_treat_post - out_y_cont_post) / np.mean(w_dt1)\n",
    "    eta_d_pre = w_d * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_d)\n",
    "    eta_dt0_pre = w_dt0 * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_dt0)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    att_d_post = np.mean(eta_d_post)\n",
    "    att_dt1_post = np.mean(eta_dt1_post)\n",
    "    att_d_pre = np.mean(eta_d_pre)\n",
    "    att_dt0_pre = np.mean(eta_dt0_pre)\n",
    "\n",
    "    # ATT estimator\n",
    "    dr_att = (\n",
    "        (att_treat_post - att_treat_pre)\n",
    "        - (att_cont_post - att_cont_pre)\n",
    "        + (att_d_post - att_dt1_post)\n",
    "        - (att_d_pre - att_dt0_pre)\n",
    "    )\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    # Leading term of the influence function: no estimation effect\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M1_post = -np.mean(\n",
    "        w_treat_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_post)\n",
    "    M1_pre = -np.mean(\n",
    "        w_treat_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_treat_or_post = np.dot(reg_cont_post.cov_params(), M1_post)\n",
    "    inf_treat_or_pre = np.dot(reg_cont_pre.cov_params(), M1_pre)\n",
    "    inf_treat_or = inf_treat_or_post + inf_treat_or_pre\n",
    "\n",
    "    # Influence function for the treated component\n",
    "    inf_treat = inf_treat_post - inf_treat_pre + np.sum(inf_treat_or)\n",
    "\n",
    "    # Now, get the influence function of control component\n",
    "    # Leading term of the influence function: no estimation effect from nuisance parameters\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "\n",
    "    # Estimation effect from gamma hat (pscore)\n",
    "    M2_pre = np.mean(\n",
    "        w_cont_pre[:, np.newaxis]\n",
    "        * (y[:, np.newaxis] - out_y_cont[:, np.newaxis] - att_cont_pre)\n",
    "        * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_pre)\n",
    "    M2_post = np.mean(\n",
    "        w_cont_post[:, np.newaxis]\n",
    "        * (y[:, np.newaxis] - out_y_cont[:, np.newaxis] - att_cont_post)\n",
    "        * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_post)\n",
    "\n",
    "    # Now the influence function related to estimation effect of pscores\n",
    "    inf_cont_ps = np.dot(pscore_tr.cov_params(), (M2_post - M2_pre))\n",
    "    inf_cont_ps = np.sum(inf_cont_ps)\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M3_post = -np.mean(\n",
    "        w_cont_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_post)\n",
    "    M3_pre = -np.mean(\n",
    "        w_cont_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_cont_or_post = np.dot(reg_cont_post.cov_params(), M3_post)\n",
    "    inf_cont_or_pre = np.dot(reg_cont_pre.cov_params(), M3_pre)\n",
    "    inf_cont_or = inf_cont_or_post + inf_cont_or_pre\n",
    "    inf_cont_or = np.sum(inf_cont_or)\n",
    "\n",
    "    # Influence function for the control component\n",
    "    inf_cont = inf_cont_post - inf_cont_pre + inf_cont_ps + inf_cont_or\n",
    "\n",
    "    # Get the influence function of the inefficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func1 = inf_treat - inf_cont\n",
    "\n",
    "    # Now, we only need to get the influence function of the adjustment terms\n",
    "    # First, the terms as if all OR parameters were known\n",
    "    inf_eff1 = eta_d_post - w_d * att_d_post / np.mean(w_d)\n",
    "    inf_eff2 = eta_dt1_post - w_dt1 * att_dt1_post / np.mean(w_dt1)\n",
    "    inf_eff3 = eta_d_pre - w_d * att_d_pre / np.mean(w_d)\n",
    "    inf_eff4 = eta_dt0_pre - w_dt0 * att_dt0_pre / np.mean(w_dt0)\n",
    "    inf_eff = (inf_eff1 - inf_eff2) - (inf_eff3 - inf_eff4)\n",
    "\n",
    "    # Now the estimation effect of the OR coefficients\n",
    "    mom_post = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt1 / np.mean(w_dt1))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    mom_pre = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt0 / np.mean(w_dt0))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    inf_or_post = np.dot(\n",
    "        (reg_treat_post.cov_params() - reg_cont_post.cov_params()),\n",
    "        mom_post,\n",
    "    )\n",
    "    inf_or_pre = np.dot(\n",
    "        (reg_treat_pre.cov_params() - reg_cont_pre.cov_params()),\n",
    "        mom_pre,\n",
    "    )\n",
    "    inf_or = inf_or_post - inf_or_pre\n",
    "    inf_or = np.sum(inf_or)\n",
    "\n",
    "    # Get the influence function of the locally efficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func = dr_att_inf_func1 + inf_eff + inf_or\n",
    "\n",
    "    if not boot:\n",
    "        # Estimate of standard error\n",
    "        se_dr_att = np.std(dr_att_inf_func) / np.sqrt(n)\n",
    "        # Estimate of upper boundary of 95% CI\n",
    "        uci = dr_att + 1.96 * se_dr_att\n",
    "        # Estimate of lower boundary of 95% CI\n",
    "        lci = dr_att - 1.96 * se_dr_att\n",
    "        # Create this null vector so we can export the bootstrap draws too.\n",
    "        dr_boot = None\n",
    "    else:\n",
    "        if nboot is None:\n",
    "            nboot = 999\n",
    "        if boot_type == \"multiplier\":\n",
    "            # Do multiplier bootstrap\n",
    "            dr_boot = mboot_did(dr_att_inf_func, nboot)\n",
    "            # Get bootstrap std errors based on IQR\n",
    "            se_dr_att = np.percentile(dr_boot, 75) - np.percentile(dr_boot, 25)\n",
    "            # Get symmetric critical values\n",
    "            cv = np.percentile(np.abs(dr_boot / se_dr_att), 95)\n",
    "            # Estimate of upper boundary of 95% CI\n",
    "            uci = dr_att + cv * se_dr_att\n",
    "            # Estimate of lower boundary of 95% CI\n",
    "            lci = dr_att - cv * se_dr_att\n",
    "        else:\n",
    "            # Do weighted bootstrap\n",
    "            dr_boot = [\n",
    "                wboot_drdid_rc(n, y, post, D, int_cov, i_weights) for _ in range(nboot)\n",
    "            ]\n",
    "            # Get bootstrap std errors based on IQR\n",
    "            se_dr_att = np.percentile(dr_boot - dr_att, 75) - np.percentile(\n",
    "                dr_boot - dr_att,\n",
    "                25,\n",
    "            )\n",
    "            # Get symmetric critical values\n",
    "            cv = np.percentile(np.abs((dr_boot - dr_att) / se_dr_att), 95)\n",
    "            # Estimate of upper boundary of 95% CI\n",
    "            uci = dr_att + cv * se_dr_att\n",
    "            # Estimate of lower boundary of 95% CI\n",
    "            lci = dr_att - cv * se_dr_att\n",
    "\n",
    "    if not inffunc:\n",
    "        dr_att_inf_func = None\n",
    "\n",
    "    return {\n",
    "        \"ATT\": dr_att,\n",
    "        \"se\": se_dr_att,\n",
    "        \"uci\": uci,\n",
    "        \"lci\": lci,\n",
    "        \"boots\": dr_boot,\n",
    "        \"att_inf_func\": dr_att_inf_func,\n",
    "        \"call_param\": None,\n",
    "        \"argu\": {\n",
    "            \"panel\": False,\n",
    "            \"estMethod\": \"trad\",\n",
    "            \"boot\": boot,\n",
    "            \"boot_type\": boot_type,\n",
    "            \"nboot\": nboot,\n",
    "            \"type\": \"dr\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "np.random.seed(42)  # You can use any integer value as the seed\n",
    "\n",
    "# Sample size\n",
    "n = 1000\n",
    "# pscore index (strength of common support)\n",
    "Xsi_ps = 0.75\n",
    "# Proportion in each period\n",
    "_lambda = 0.5\n",
    "# Number of bootstrapped draws\n",
    "\n",
    "# Mean and Std deviation of Z's without truncation\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "\n",
    "for _i in range(1000):\n",
    "    # Generate covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    np.column_stack((x1, x2, x3, x4))\n",
    "    np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "    # Generate treatment groups\n",
    "    # Propensity score\n",
    "    pi = logistic_cdf(Xsi_ps * (-x1 + 0.5 * x2 - 0.25 * x3 - 0.1 * x4))\n",
    "    d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "    # Create heterogenenous effects for the ATT, which is set approximately equal to zero\n",
    "    index_unobs_het = d * (index_lin)\n",
    "    index_att = 0\n",
    "\n",
    "    # This is the key for consistency of outcome regression\n",
    "    index_trend = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "    # v is the unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "    # Gen realized outcome at time 0\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "    # Gen outcomes at time 1\n",
    "    # First let's generate potential outcomes: y_1_potential\n",
    "    y01 = (\n",
    "        index_lin + v + np.random.normal(scale=1, size=n) + index_trend\n",
    "    )  # This is the baseline\n",
    "    y11 = (\n",
    "        index_lin + v + np.random.normal(scale=1, size=n) + index_trend + index_att\n",
    "    )  # This is the baseline\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "    y = np.where(\n",
    "        d & post,\n",
    "        y11,\n",
    "        np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "    )\n",
    "\n",
    "    # Gen id\n",
    "    id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "    time = np.tile([0, 1], n)\n",
    "\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"time\": time,\n",
    "            \"y\": np.tile(y, 2),\n",
    "            \"post\": np.tile(post.astype(int), 2),\n",
    "            \"d\": np.tile(d.astype(int), 2),\n",
    "            \"x1\": np.tile(z1, 2),\n",
    "            \"x2\": np.tile(z2, 2),\n",
    "            \"x3\": np.tile(z3, 2),\n",
    "            \"x4\": np.tile(z4, 2),\n",
    "        },\n",
    "    )\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "    dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "    covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "    y = dta_long[\"y\"].values\n",
    "    post = dta_long[\"post\"].values\n",
    "    D = dta_long[\"d\"].values\n",
    "\n",
    "    result = drdid_rc(y, post, D, covariates)\n",
    "\n",
    "    ATTE_estimates.append(result[\"ATT\"])\n",
    "    asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "\n",
    "# Calculate average bias, median bias, and RMSE\n",
    "true_ATT = 0\n",
    "average_bias = np.mean(ATTE_estimates) - true_ATT\n",
    "median_bias = np.median(ATTE_estimates) - true_ATT\n",
    "rmse = np.sqrt(np.mean((np.array(ATTE_estimates) - true_ATT) ** 2))\n",
    "\n",
    "# Calculate average of the variance\n",
    "average_variance = np.mean(asymptotic_variance)\n",
    "\n",
    "print(\"Average Bias:\", average_bias)\n",
    "print(\"Median Bias:\", median_bias)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"Average Variance:\", average_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP 3 SZ - Deep Learning\n",
    "The normal version not the improved one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.special import expit as logistic_cdf\n",
    "\n",
    "# Define the DRDID function\n",
    "\n",
    "\n",
    "def drdid_rc(\n",
    "    y,\n",
    "    post,\n",
    "    D,\n",
    "    covariates=None,\n",
    "    i_weights=None,\n",
    "    boot=False,\n",
    "    boot_type=\"weighted\",\n",
    "    nboot=None,\n",
    "    inffunc=False,\n",
    "):\n",
    "    # Ensure D is a vector\n",
    "    D = np.asarray(D)\n",
    "    # Sample size\n",
    "    n = len(D)\n",
    "    # Ensure y is a vector\n",
    "    y = np.asarray(y)\n",
    "    # Ensure post is a vector\n",
    "    post = np.asarray(post)\n",
    "    # Add constant to covariate vector\n",
    "    int_cov = np.ones((n, 1))\n",
    "    if covariates is not None:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "    elif np.min(i_weights) < 0:\n",
    "        msg = \"i.weights must be non-negative\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # Compute the Pscore by MLE\n",
    "    pscore_tr = sm.GLM(\n",
    "        D,\n",
    "        int_cov,\n",
    "        family=sm.families.Binomial(),\n",
    "        freq_weights=i_weights,\n",
    "    ).fit()\n",
    "    if not pscore_tr.converged:\n",
    "        warnings.warn(\"GLM algorithm did not converge\")\n",
    "    if np.any(np.isnan(pscore_tr.params)):\n",
    "        msg = \"Propensity score model coefficients have NA components. Multicollinearity (or lack of variation) of covariates is a likely reason.\"\n",
    "        raise ValueError(\n",
    "            msg,\n",
    "        )\n",
    "    ps_fit = pscore_tr.fittedvalues\n",
    "    # Avoid divide by zero\n",
    "    ps_fit = np.clip(ps_fit, 1e-16, 1 - 1e-16)\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the pre-treatment period, using OLS\n",
    "    reg_cont_pre = sm.WLS(\n",
    "        y[(D == 0) & (post == 0)],\n",
    "        int_cov[(D == 0) & (post == 0)],\n",
    "        weights=i_weights[(D == 0) & (post == 0)],\n",
    "    ).fit()\n",
    "    if np.any(np.isnan(reg_cont_pre.params)):\n",
    "        msg = \"Outcome regression model coefficients have NA components. Multicollinearity (or lack of variation) of covariates is a likely reason.\"\n",
    "        raise ValueError(\n",
    "            msg,\n",
    "        )\n",
    "    out_y_cont_pre = int_cov @ reg_cont_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the post-treatment period, using OLS\n",
    "    reg_cont_post = sm.WLS(\n",
    "        y[(D == 0) & (post == 1)],\n",
    "        int_cov[(D == 0) & (post == 1)],\n",
    "        weights=i_weights[(D == 0) & (post == 1)],\n",
    "    ).fit()\n",
    "    if np.any(np.isnan(reg_cont_post.params)):\n",
    "        msg = \"Outcome regression model coefficients have NA components. Multicollinearity (or lack of variation) of covariates is a likely reason.\"\n",
    "        raise ValueError(\n",
    "            msg,\n",
    "        )\n",
    "    out_y_cont_post = int_cov @ reg_cont_post.params\n",
    "\n",
    "    # Combine the ORs for control group\n",
    "    out_y_cont = post * out_y_cont_post + (1 - post) * out_y_cont_pre\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the pre-treatment period, using OLS\n",
    "    reg_treat_pre = sm.WLS(\n",
    "        y[(D == 1) & (post == 0)],\n",
    "        int_cov[(D == 1) & (post == 0)],\n",
    "        weights=i_weights[(D == 1) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_treat_pre = int_cov @ reg_treat_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the post-treatment period, using OLS\n",
    "    reg_treat_post = sm.WLS(\n",
    "        y[(D == 1) & (post == 1)],\n",
    "        int_cov[(D == 1) & (post == 1)],\n",
    "        weights=i_weights[(D == 1) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_treat_post = int_cov @ reg_treat_post.params\n",
    "\n",
    "    # Weights\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    w_d = i_weights * D\n",
    "    w_dt1 = i_weights * D * post\n",
    "    w_dt0 = i_weights * D * (1 - post)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * (y - out_y_cont) / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * (y - out_y_cont) / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * (y - out_y_cont) / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * (y - out_y_cont) / np.mean(w_cont_post)\n",
    "\n",
    "    # Extra elements for the locally efficient DRDID\n",
    "    eta_d_post = w_d * (out_y_treat_post - out_y_cont_post) / np.mean(w_d)\n",
    "    eta_dt1_post = w_dt1 * (out_y_treat_post - out_y_cont_post) / np.mean(w_dt1)\n",
    "    eta_d_pre = w_d * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_d)\n",
    "    eta_dt0_pre = w_dt0 * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_dt0)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    att_d_post = np.mean(eta_d_post)\n",
    "    att_dt1_post = np.mean(eta_dt1_post)\n",
    "    att_d_pre = np.mean(eta_d_pre)\n",
    "    att_dt0_pre = np.mean(eta_dt0_pre)\n",
    "\n",
    "    # ATT estimator\n",
    "    dr_att = (\n",
    "        (att_treat_post - att_treat_pre)\n",
    "        - (att_cont_post - att_cont_pre)\n",
    "        + (att_d_post - att_dt1_post)\n",
    "        - (att_d_pre - att_dt0_pre)\n",
    "    )\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    # Leading term of the influence function: no estimation effect\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M1_post = -np.mean(\n",
    "        w_treat_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_post)\n",
    "    M1_pre = -np.mean(\n",
    "        w_treat_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_treat_or_post = np.dot(reg_cont_post.cov_params(), M1_post)\n",
    "    inf_treat_or_pre = np.dot(reg_cont_pre.cov_params(), M1_pre)\n",
    "    inf_treat_or = inf_treat_or_post + inf_treat_or_pre\n",
    "\n",
    "    # Influence function for the treated component\n",
    "    inf_treat = inf_treat_post - inf_treat_pre + np.sum(inf_treat_or)\n",
    "\n",
    "    # Now, get the influence function of control component\n",
    "    # Leading term of the influence function: no estimation effect from nuisance parameters\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "\n",
    "    # Estimation effect from gamma hat (pscore)\n",
    "    M2_pre = np.mean(\n",
    "        w_cont_pre[:, np.newaxis]\n",
    "        * (y[:, np.newaxis] - out_y_cont[:, np.newaxis] - att_cont_pre)\n",
    "        * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_pre)\n",
    "    M2_post = np.mean(\n",
    "        w_cont_post[:, np.newaxis]\n",
    "        * (y[:, np.newaxis] - out_y_cont[:, np.newaxis] - att_cont_post)\n",
    "        * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_post)\n",
    "\n",
    "    # Now the influence function related to estimation effect of pscores\n",
    "    inf_cont_ps = np.dot(pscore_tr.cov_params(), (M2_post - M2_pre))\n",
    "    inf_cont_ps = np.sum(inf_cont_ps)\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M3_post = -np.mean(\n",
    "        w_cont_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_post)\n",
    "    M3_pre = -np.mean(\n",
    "        w_cont_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_cont_or_post = np.dot(reg_cont_post.cov_params(), M3_post)\n",
    "    inf_cont_or_pre = np.dot(reg_cont_pre.cov_params(), M3_pre)\n",
    "    inf_cont_or = inf_cont_or_post + inf_cont_or_pre\n",
    "    inf_cont_or = np.sum(inf_cont_or)\n",
    "\n",
    "    # Influence function for the control component\n",
    "    inf_cont = inf_cont_post - inf_cont_pre + inf_cont_ps + inf_cont_or\n",
    "\n",
    "    # Get the influence function of the inefficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func1 = inf_treat - inf_cont\n",
    "\n",
    "    # Now, we only need to get the influence function of the adjustment terms\n",
    "    # First, the terms as if all OR parameters were known\n",
    "    inf_eff1 = eta_d_post - w_d * att_d_post / np.mean(w_d)\n",
    "    inf_eff2 = eta_dt1_post - w_dt1 * att_dt1_post / np.mean(w_dt1)\n",
    "    inf_eff3 = eta_d_pre - w_d * att_d_pre / np.mean(w_d)\n",
    "    inf_eff4 = eta_dt0_pre - w_dt0 * att_dt0_pre / np.mean(w_dt0)\n",
    "    inf_eff = (inf_eff1 - inf_eff2) - (inf_eff3 - inf_eff4)\n",
    "\n",
    "    # Now the estimation effect of the OR coefficients\n",
    "    mom_post = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt1 / np.mean(w_dt1))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    mom_pre = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt0 / np.mean(w_dt0))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    inf_or_post = np.dot(\n",
    "        (reg_treat_post.cov_params() - reg_cont_post.cov_params()),\n",
    "        mom_post,\n",
    "    )\n",
    "    inf_or_pre = np.dot(\n",
    "        (reg_treat_pre.cov_params() - reg_cont_pre.cov_params()),\n",
    "        mom_pre,\n",
    "    )\n",
    "    inf_or = inf_or_post - inf_or_pre\n",
    "    inf_or = np.sum(inf_or)\n",
    "\n",
    "    # Get the influence function of the locally efficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func = dr_att_inf_func1 + inf_eff + inf_or\n",
    "\n",
    "    if not boot:\n",
    "        # Estimate of standard error\n",
    "        se_dr_att = np.std(dr_att_inf_func) / np.sqrt(n)\n",
    "        # Estimate of upper boundary of 95% CI\n",
    "        uci = dr_att + 1.96 * se_dr_att\n",
    "        # Estimate of lower boundary of 95% CI\n",
    "        lci = dr_att - 1.96 * se_dr_att\n",
    "        # Create this null vector so we can export the bootstrap draws too.\n",
    "        dr_boot = None\n",
    "    else:\n",
    "        if nboot is None:\n",
    "            nboot = 999\n",
    "        if boot_type == \"multiplier\":\n",
    "            # Do multiplier bootstrap\n",
    "            dr_boot = mboot_did(dr_att_inf_func, nboot)\n",
    "            # Get bootstrap std errors based on IQR\n",
    "            se_dr_att = np.percentile(dr_boot, 75) - np.percentile(dr_boot, 25)\n",
    "            # Get symmetric critical values\n",
    "            cv = np.percentile(np.abs(dr_boot / se_dr_att), 95)\n",
    "            # Estimate of upper boundary of 95% CI\n",
    "            uci = dr_att + cv * se_dr_att\n",
    "            # Estimate of lower boundary of 95% CI\n",
    "            lci = dr_att - cv * se_dr_att\n",
    "        else:\n",
    "            # Do weighted bootstrap\n",
    "            dr_boot = [\n",
    "                wboot_drdid_rc(n, y, post, D, int_cov, i_weights) for _ in range(nboot)\n",
    "            ]\n",
    "            # Get bootstrap std errors based on IQR\n",
    "            se_dr_att = np.percentile(dr_boot - dr_att, 75) - np.percentile(\n",
    "                dr_boot - dr_att,\n",
    "                25,\n",
    "            )\n",
    "            # Get symmetric critical values\n",
    "            cv = np.percentile(np.abs((dr_boot - dr_att) / se_dr_att), 95)\n",
    "            # Estimate of upper boundary of 95% CI\n",
    "            uci = dr_att + cv * se_dr_att\n",
    "            # Estimate of lower boundary of 95% CI\n",
    "            lci = dr_att - cv * se_dr_att\n",
    "\n",
    "    if not inffunc:\n",
    "        dr_att_inf_func = None\n",
    "\n",
    "    return {\n",
    "        \"ATT\": dr_att,\n",
    "        \"se\": se_dr_att,\n",
    "        \"uci\": uci,\n",
    "        \"lci\": lci,\n",
    "        \"boots\": dr_boot,\n",
    "        \"att_inf_func\": dr_att_inf_func,\n",
    "        \"call_param\": None,\n",
    "        \"argu\": {\n",
    "            \"panel\": False,\n",
    "            \"estMethod\": \"trad\",\n",
    "            \"boot\": boot,\n",
    "            \"boot_type\": boot_type,\n",
    "            \"nboot\": nboot,\n",
    "            \"type\": \"dr\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "np.random.seed(42)  # You can use any integer value as the seed\n",
    "\n",
    "\n",
    "def sz_dr_dgp1():\n",
    "    # Sample size\n",
    "    n = 1000\n",
    "    # pscore index (strength of common support)\n",
    "    Xsi_ps = 0.75\n",
    "    # Proportion in each period\n",
    "    # Number of bootstrapped draws\n",
    "\n",
    "    # Mean and Std deviation of Z's without truncation\n",
    "    mean_z1 = np.exp(0.25 / 2)\n",
    "    sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "    mean_z2 = 10\n",
    "    sd_z2 = 0.54164\n",
    "    mean_z3 = 0.21887\n",
    "    sd_z3 = 0.04453\n",
    "    mean_z4 = 402\n",
    "    sd_z4 = 56.63891\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    ATTE_estimates = []\n",
    "    asymptotic_variance = []\n",
    "\n",
    "    for _i in range(1000):\n",
    "        # Generate covariates\n",
    "        x1 = np.random.normal(0, 1, n)\n",
    "        x2 = np.random.normal(0, 1, n)\n",
    "        x3 = np.random.normal(0, 1, n)\n",
    "        x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "        z1 = np.exp(x1 / 2)\n",
    "        z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "        z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "        z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "        z1 = (z1 - mean_z1) / sd_z1\n",
    "        z2 = (z2 - mean_z2) / sd_z2\n",
    "        z3 = (z3 - mean_z3) / sd_z3\n",
    "        z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "        np.column_stack((x1, x2, x3, x4))\n",
    "        np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "        # Propensity score\n",
    "        pi = logistic_cdf(Xsi_ps * (-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4))\n",
    "        d = (np.random.uniform(size=n) <= pi).astype(int)\n",
    "\n",
    "        # Generate aux indexes for the potential outcomes\n",
    "        index_lin = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "        # Create heterogeneous effects for the ATT, which is set approximately equal to zero\n",
    "        index_unobs_het = d * index_lin\n",
    "        index_att = 0\n",
    "\n",
    "        # This is the key for consistency of outcome regression\n",
    "        index_trend = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "        # v is the unobserved heterogeneity\n",
    "        v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "        # Gen realized outcome at time 0\n",
    "        y00 = index_lin + v + np.random.normal(size=n)\n",
    "        y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "        # Gen outcomes at time 1\n",
    "        y01 = (\n",
    "            index_lin + v + np.random.normal(size=n) + index_trend\n",
    "        )  # This is the baseline\n",
    "        y11 = (\n",
    "            index_lin + v + np.random.normal(size=n) + index_trend + index_att\n",
    "        )  # This is the baseline\n",
    "\n",
    "        # Generate \"T\"\n",
    "        ti_nt = 0.5\n",
    "        ti_t = 0.5\n",
    "        ti = d * ti_t + (1 - d) * ti_nt\n",
    "        post = (np.random.uniform(size=n) <= ti).astype(int)\n",
    "\n",
    "        y = np.where(\n",
    "            d & post,\n",
    "            y11,\n",
    "            np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "        )\n",
    "\n",
    "        # Gen id\n",
    "        id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "        time = np.tile([0, 1], n)\n",
    "\n",
    "        # Put in a long data frame\n",
    "        dta_long = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": id_,\n",
    "                \"time\": time,\n",
    "                \"y\": np.tile(y, 2),\n",
    "                \"post\": np.tile(post.astype(int), 2),\n",
    "                \"d\": np.tile(d.astype(int), 2),\n",
    "                \"x1\": np.tile(z1, 2),\n",
    "                \"x2\": np.tile(z2, 2),\n",
    "                \"x3\": np.tile(z3, 2),\n",
    "                \"x4\": np.tile(z4, 2),\n",
    "            },\n",
    "        )\n",
    "        dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "        dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "        covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "        y = dta_long[\"y\"].values\n",
    "        post = dta_long[\"post\"].values\n",
    "        D = dta_long[\"d\"].values\n",
    "\n",
    "        result = drdid_rc(y, post, D, covariates)\n",
    "\n",
    "        ATTE_estimates.append(result[\"ATT\"])\n",
    "        asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "\n",
    "    # Calculate average bias, median bias, and RMSE\n",
    "    true_ATT = 0\n",
    "    average_bias = np.mean(ATTE_estimates) - true_ATT\n",
    "    median_bias = np.median(ATTE_estimates) - true_ATT\n",
    "    rmse = np.sqrt(np.mean((np.array(ATTE_estimates) - true_ATT) ** 2))\n",
    "\n",
    "    # Calculate average of the variance\n",
    "    average_variance = np.mean(asymptotic_variance)\n",
    "    # Display the results\n",
    "    return {\n",
    "        \"Average Bias\": average_bias,\n",
    "        \"Median Bias\": median_bias,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Average Variance of ATT\": average_variance,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP 4  SZ - Deep Learning\n",
    "The normal version not the improved one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.special import expit as logistic_cdf\n",
    "\n",
    "# Define the DRDID function\n",
    "\n",
    "\n",
    "def drdid_rc(\n",
    "    y,\n",
    "    post,\n",
    "    D,\n",
    "    covariates=None,\n",
    "    i_weights=None,\n",
    "    boot=False,\n",
    "    boot_type=\"weighted\",\n",
    "    nboot=None,\n",
    "    inffunc=False,\n",
    "):\n",
    "    # Ensure D is a vector\n",
    "    D = np.asarray(D)\n",
    "    # Sample size\n",
    "    n = len(D)\n",
    "    # Ensure y is a vector\n",
    "    y = np.asarray(y)\n",
    "    # Ensure post is a vector\n",
    "    post = np.asarray(post)\n",
    "    # Add constant to covariate vector\n",
    "    int_cov = np.ones((n, 1))\n",
    "    if covariates is not None:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "    elif np.min(i_weights) < 0:\n",
    "        msg = \"i.weights must be non-negative\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # Compute the Pscore by MLE\n",
    "    pscore_tr = sm.GLM(\n",
    "        D,\n",
    "        int_cov,\n",
    "        family=sm.families.Binomial(),\n",
    "        freq_weights=i_weights,\n",
    "    ).fit()\n",
    "    if not pscore_tr.converged:\n",
    "        warnings.warn(\"GLM algorithm did not converge\")\n",
    "    if np.any(np.isnan(pscore_tr.params)):\n",
    "        msg = \"Propensity score model coefficients have NA components. Multicollinearity (or lack of variation) of covariates is a likely reason.\"\n",
    "        raise ValueError(\n",
    "            msg,\n",
    "        )\n",
    "    ps_fit = pscore_tr.fittedvalues\n",
    "    # Avoid divide by zero\n",
    "    ps_fit = np.clip(ps_fit, 1e-16, 1 - 1e-16)\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the pre-treatment period, using OLS\n",
    "    reg_cont_pre = sm.WLS(\n",
    "        y[(D == 0) & (post == 0)],\n",
    "        int_cov[(D == 0) & (post == 0)],\n",
    "        weights=i_weights[(D == 0) & (post == 0)],\n",
    "    ).fit()\n",
    "    if np.any(np.isnan(reg_cont_pre.params)):\n",
    "        msg = \"Outcome regression model coefficients have NA components. Multicollinearity (or lack of variation) of covariates is a likely reason.\"\n",
    "        raise ValueError(\n",
    "            msg,\n",
    "        )\n",
    "    out_y_cont_pre = int_cov @ reg_cont_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the post-treatment period, using OLS\n",
    "    reg_cont_post = sm.WLS(\n",
    "        y[(D == 0) & (post == 1)],\n",
    "        int_cov[(D == 0) & (post == 1)],\n",
    "        weights=i_weights[(D == 0) & (post == 1)],\n",
    "    ).fit()\n",
    "    if np.any(np.isnan(reg_cont_post.params)):\n",
    "        msg = \"Outcome regression model coefficients have NA components. Multicollinearity (or lack of variation) of covariates is a likely reason.\"\n",
    "        raise ValueError(\n",
    "            msg,\n",
    "        )\n",
    "    out_y_cont_post = int_cov @ reg_cont_post.params\n",
    "\n",
    "    # Combine the ORs for control group\n",
    "    out_y_cont = post * out_y_cont_post + (1 - post) * out_y_cont_pre\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the pre-treatment period, using OLS\n",
    "    reg_treat_pre = sm.WLS(\n",
    "        y[(D == 1) & (post == 0)],\n",
    "        int_cov[(D == 1) & (post == 0)],\n",
    "        weights=i_weights[(D == 1) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_treat_pre = int_cov @ reg_treat_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the post-treatment period, using OLS\n",
    "    reg_treat_post = sm.WLS(\n",
    "        y[(D == 1) & (post == 1)],\n",
    "        int_cov[(D == 1) & (post == 1)],\n",
    "        weights=i_weights[(D == 1) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_treat_post = int_cov @ reg_treat_post.params\n",
    "\n",
    "    # Weights\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    w_d = i_weights * D\n",
    "    w_dt1 = i_weights * D * post\n",
    "    w_dt0 = i_weights * D * (1 - post)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * (y - out_y_cont) / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * (y - out_y_cont) / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * (y - out_y_cont) / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * (y - out_y_cont) / np.mean(w_cont_post)\n",
    "\n",
    "    # Extra elements for the locally efficient DRDID\n",
    "    eta_d_post = w_d * (out_y_treat_post - out_y_cont_post) / np.mean(w_d)\n",
    "    eta_dt1_post = w_dt1 * (out_y_treat_post - out_y_cont_post) / np.mean(w_dt1)\n",
    "    eta_d_pre = w_d * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_d)\n",
    "    eta_dt0_pre = w_dt0 * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_dt0)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    att_d_post = np.mean(eta_d_post)\n",
    "    att_dt1_post = np.mean(eta_dt1_post)\n",
    "    att_d_pre = np.mean(eta_d_pre)\n",
    "    att_dt0_pre = np.mean(eta_dt0_pre)\n",
    "\n",
    "    # ATT estimator\n",
    "    dr_att = (\n",
    "        (att_treat_post - att_treat_pre)\n",
    "        - (att_cont_post - att_cont_pre)\n",
    "        + (att_d_post - att_dt1_post)\n",
    "        - (att_d_pre - att_dt0_pre)\n",
    "    )\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    # Leading term of the influence function: no estimation effect\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M1_post = -np.mean(\n",
    "        w_treat_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_post)\n",
    "    M1_pre = -np.mean(\n",
    "        w_treat_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_treat_or_post = np.dot(reg_cont_post.cov_params(), M1_post)\n",
    "    inf_treat_or_pre = np.dot(reg_cont_pre.cov_params(), M1_pre)\n",
    "    inf_treat_or = inf_treat_or_post + inf_treat_or_pre\n",
    "\n",
    "    # Influence function for the treated component\n",
    "    inf_treat = inf_treat_post - inf_treat_pre + np.sum(inf_treat_or)\n",
    "\n",
    "    # Now, get the influence function of control component\n",
    "    # Leading term of the influence function: no estimation effect from nuisance parameters\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "\n",
    "    # Estimation effect from gamma hat (pscore)\n",
    "    M2_pre = np.mean(\n",
    "        w_cont_pre[:, np.newaxis]\n",
    "        * (y[:, np.newaxis] - out_y_cont[:, np.newaxis] - att_cont_pre)\n",
    "        * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_pre)\n",
    "    M2_post = np.mean(\n",
    "        w_cont_post[:, np.newaxis]\n",
    "        * (y[:, np.newaxis] - out_y_cont[:, np.newaxis] - att_cont_post)\n",
    "        * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_post)\n",
    "\n",
    "    # Now the influence function related to estimation effect of pscores\n",
    "    inf_cont_ps = np.dot(pscore_tr.cov_params(), (M2_post - M2_pre))\n",
    "    inf_cont_ps = np.sum(inf_cont_ps)\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M3_post = -np.mean(\n",
    "        w_cont_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_post)\n",
    "    M3_pre = -np.mean(\n",
    "        w_cont_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_cont_or_post = np.dot(reg_cont_post.cov_params(), M3_post)\n",
    "    inf_cont_or_pre = np.dot(reg_cont_pre.cov_params(), M3_pre)\n",
    "    inf_cont_or = inf_cont_or_post + inf_cont_or_pre\n",
    "    inf_cont_or = np.sum(inf_cont_or)\n",
    "\n",
    "    # Influence function for the control component\n",
    "    inf_cont = inf_cont_post - inf_cont_pre + inf_cont_ps + inf_cont_or\n",
    "\n",
    "    # Get the influence function of the inefficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func1 = inf_treat - inf_cont\n",
    "\n",
    "    # Now, we only need to get the influence function of the adjustment terms\n",
    "    # First, the terms as if all OR parameters were known\n",
    "    inf_eff1 = eta_d_post - w_d * att_d_post / np.mean(w_d)\n",
    "    inf_eff2 = eta_dt1_post - w_dt1 * att_dt1_post / np.mean(w_dt1)\n",
    "    inf_eff3 = eta_d_pre - w_d * att_d_pre / np.mean(w_d)\n",
    "    inf_eff4 = eta_dt0_pre - w_dt0 * att_dt0_pre / np.mean(w_dt0)\n",
    "    inf_eff = (inf_eff1 - inf_eff2) - (inf_eff3 - inf_eff4)\n",
    "\n",
    "    # Now the estimation effect of the OR coefficients\n",
    "    mom_post = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt1 / np.mean(w_dt1))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    mom_pre = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt0 / np.mean(w_dt0))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    inf_or_post = np.dot(\n",
    "        (reg_treat_post.cov_params() - reg_cont_post.cov_params()),\n",
    "        mom_post,\n",
    "    )\n",
    "    inf_or_pre = np.dot(\n",
    "        (reg_treat_pre.cov_params() - reg_cont_pre.cov_params()),\n",
    "        mom_pre,\n",
    "    )\n",
    "    inf_or = inf_or_post - inf_or_pre\n",
    "    inf_or = np.sum(inf_or)\n",
    "\n",
    "    # Get the influence function of the locally efficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func = dr_att_inf_func1 + inf_eff + inf_or\n",
    "\n",
    "    if not boot:\n",
    "        # Estimate of standard error\n",
    "        se_dr_att = np.std(dr_att_inf_func) / np.sqrt(n)\n",
    "        # Estimate of upper boundary of 95% CI\n",
    "        uci = dr_att + 1.96 * se_dr_att\n",
    "        # Estimate of lower boundary of 95% CI\n",
    "        lci = dr_att - 1.96 * se_dr_att\n",
    "        # Create this null vector so we can export the bootstrap draws too.\n",
    "        dr_boot = None\n",
    "    else:\n",
    "        if nboot is None:\n",
    "            nboot = 999\n",
    "        if boot_type == \"multiplier\":\n",
    "            # Do multiplier bootstrap\n",
    "            dr_boot = mboot_did(dr_att_inf_func, nboot)\n",
    "            # Get bootstrap std errors based on IQR\n",
    "            se_dr_att = np.percentile(dr_boot, 75) - np.percentile(dr_boot, 25)\n",
    "            # Get symmetric critical values\n",
    "            cv = np.percentile(np.abs(dr_boot / se_dr_att), 95)\n",
    "            # Estimate of upper boundary of 95% CI\n",
    "            uci = dr_att + cv * se_dr_att\n",
    "            # Estimate of lower boundary of 95% CI\n",
    "            lci = dr_att - cv * se_dr_att\n",
    "        else:\n",
    "            # Do weighted bootstrap\n",
    "            dr_boot = [\n",
    "                wboot_drdid_rc(n, y, post, D, int_cov, i_weights) for _ in range(nboot)\n",
    "            ]\n",
    "            # Get bootstrap std errors based on IQR\n",
    "            se_dr_att = np.percentile(dr_boot - dr_att, 75) - np.percentile(\n",
    "                dr_boot - dr_att,\n",
    "                25,\n",
    "            )\n",
    "            # Get symmetric critical values\n",
    "            cv = np.percentile(np.abs((dr_boot - dr_att) / se_dr_att), 95)\n",
    "            # Estimate of upper boundary of 95% CI\n",
    "            uci = dr_att + cv * se_dr_att\n",
    "            # Estimate of lower boundary of 95% CI\n",
    "            lci = dr_att - cv * se_dr_att\n",
    "\n",
    "    if not inffunc:\n",
    "        dr_att_inf_func = None\n",
    "\n",
    "    return {\n",
    "        \"ATT\": dr_att,\n",
    "        \"se\": se_dr_att,\n",
    "        \"uci\": uci,\n",
    "        \"lci\": lci,\n",
    "        \"boots\": dr_boot,\n",
    "        \"att_inf_func\": dr_att_inf_func,\n",
    "        \"call_param\": None,\n",
    "        \"argu\": {\n",
    "            \"panel\": False,\n",
    "            \"estMethod\": \"trad\",\n",
    "            \"boot\": boot,\n",
    "            \"boot_type\": boot_type,\n",
    "            \"nboot\": nboot,\n",
    "            \"type\": \"dr\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "np.random.seed(42)  # You can use any integer value as the seed\n",
    "\n",
    "\n",
    "def sz_dr_dgp4():\n",
    "    # Sample size\n",
    "    n = 1000\n",
    "\n",
    "    # pscore index (strength of common support)\n",
    "    Xsi_ps = 0.75\n",
    "\n",
    "    # Proportion in each period\n",
    "\n",
    "    # Number of bootstrapped draws\n",
    "\n",
    "    # Mean and Std deviation of Z's without truncation\n",
    "    mean_z1 = np.exp(0.25 / 2)\n",
    "    sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "    mean_z2 = 10\n",
    "    sd_z2 = 0.54164\n",
    "    mean_z3 = 0.21887\n",
    "    sd_z3 = 0.04453\n",
    "    mean_z4 = 402\n",
    "    sd_z4 = 56.63891\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    ATTE_estimates = []\n",
    "    asymptotic_variance = []\n",
    "    for _i in range(1000):\n",
    "        # Gen covariates\n",
    "        x1 = np.random.normal(0, 1, n)\n",
    "        x2 = np.random.normal(0, 1, n)\n",
    "        x3 = np.random.normal(0, 1, n)\n",
    "        x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "        z1 = np.exp(x1 / 2)\n",
    "        z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "        z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "        z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "        z1 = (z1 - mean_z1) / sd_z1\n",
    "        z2 = (z2 - mean_z2) / sd_z2\n",
    "        z3 = (z3 - mean_z3) / sd_z3\n",
    "        z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "        np.column_stack((x1, x2, x3, x4))\n",
    "        np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "        # Gen treatment groups\n",
    "        # Propensity score\n",
    "        pi = 1 / (1 + np.exp(-Xsi_ps * (-x1 + 0.5 * x2 - 0.25 * x3 - 0.1 * x4)))\n",
    "        d = np.random.rand(n) <= pi\n",
    "\n",
    "        # Generate aux indexes for the potential outcomes\n",
    "        index_lin = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "        # Create heterogeneous effects for the ATT, which is set approximately equal to zero\n",
    "        index_unobs_het = d * index_lin\n",
    "        index_att = 0\n",
    "\n",
    "        # This is the key for consistency of outcome regression\n",
    "        index_trend = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "        # v is the unobserved heterogeneity\n",
    "        v = np.random.normal(index_unobs_het, 1, n)\n",
    "\n",
    "        # Gen realized outcome at time 0\n",
    "        y00 = index_lin + v + np.random.normal(size=n)\n",
    "        y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "        # Gen outcomes at time 1\n",
    "        # First let's generate potential outcomes: y_1_potential\n",
    "        y01 = index_lin + v + np.random.normal(size=n) + index_trend\n",
    "        y11 = index_lin + v + np.random.normal(size=n) + index_trend + index_att\n",
    "\n",
    "        # Generate \"T\"\n",
    "        ti_nt = 0.5\n",
    "        ti_t = 0.5\n",
    "        ti = d * ti_t + (1 - d) * ti_nt\n",
    "        post = np.random.rand(n) <= ti\n",
    "\n",
    "        # Combine outcomes into panel data format\n",
    "        y = np.where(\n",
    "            d & post,\n",
    "            y11,\n",
    "            np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "        )\n",
    "\n",
    "        # Gen id\n",
    "        id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "        time = np.tile([0, 1], n)\n",
    "\n",
    "        # Put in a long data frame\n",
    "        dta_long = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": id_,\n",
    "                \"time\": time,\n",
    "                \"y\": np.tile(y, 2),\n",
    "                \"post\": np.tile(post.astype(int), 2),\n",
    "                \"d\": np.tile(d.astype(int), 2),\n",
    "                \"x1\": np.tile(z1, 2),\n",
    "                \"x2\": np.tile(z2, 2),\n",
    "                \"x3\": np.tile(z3, 2),\n",
    "                \"x4\": np.tile(z4, 2),\n",
    "            },\n",
    "        )\n",
    "        dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "        dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "        covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "        y = dta_long[\"y\"].values\n",
    "        post = dta_long[\"post\"].values\n",
    "        D = dta_long[\"d\"].values\n",
    "\n",
    "        result = drdid_rc(y, post, D, covariates)\n",
    "\n",
    "        ATTE_estimates.append(result[\"ATT\"])\n",
    "        asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "\n",
    "    # Calculate average bias, median bias, and RMSE\n",
    "    true_ATT = 0\n",
    "    average_bias = np.mean(ATTE_estimates) - true_ATT\n",
    "    median_bias = np.median(ATTE_estimates) - true_ATT\n",
    "    rmse = np.sqrt(np.mean((np.array(ATTE_estimates) - true_ATT) ** 2))\n",
    "\n",
    "    # Calculate average of the variance\n",
    "    average_variance = np.mean(asymptotic_variance)\n",
    "\n",
    "    # Display the results\n",
    "    return {\n",
    "        \"Average Bias\": average_bias,\n",
    "        \"Median Bias\": median_bias,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Average Variance of ATT\": average_variance,\n",
    "    }\n",
    "\n",
    "\n",
    "sz_dr_dgp4()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
