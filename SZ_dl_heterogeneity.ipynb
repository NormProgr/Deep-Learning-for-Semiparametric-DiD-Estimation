{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strong heterogeneity with DGP where PS and OR are incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treatment heterogeneity\n",
    "- index_att = 10 * (z1 + z2 - z3 + z4)  # Strong heterogeneity in treatment effect\n",
    "- This means that the treatment effect is a complex function of the covariates z1,z2,z3, and z4 Consequently, the effect of the treatment is not uniform across all individuals but varies depending on these covariates.\n",
    "- Heterogeneous Treatment Effects: Variability in the treatment effect itself, based on measured covariates.\n",
    "\n",
    "Not related is:\n",
    "- unobserved heterogeneity of v = np.random.normal(index_lin, 1, n)\n",
    "- This introduces variability in the outcomes that is correlated with the observed covariates.\n",
    "- Unobserved heterogeneity: Variability in the outcome due to unmeasured factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Header\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.losses.append(logs.get(\"loss\"))\n",
    "        self.val_losses.append(logs.get(\"val_loss\"))\n",
    "\n",
    "    def get_min_loss(self):\n",
    "        return min(self.losses), min(self.val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPW DL heterogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import iqr, norm\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def std_ipw_did_rc(\n",
    "    y,\n",
    "    post,\n",
    "    D,\n",
    "    covariates=None,\n",
    "    i_weights=None,\n",
    "    boot=False,\n",
    "    boot_type=\"weighted\",\n",
    "    nboot=None,\n",
    "    inffunc=False,\n",
    "):\n",
    "    # Convert inputs to numpy arrays\n",
    "    D = np.asarray(D).flatten()\n",
    "    n = len(D)\n",
    "    y = np.asarray(y).flatten()\n",
    "    post = np.asarray(post).flatten()\n",
    "\n",
    "    # Add constant to covariate vector\n",
    "    if covariates is None:\n",
    "        int_cov = np.ones((n, 1))\n",
    "    else:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "    elif np.min(i_weights) < 0:\n",
    "        msg = \"i.weights must be non-negative\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    def create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg):\n",
    "        np.random.seed(42)\n",
    "\n",
    "        inputs = Input(shape=(input_dim,))\n",
    "        x = Dense(units, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        for _ in range(depth - 2):\n",
    "            x = Dense(units, kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = ReLU()(x)\n",
    "\n",
    "        outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        # Compile the model with Adam optimizer and specified learning rate\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Assume `int_cov`, `D`, and `i_weights` are already defined as in your original code\n",
    "    depth = 3\n",
    "    units = 32\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.01\n",
    "    input_dim = int_cov.shape[1]\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    (\n",
    "        int_cov_train,\n",
    "        int_cov_val,\n",
    "        D_train,\n",
    "        D_val,\n",
    "        weights_train,\n",
    "        weights_val,\n",
    "    ) = train_test_split(int_cov, D, i_weights, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and compile the model with the optimal hyperparameters\n",
    "    model = create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg)\n",
    "    history = LossHistory()\n",
    "\n",
    "    # Train the model\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        int_cov_train,\n",
    "        D_train,\n",
    "        sample_weight=weights_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        validation_data=(int_cov_val, D_val, weights_val),\n",
    "        callbacks=[history],\n",
    "    )\n",
    "    min_training_loss, min_validation_loss = history.get_min_loss()\n",
    "\n",
    "    # Predict the probabilities\n",
    "    ps_fit = model.predict(int_cov).flatten()\n",
    "\n",
    "    # Ensure no values are exactly 0 or 1\n",
    "\n",
    "    # Compute IPW estimator\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * y / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * y / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * y / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * y / np.mean(w_cont_post)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    # ATT estimator\n",
    "    ipw_att = (att_treat_post - att_treat_pre) - (att_cont_post - att_cont_pre)\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    score_ps = i_weights.reshape(-1, 1) * (D - ps_fit).reshape(-1, 1) * int_cov\n",
    "    hessian_ps = np.linalg.inv(np.dot(score_ps.T, score_ps) / n)\n",
    "    asy_lin_rep_ps = np.dot(score_ps, hessian_ps)\n",
    "\n",
    "    # Influence function of the \"treat\" component\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "    inf_treat = inf_treat_post - inf_treat_pre\n",
    "\n",
    "    # Influence function of the control component\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Estimation effect from gamma hat (pscore)\n",
    "    M2_pre = np.mean(\n",
    "        w_cont_pre.reshape(-1, 1)\n",
    "        * (y - att_cont_pre).reshape(-1, 1)\n",
    "        * int_cov\n",
    "        / np.mean(w_cont_pre),\n",
    "        axis=0,\n",
    "    )\n",
    "    M2_post = np.mean(\n",
    "        w_cont_post.reshape(-1, 1)\n",
    "        * (y - att_cont_post).reshape(-1, 1)\n",
    "        * int_cov\n",
    "        / np.mean(w_cont_post),\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    inf_cont_ps = np.dot(asy_lin_rep_ps, (M2_post - M2_pre))\n",
    "    inf_cont += inf_cont_ps\n",
    "\n",
    "    # Influence function of the DR estimator\n",
    "    att_inf_func = inf_treat - inf_cont\n",
    "\n",
    "    if not boot:\n",
    "        # Estimate standard error\n",
    "        se_att = np.std(att_inf_func) / np.sqrt(n)\n",
    "        uci = ipw_att + 1.96 * se_att\n",
    "        lci = ipw_att - 1.96 * se_att\n",
    "        ipw_boot = None\n",
    "    else:\n",
    "        if nboot is None:\n",
    "            nboot = 999\n",
    "        if boot_type == \"multiplier\":\n",
    "            # Multiplier bootstrap\n",
    "            multipliers = np.random.normal(size=(nboot, n))\n",
    "            ipw_boot = [np.mean(m * att_inf_func) for m in multipliers]\n",
    "            se_att = iqr(ipw_boot) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.percentile(np.abs(ipw_boot / se_att), 95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "        else:\n",
    "            # Weighted bootstrap\n",
    "            ipw_boot = [\n",
    "                wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights)\n",
    "                for _ in range(nboot)\n",
    "            ]\n",
    "            se_att = iqr(ipw_boot - ipw_att) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.percentile(np.abs((ipw_boot - ipw_att) / se_att), 95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "\n",
    "    if not inffunc:\n",
    "        att_inf_func = None\n",
    "\n",
    "    return {\n",
    "        \"ATT\": ipw_att,\n",
    "        \"se\": se_att,\n",
    "        \"uci\": uci,\n",
    "        \"lci\": lci,\n",
    "        \"boots\": ipw_boot,\n",
    "        \"att_inf_func\": att_inf_func,\n",
    "        \"min_training_loss\": min_training_loss,\n",
    "        \"min_validation_loss\": min_validation_loss,\n",
    "    }\n",
    "\n",
    "\n",
    "def wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights):\n",
    "    boot_weights = np.random.choice(np.arange(1, n + 1), size=n, replace=True)\n",
    "    return std_ipw_did_rc(y, post, D, int_cov, i_weights=boot_weights)[\"ATT\"]\n",
    "\n",
    "\n",
    "# Simulation setup\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def ipw_sim_run(dgp_type):\n",
    "    # Define parameters\n",
    "    np.random.seed(42)  # You can use any integer value as the seed\n",
    "    # Sample size\n",
    "    n = 1000\n",
    "\n",
    "    # pscore index (strength of common support)\n",
    "    Xsi_ps = 0.75\n",
    "\n",
    "    # Proportion in each period\n",
    "\n",
    "    # Number of bootstrapped draws\n",
    "\n",
    "    # Mean and Std deviation of Z's without truncation\n",
    "    mean_z1 = np.exp(0.25 / 2)\n",
    "    sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "    mean_z2 = 10\n",
    "    sd_z2 = 0.54164\n",
    "    mean_z3 = 0.21887\n",
    "    sd_z3 = 0.04453\n",
    "    mean_z4 = 402\n",
    "    sd_z4 = 56.63891\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    ATTE_estimates = []\n",
    "    asymptotic_variance = []\n",
    "    min_training_losses = []\n",
    "    min_validation_losses = []\n",
    "    coverage_indicators = []\n",
    "\n",
    "    for _i in range(1000):\n",
    "        # Gen covariates\n",
    "        x1 = np.random.normal(0, 1, n)\n",
    "        x2 = np.random.normal(0, 1, n)\n",
    "        x3 = np.random.normal(0, 1, n)\n",
    "        x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "        z1 = np.exp(x1 / 2)\n",
    "        z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "        z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "        z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "        z1 = (z1 - mean_z1) / sd_z1\n",
    "        z2 = (z2 - mean_z2) / sd_z2\n",
    "        z3 = (z3 - mean_z3) / sd_z3\n",
    "        z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "        np.column_stack((x1, x2, x3, x4))\n",
    "        np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "        # Gen treatment groups\n",
    "        # Propensity score\n",
    "        pi = 1 / (1 + np.exp(-Xsi_ps * (-x1 + 0.5 * x2 - 0.25 * x3 - 0.1 * x4)))\n",
    "        d = np.random.rand(n) <= pi\n",
    "\n",
    "        # Generate aux indexes for the potential outcomes\n",
    "        index_lin = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "        # Create heterogeneous effects for the ATT, which is set approximately equal to zero\n",
    "        index_unobs_het = d * index_lin\n",
    "        # Create heterogeneous effects for the ATT\n",
    "        index_att = 10 * (z1 + z2 - z3 + z4)  # Strong heterogeneity in treatment effect\n",
    "\n",
    "        # This is the key for consistency of outcome regression\n",
    "        index_trend = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "        # v is the unobserved heterogeneity\n",
    "        v = np.random.normal(index_unobs_het, 1, n)\n",
    "\n",
    "        # Gen realized outcome at time 0\n",
    "        y00 = index_lin + v + np.random.normal(size=n)\n",
    "        y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "        # Gen outcomes at time 1\n",
    "        # First let's generate potential outcomes: y_1_potential\n",
    "        y01 = index_lin + v + np.random.normal(size=n) + index_trend\n",
    "        y11 = index_lin + v + np.random.normal(size=n) + index_trend + index_att\n",
    "\n",
    "        # Generate \"T\"\n",
    "        ti_nt = 0.5\n",
    "        ti_t = 0.5\n",
    "        ti = d * ti_t + (1 - d) * ti_nt\n",
    "        post = np.random.rand(n) <= ti\n",
    "\n",
    "        # Combine outcomes into panel data format\n",
    "        y = np.where(\n",
    "            d & post,\n",
    "            y11,\n",
    "            np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "        )\n",
    "\n",
    "        # Gen id\n",
    "        id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "        time = np.tile([0, 1], n)\n",
    "\n",
    "        # Put in a long data frame\n",
    "        dta_long = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": id_,\n",
    "                \"time\": time,\n",
    "                \"y\": np.tile(y, 2),\n",
    "                \"post\": np.tile(post.astype(int), 2),\n",
    "                \"d\": np.tile(d.astype(int), 2),\n",
    "                \"x1\": np.tile(z1, 2),\n",
    "                \"x2\": np.tile(z2, 2),\n",
    "                \"x3\": np.tile(z3, 2),\n",
    "                \"x4\": np.tile(z4, 2),\n",
    "            },\n",
    "        )\n",
    "        dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "        dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "        # Run the IPW-DID estimator\n",
    "        covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "        y = dta_long[\"y\"].values\n",
    "        post = dta_long[\"post\"].values\n",
    "        D = dta_long[\"d\"].values\n",
    "\n",
    "        result = std_ipw_did_rc(y, post, D, covariates)\n",
    "\n",
    "        ATTE_estimates.append(result[\"ATT\"])\n",
    "        asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "        min_training_losses.append(result[\"min_training_loss\"])\n",
    "        min_validation_losses.append(result[\"min_validation_loss\"])\n",
    "\n",
    "        # Calculate coverage indicator\n",
    "        coverage_indicator = int(result[\"lci\"] <= 0 <= result[\"uci\"])\n",
    "        coverage_indicators.append(coverage_indicator)\n",
    "    # Calculate average bias, median bias, and RMSE\n",
    "    true_ATT = 0\n",
    "\n",
    "    # Bias calculations\n",
    "    biases = np.array(ATTE_estimates) - true_ATT\n",
    "    average_bias = np.mean(biases)\n",
    "    median_bias = np.median(biases)\n",
    "    average_variance = np.mean(asymptotic_variance)\n",
    "    # RMSE calculation\n",
    "    rmse = np.sqrt(np.mean(biases**2))\n",
    "    avg_min_training_loss = np.mean(min_training_losses)\n",
    "    avg_min_validation_loss = np.mean(min_validation_losses)\n",
    "    avg_coverage_prob = np.mean(\n",
    "        coverage_indicators,\n",
    "    )  # Calculate average coverage probability\n",
    "\n",
    "    results = {\n",
    "        \"avg_bias\": average_bias,\n",
    "        \"med_bias\": median_bias,\n",
    "        \"rmse\": rmse,\n",
    "        \"average_variance\": average_variance,\n",
    "        \"average_min_training_loss\": avg_min_training_loss,\n",
    "        \"average_min_validation_loss\": avg_min_validation_loss,\n",
    "        \"avg_coverage_prob\": avg_coverage_prob,\n",
    "    }\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(\"bld/het_results\", exist_ok=True)\n",
    "    latex_filename = f\"bld/het_results/het_ipw_dl_{dgp_type}.tex\"\n",
    "\n",
    "    # Writing the results to a LaTeX file\n",
    "    with open(latex_filename, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[ht]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{|l|r|}\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"Metric & Value \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key.replace('_', ' ').title()} & {value:.4f} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\n",
    "            f\"\\\\caption{{Simulation Results for double robust deep learning with DGP Type {dgp_type}}}\\n\",\n",
    "        )\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "    # Save ATTE estimates as a pickle file\n",
    "    os.makedirs(\"bld/het_results\", exist_ok=True)\n",
    "    pickle_filename = f\"bld/het_results/het_ipw_dl_atte_estimates_dgp_{dgp_type}.pkl\"\n",
    "    with open(pickle_filename, \"wb\") as f:\n",
    "        pickle.dump(ATTE_estimates, f)\n",
    "\n",
    "    # Display the results\n",
    "    return {\n",
    "        \"Average Bias\": average_bias,\n",
    "        \"Median Bias\": median_bias,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Average Variance of ATT\": average_variance,\n",
    "        \"average_min_training_loss\": avg_min_training_loss,\n",
    "        \"average_min_validation_loss\": avg_min_validation_loss,\n",
    "        \"Average Coverage Probability\": avg_coverage_prob,\n",
    "    }\n",
    "\n",
    "\n",
    "ipw_sim_run(dgp_type=\"4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DR DL NN Heterogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DRDID function\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(units, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    for _ in range(depth - 2):\n",
    "        x = Dense(units, kernel_regularizer=l2(l2_reg))(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with Adam optimizer and specified learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def drdid_rc(y, post, D, covariates=None, i_weights=None):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Ensure D is a vector\n",
    "    D = np.asarray(D)\n",
    "    # Sample size\n",
    "    n = len(D)\n",
    "    # Ensure y is a vector\n",
    "    y = np.asarray(y)\n",
    "    # Ensure post is a vector\n",
    "    post = np.asarray(post)\n",
    "    # Add constant to covariate vector\n",
    "    int_cov = np.ones((n, 1))\n",
    "    if covariates is not None:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "\n",
    "    # Define parameters for the neural network\n",
    "    depth = 3\n",
    "    units = 32\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.01\n",
    "    input_dim = int_cov.shape[1]\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    (\n",
    "        int_cov_train,\n",
    "        int_cov_val,\n",
    "        D_train,\n",
    "        D_val,\n",
    "        weights_train,\n",
    "        weights_val,\n",
    "    ) = train_test_split(int_cov, D, i_weights, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and compile the model with the optimal hyperparameters\n",
    "    model = create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg)\n",
    "\n",
    "    history = LossHistory()\n",
    "    # Train the model with validation data\n",
    "    model.fit(\n",
    "        int_cov_train,\n",
    "        D_train,\n",
    "        sample_weight=weights_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        validation_data=(int_cov_val, D_val, weights_val),\n",
    "        callbacks=[history],\n",
    "    )\n",
    "    min_training_loss, min_validation_loss = history.get_min_loss()\n",
    "\n",
    "    # Predict the probabilities\n",
    "    ps_fit = model.predict(int_cov).flatten()\n",
    "\n",
    "    # Ensure no values are exactly 0 or 1\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the pre-treatment period, using OLS\n",
    "    reg_cont_pre = sm.WLS(\n",
    "        y[(D == 0) & (post == 0)],\n",
    "        int_cov[(D == 0) & (post == 0)],\n",
    "        weights=i_weights[(D == 0) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_cont_pre = int_cov @ reg_cont_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the post-treatment period, using OLS\n",
    "    reg_cont_post = sm.WLS(\n",
    "        y[(D == 0) & (post == 1)],\n",
    "        int_cov[(D == 0) & (post == 1)],\n",
    "        weights=i_weights[(D == 0) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_cont_post = int_cov @ reg_cont_post.params\n",
    "\n",
    "    # Combine the ORs for control group\n",
    "    out_y_cont = post * out_y_cont_post + (1 - post) * out_y_cont_pre\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the pre-treatment period, using OLS\n",
    "    reg_treat_pre = sm.WLS(\n",
    "        y[(D == 1) & (post == 0)],\n",
    "        int_cov[(D == 1) & (post == 0)],\n",
    "        weights=i_weights[(D == 1) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_treat_pre = int_cov @ reg_treat_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the post-treatment period, using OLS\n",
    "    reg_treat_post = sm.WLS(\n",
    "        y[(D == 1) & (post == 1)],\n",
    "        int_cov[(D == 1) & (post == 1)],\n",
    "        weights=i_weights[(D == 1) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_treat_post = int_cov @ reg_treat_post.params\n",
    "\n",
    "    # Weights\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    w_d = i_weights * D\n",
    "    w_dt1 = i_weights * D * post\n",
    "    w_dt0 = i_weights * D * (1 - post)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * (y - out_y_cont) / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * (y - out_y_cont) / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * (y - out_y_cont) / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * (y - out_y_cont) / np.mean(w_cont_post)\n",
    "\n",
    "    # Extra elements for the locally efficient DRDID\n",
    "    eta_d_post = w_d * (out_y_treat_post - out_y_cont_post) / np.mean(w_d)\n",
    "    eta_dt1_post = w_dt1 * (out_y_treat_post - out_y_cont_post) / np.mean(w_dt1)\n",
    "    eta_d_pre = w_d * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_d)\n",
    "    eta_dt0_pre = w_dt0 * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_dt0)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    att_d_post = np.mean(eta_d_post)\n",
    "    att_dt1_post = np.mean(eta_dt1_post)\n",
    "    att_d_pre = np.mean(eta_d_pre)\n",
    "    att_dt0_pre = np.mean(eta_dt0_pre)\n",
    "\n",
    "    # ATT estimator\n",
    "    dr_att = (\n",
    "        (att_treat_post - att_treat_pre)\n",
    "        - (att_cont_post - att_cont_pre)\n",
    "        + (att_d_post - att_dt1_post)\n",
    "        - (att_d_pre - att_dt0_pre)\n",
    "    )\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    # Leading term of the influence function: no estimation effect\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M1_post = -np.mean(\n",
    "        w_treat_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_post)\n",
    "    M1_pre = -np.mean(\n",
    "        w_treat_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_treat_or_post = np.dot(reg_cont_post.cov_params(), M1_post)\n",
    "    inf_treat_or_pre = np.dot(reg_cont_pre.cov_params(), M1_pre)\n",
    "    inf_treat_or = inf_treat_or_post + inf_treat_or_pre\n",
    "\n",
    "    # Influence function for the treated component\n",
    "    inf_treat = inf_treat_post - inf_treat_pre + np.sum(inf_treat_or)\n",
    "\n",
    "    # Now, get the influence function of control component\n",
    "    # Leading term of the influence function: no estimation effect from nuisance parameters\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "\n",
    "    # Influence function for the control component\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Get the influence function of the inefficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func1 = inf_treat - inf_cont\n",
    "\n",
    "    # Now, we only need to get the influence function of the adjustment terms\n",
    "    # First, the terms as if all OR parameters were known\n",
    "    inf_eff1 = eta_d_post - w_d * att_d_post / np.mean(w_d)\n",
    "    inf_eff2 = eta_dt1_post - w_dt1 * att_dt1_post / np.mean(w_dt1)\n",
    "    inf_eff3 = eta_d_pre - w_d * att_d_pre / np.mean(w_d)\n",
    "    inf_eff4 = eta_dt0_pre - w_dt0 * att_dt0_pre / np.mean(w_dt0)\n",
    "    inf_eff = (inf_eff1 - inf_eff2) - (inf_eff3 - inf_eff4)\n",
    "\n",
    "    # Now the estimation effect of the OR coefficients\n",
    "    mom_post = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt1 / np.mean(w_dt1))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    mom_pre = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt0 / np.mean(w_dt0))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    inf_or_post = np.dot(\n",
    "        (reg_treat_post.cov_params() - reg_cont_post.cov_params()),\n",
    "        mom_post,\n",
    "    )\n",
    "    inf_or_pre = np.dot(\n",
    "        (reg_treat_pre.cov_params() - reg_cont_pre.cov_params()),\n",
    "        mom_pre,\n",
    "    )\n",
    "    inf_or = inf_or_post - inf_or_pre\n",
    "    inf_or = np.sum(inf_or)\n",
    "\n",
    "    # Get the influence function of the locally efficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func = dr_att_inf_func1 + inf_eff + inf_or\n",
    "\n",
    "    # Estimate of standard error\n",
    "    se_dr_att = np.std(dr_att_inf_func) / np.sqrt(n)\n",
    "\n",
    "    uci = dr_att + 1.96 * se_dr_att  # Upper confidence interval\n",
    "    lci = dr_att - 1.96 * se_dr_att  # Lower confidence interval\n",
    "    return {\n",
    "        \"ATT\": dr_att,\n",
    "        \"se\": se_dr_att,\n",
    "        \"min_training_loss\": min_training_loss,\n",
    "        \"min_validation_loss\": min_validation_loss,\n",
    "        \"uci\": uci,\n",
    "        \"lci\": lci,\n",
    "    }\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "\n",
    "def sz_dl_dgp4(dgp_type):\n",
    "    np.random.seed(42)  # You can use any integer value as the seed\n",
    "    # Sample size\n",
    "    n = 1000\n",
    "\n",
    "    # pscore index (strength of common support)\n",
    "    Xsi_ps = 0.75\n",
    "\n",
    "    # Proportion in each period\n",
    "\n",
    "    # Number of bootstrapped draws\n",
    "\n",
    "    # Mean and Std deviation of Z's without truncation\n",
    "    mean_z1 = np.exp(0.25 / 2)\n",
    "    sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "    mean_z2 = 10\n",
    "    sd_z2 = 0.54164\n",
    "    mean_z3 = 0.21887\n",
    "    sd_z3 = 0.04453\n",
    "    mean_z4 = 402\n",
    "    sd_z4 = 56.63891\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    ATTE_estimates = []\n",
    "    asymptotic_variance = []\n",
    "    min_training_losses = []\n",
    "    min_validation_losses = []\n",
    "    coverage_indicators = []\n",
    "    individual_coverage_probs = []\n",
    "    for _i in range(1000):\n",
    "        # Gen covariates\n",
    "        x1 = np.random.normal(0, 1, n)\n",
    "        x2 = np.random.normal(0, 1, n)\n",
    "        x3 = np.random.normal(0, 1, n)\n",
    "        x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "        z1 = np.exp(x1 / 2)\n",
    "        z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "        z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "        z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "        z1 = (z1 - mean_z1) / sd_z1\n",
    "        z2 = (z2 - mean_z2) / sd_z2\n",
    "        z3 = (z3 - mean_z3) / sd_z3\n",
    "        z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "        np.column_stack((x1, x2, x3, x4))\n",
    "        np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "        # Gen treatment groups\n",
    "        # Propensity score\n",
    "        pi = 1 / (1 + np.exp(-Xsi_ps * (-x1 + 0.5 * x2 - 0.25 * x3 - 0.1 * x4)))\n",
    "        d = np.random.rand(n) <= pi\n",
    "\n",
    "        # Generate aux indexes for the potential outcomes\n",
    "        index_lin = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "        # Create heterogeneous effects for the ATT, which is set approximately equal to zero\n",
    "        index_unobs_het = d * index_lin\n",
    "        index_att = 0\n",
    "\n",
    "        # This is the key for consistency of outcome regression\n",
    "        index_trend = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "        # Create heterogeneous effects for the ATT\n",
    "        index_att = 10 * (z1 + z2 - z3 + z4)  # Strong heterogeneity in treatment effect\n",
    "        # v is the unobserved heterogeneity\n",
    "        v = np.random.normal(index_unobs_het, 1, n)\n",
    "\n",
    "        # Gen realized outcome at time 0\n",
    "        y00 = index_lin + v + np.random.normal(size=n)\n",
    "        y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "        # Gen outcomes at time 1\n",
    "        # First let's generate potential outcomes: y_1_potential\n",
    "        y01 = index_lin + v + np.random.normal(size=n) + index_trend\n",
    "        y11 = index_lin + v + np.random.normal(size=n) + index_trend + index_att\n",
    "\n",
    "        # Generate \"T\"\n",
    "        ti_nt = 0.5\n",
    "        ti_t = 0.5\n",
    "        ti = d * ti_t + (1 - d) * ti_nt\n",
    "        post = np.random.rand(n) <= ti\n",
    "\n",
    "        # Combine outcomes into panel data format\n",
    "        y = np.where(\n",
    "            d & post,\n",
    "            y11,\n",
    "            np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "        )\n",
    "\n",
    "        # Gen id\n",
    "        id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "        time = np.tile([0, 1], n)\n",
    "\n",
    "        # Put in a long data frame\n",
    "        dta_long = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": id_,\n",
    "                \"time\": time,\n",
    "                \"y\": np.tile(y, 2),\n",
    "                \"post\": np.tile(post.astype(int), 2),\n",
    "                \"d\": np.tile(d.astype(int), 2),\n",
    "                \"x1\": np.tile(z1, 2),\n",
    "                \"x2\": np.tile(z2, 2),\n",
    "                \"x3\": np.tile(z3, 2),\n",
    "                \"x4\": np.tile(z4, 2),\n",
    "            },\n",
    "        )\n",
    "        dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "        dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "        # Run the IPW-DID estimator\n",
    "        covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "        y = dta_long[\"y\"].values\n",
    "        post = dta_long[\"post\"].values\n",
    "        D = dta_long[\"d\"].values\n",
    "\n",
    "        result = drdid_rc(y, post, D, covariates)\n",
    "\n",
    "        ATTE_estimates.append(result[\"ATT\"])\n",
    "        asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "        min_training_losses.append(result[\"min_training_loss\"])\n",
    "        min_validation_losses.append(result[\"min_validation_loss\"])\n",
    "\n",
    "        coverage_indicator = int(result[\"lci\"] <= 0 <= result[\"uci\"])\n",
    "        coverage_indicators.append(coverage_indicator)\n",
    "        individual_coverage_probs.append(coverage_indicator)\n",
    "\n",
    "    # Calculate average bias, median bias, and RMSE\n",
    "    true_ATT = 0\n",
    "    average_bias = np.mean(ATTE_estimates) - true_ATT\n",
    "    median_bias = np.median(ATTE_estimates) - true_ATT\n",
    "    rmse = np.sqrt(np.mean((np.array(ATTE_estimates) - true_ATT) ** 2))\n",
    "\n",
    "    avg_min_training_loss = np.mean(min_training_losses)\n",
    "    avg_min_validation_loss = np.mean(min_validation_losses)\n",
    "    avg_coverage_prob = np.mean(\n",
    "        coverage_indicators,\n",
    "    )  # Calculate average coverage probability\n",
    "\n",
    "    # Calculate average of the variance\n",
    "    average_variance = np.mean(asymptotic_variance)\n",
    "    results = {\n",
    "        \"avg_bias\": average_bias,\n",
    "        \"med_bias\": median_bias,\n",
    "        \"rmse\": rmse,\n",
    "        \"average_variance\": average_variance,\n",
    "        \"average_min_training_loss\": avg_min_training_loss,\n",
    "        \"average_min_validation_loss\": avg_min_validation_loss,\n",
    "        \"avg_coverage_prob\": avg_coverage_prob,  # Include average coverage probability\n",
    "    }\n",
    "    # Ensure the directory exists\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(\"bld/het_results\", exist_ok=True)\n",
    "    latex_filename = f\"bld/het_results/het_dr_dl_{dgp_type}.tex\"\n",
    "\n",
    "    # Writing the results to a LaTeX file\n",
    "    with open(latex_filename, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[ht]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{|l|r|}\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"Metric & Value \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key.replace('_', ' ').title()} & {value:.4f} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\n",
    "            f\"\\\\caption{{Simulation Results for double robust deep learning with DGP Type {dgp_type}}}\\n\",\n",
    "        )\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "    # Save ATTE estimates as a pickle file\n",
    "    os.makedirs(\"bld/het_results\", exist_ok=True)\n",
    "    pickle_filename = f\"bld/het_results/het_dr_dl_{dgp_type}.pkl\"\n",
    "    with open(pickle_filename, \"wb\") as f:\n",
    "        pickle.dump(ATTE_estimates, f)\n",
    "\n",
    "    return {\n",
    "        \"Average Bias\": average_bias,\n",
    "        \"Median Bias\": median_bias,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Average Variance of ATT\": average_variance,\n",
    "        \"average_min_training_loss\": avg_min_training_loss,\n",
    "        \"average_min_validation_loss\": avg_min_validation_loss,\n",
    "        \"Average Coverage Probability\": avg_coverage_prob,\n",
    "    }\n",
    "\n",
    "\n",
    "sz_dl_dgp4(dgp_type=\"4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosis Probability Bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.losses.append(logs.get(\"loss\"))\n",
    "        self.val_losses.append(logs.get(\"val_loss\"))\n",
    "\n",
    "    def get_min_loss(self):\n",
    "        return min(self.losses), min(self.val_losses)\n",
    "\n",
    "\n",
    "def create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(units, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    for _ in range(depth - 2):\n",
    "        x = Dense(units, kernel_regularizer=l2(l2_reg))(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with Adam optimizer and specified learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def drdid_rc(y, post, D, covariates=None, i_weights=None):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Ensure D is a vector\n",
    "    D = np.asarray(D)\n",
    "    # Sample size\n",
    "    n = len(D)\n",
    "    # Ensure y is a vector\n",
    "    y = np.asarray(y)\n",
    "    # Ensure post is a vector\n",
    "    post = np.asarray(post)\n",
    "    # Add constant to covariate vector\n",
    "    int_cov = np.ones((n, 1))\n",
    "    if covariates is not None:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "\n",
    "    # Define parameters for the neural network\n",
    "    depth = 3\n",
    "    units = 32\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.01\n",
    "    input_dim = int_cov.shape[1]\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    (\n",
    "        int_cov_train,\n",
    "        int_cov_val,\n",
    "        D_train,\n",
    "        D_val,\n",
    "        weights_train,\n",
    "        weights_val,\n",
    "    ) = train_test_split(int_cov, D, i_weights, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and compile the model with the optimal hyperparameters\n",
    "    model = create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg)\n",
    "\n",
    "    history = LossHistory()\n",
    "    # Train the model with validation data\n",
    "    model.fit(\n",
    "        int_cov_train,\n",
    "        D_train,\n",
    "        sample_weight=weights_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        validation_data=(int_cov_val, D_val, weights_val),\n",
    "        callbacks=[history],\n",
    "    )\n",
    "    min_training_loss, min_validation_loss = history.get_min_loss()\n",
    "\n",
    "    # Predict the probabilities\n",
    "    ps_fit = model.predict(int_cov).flatten()\n",
    "\n",
    "    # Ensure no values are exactly 0 or 1\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the pre-treatment period, using OLS\n",
    "    reg_cont_pre = sm.WLS(\n",
    "        y[(D == 0) & (post == 0)],\n",
    "        int_cov[(D == 0) & (post == 0)],\n",
    "        weights=i_weights[(D == 0) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_cont_pre = int_cov @ reg_cont_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the control group at the post-treatment period, using OLS\n",
    "    reg_cont_post = sm.WLS(\n",
    "        y[(D == 0) & (post == 1)],\n",
    "        int_cov[(D == 0) & (post == 1)],\n",
    "        weights=i_weights[(D == 0) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_cont_post = int_cov @ reg_cont_post.params\n",
    "\n",
    "    # Combine the ORs for control group\n",
    "    out_y_cont = post * out_y_cont_post + (1 - post) * out_y_cont_pre\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the pre-treatment period, using OLS\n",
    "    reg_treat_pre = sm.WLS(\n",
    "        y[(D == 1) & (post == 0)],\n",
    "        int_cov[(D == 1) & (post == 0)],\n",
    "        weights=i_weights[(D == 1) & (post == 0)],\n",
    "    ).fit()\n",
    "    out_y_treat_pre = int_cov @ reg_treat_pre.params\n",
    "\n",
    "    # Compute the Outcome regression for the treated group at the post-treatment period, using OLS\n",
    "    reg_treat_post = sm.WLS(\n",
    "        y[(D == 1) & (post == 1)],\n",
    "        int_cov[(D == 1) & (post == 1)],\n",
    "        weights=i_weights[(D == 1) & (post == 1)],\n",
    "    ).fit()\n",
    "    out_y_treat_post = int_cov @ reg_treat_post.params\n",
    "\n",
    "    # Weights\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    w_d = i_weights * D\n",
    "    w_dt1 = i_weights * D * post\n",
    "    w_dt0 = i_weights * D * (1 - post)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * (y - out_y_cont) / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * (y - out_y_cont) / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * (y - out_y_cont) / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * (y - out_y_cont) / np.mean(w_cont_post)\n",
    "\n",
    "    # Extra elements for the locally efficient DRDID\n",
    "    eta_d_post = w_d * (out_y_treat_post - out_y_cont_post) / np.mean(w_d)\n",
    "    eta_dt1_post = w_dt1 * (out_y_treat_post - out_y_cont_post) / np.mean(w_dt1)\n",
    "    eta_d_pre = w_d * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_d)\n",
    "    eta_dt0_pre = w_dt0 * (out_y_treat_pre - out_y_cont_pre) / np.mean(w_dt0)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    att_d_post = np.mean(eta_d_post)\n",
    "    att_dt1_post = np.mean(eta_dt1_post)\n",
    "    att_d_pre = np.mean(eta_d_pre)\n",
    "    att_dt0_pre = np.mean(eta_dt0_pre)\n",
    "\n",
    "    # ATT estimator\n",
    "    dr_att = (\n",
    "        (att_treat_post - att_treat_pre)\n",
    "        - (att_cont_post - att_cont_pre)\n",
    "        + (att_d_post - att_dt1_post)\n",
    "        - (att_d_pre - att_dt0_pre)\n",
    "    )\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    # Leading term of the influence function: no estimation effect\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "\n",
    "    # Estimation effect from beta hat from post and pre-periods\n",
    "    M1_post = -np.mean(\n",
    "        w_treat_post[:, np.newaxis] * post[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_post)\n",
    "    M1_pre = -np.mean(\n",
    "        w_treat_pre[:, np.newaxis] * (1 - post)[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_treat_pre)\n",
    "\n",
    "    # Now get the influence function related to the estimation effect related to beta's\n",
    "    inf_treat_or_post = np.dot(reg_cont_post.cov_params(), M1_post)\n",
    "    inf_treat_or_pre = np.dot(reg_cont_pre.cov_params(), M1_pre)\n",
    "    inf_treat_or = inf_treat_or_post + inf_treat_or_pre\n",
    "\n",
    "    # Influence function for the treated component\n",
    "    inf_treat = inf_treat_post - inf_treat_pre + np.sum(inf_treat_or)\n",
    "\n",
    "    # Now, get the influence function of control component\n",
    "    # Leading term of the influence function: no estimation effect from nuisance parameters\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "\n",
    "    # Influence function for the control component\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Get the influence function of the inefficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func1 = inf_treat - inf_cont\n",
    "\n",
    "    # Now, we only need to get the influence function of the adjustment terms\n",
    "    # First, the terms as if all OR parameters were known\n",
    "    inf_eff1 = eta_d_post - w_d * att_d_post / np.mean(w_d)\n",
    "    inf_eff2 = eta_dt1_post - w_dt1 * att_dt1_post / np.mean(w_dt1)\n",
    "    inf_eff3 = eta_d_pre - w_d * att_d_pre / np.mean(w_d)\n",
    "    inf_eff4 = eta_dt0_pre - w_dt0 * att_dt0_pre / np.mean(w_dt0)\n",
    "    inf_eff = (inf_eff1 - inf_eff2) - (inf_eff3 - inf_eff4)\n",
    "\n",
    "    # Now the estimation effect of the OR coefficients\n",
    "    mom_post = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt1 / np.mean(w_dt1))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    mom_pre = np.mean(\n",
    "        (w_d / np.mean(w_d) - w_dt0 / np.mean(w_dt0))[:, np.newaxis] * int_cov,\n",
    "        axis=0,\n",
    "    )\n",
    "    inf_or_post = np.dot(\n",
    "        (reg_treat_post.cov_params() - reg_cont_post.cov_params()),\n",
    "        mom_post,\n",
    "    )\n",
    "    inf_or_pre = np.dot(\n",
    "        (reg_treat_pre.cov_params() - reg_cont_pre.cov_params()),\n",
    "        mom_pre,\n",
    "    )\n",
    "    inf_or = inf_or_post - inf_or_pre\n",
    "    inf_or = np.sum(inf_or)\n",
    "\n",
    "    # Get the influence function of the locally efficient DR estimator (put all pieces together)\n",
    "    dr_att_inf_func = dr_att_inf_func1 + inf_eff + inf_or\n",
    "\n",
    "    # Estimate of standard error\n",
    "    se_dr_att = np.std(dr_att_inf_func) / np.sqrt(n)\n",
    "\n",
    "    uci = dr_att + 1.96 * se_dr_att  # Upper confidence interval\n",
    "    lci = dr_att - 1.96 * se_dr_att  # Lower confidence interval\n",
    "    return {\n",
    "        \"ATT\": dr_att,\n",
    "        \"se\": se_dr_att,\n",
    "        \"min_training_loss\": min_training_loss,\n",
    "        \"min_validation_loss\": min_validation_loss,\n",
    "        \"uci\": uci,\n",
    "        \"lci\": lci,\n",
    "    }\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "\n",
    "def sz_dl_dgp4(dgp_type):\n",
    "    np.random.seed(42)  # You can use any integer value as the seed\n",
    "    # Sample size\n",
    "    n = 1000\n",
    "\n",
    "    # pscore index (strength of common support)\n",
    "    Xsi_ps = 0.75\n",
    "\n",
    "    # Proportion in each period\n",
    "\n",
    "    # Number of bootstrapped draws\n",
    "\n",
    "    # Mean and Std deviation of Z's without truncation\n",
    "    mean_z1 = np.exp(0.25 / 2)\n",
    "    sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "    mean_z2 = 10\n",
    "    sd_z2 = 0.54164\n",
    "    mean_z3 = 0.21887\n",
    "    sd_z3 = 0.04453\n",
    "    mean_z4 = 402\n",
    "    sd_z4 = 56.63891\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    ATTE_estimates = []\n",
    "    asymptotic_variance = []\n",
    "    min_training_losses = []\n",
    "    min_validation_losses = []\n",
    "    coverage_indicators = []\n",
    "    individual_coverage_probs = (\n",
    "        []\n",
    "    )  # New list to store individual coverage probabilities\n",
    "    ci_bounds = []\n",
    "    for _i in range(1000):\n",
    "        # Gen covariates\n",
    "        x1 = np.random.normal(0, 1, n)\n",
    "        x2 = np.random.normal(0, 1, n)\n",
    "        x3 = np.random.normal(0, 1, n)\n",
    "        x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "        z1 = np.exp(x1 / 2)\n",
    "        z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "        z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "        z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "        z1 = (z1 - mean_z1) / sd_z1\n",
    "        z2 = (z2 - mean_z2) / sd_z2\n",
    "        z3 = (z3 - mean_z3) / sd_z3\n",
    "        z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "        np.column_stack((x1, x2, x3, x4))\n",
    "        np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "        # Gen treatment groups\n",
    "        # Propensity score\n",
    "        pi = 1 / (1 + np.exp(-Xsi_ps * (-x1 + 0.5 * x2 - 0.25 * x3 - 0.1 * x4)))\n",
    "        d = np.random.rand(n) <= pi\n",
    "\n",
    "        # Generate aux indexes for the potential outcomes\n",
    "        index_lin = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "        # Create heterogeneous effects for the ATT, which is set approximately equal to zero\n",
    "        index_unobs_het = d * index_lin\n",
    "        index_att = 0\n",
    "\n",
    "        # This is the key for consistency of outcome regression\n",
    "        index_trend = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "        # Create heterogeneous effects for the ATT\n",
    "        index_att = 10 * (z1 + z2 - z3 + z4)  # Strong heterogeneity in treatment effect\n",
    "        # v is the unobserved heterogeneity\n",
    "        v = np.random.normal(index_unobs_het, 1, n)\n",
    "\n",
    "        # Gen realized outcome at time 0\n",
    "        y00 = index_lin + v + np.random.normal(size=n)\n",
    "        y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "        # Gen outcomes at time 1\n",
    "        # First let's generate potential outcomes: y_1_potential\n",
    "        y01 = index_lin + v + np.random.normal(size=n) + index_trend\n",
    "        y11 = index_lin + v + np.random.normal(size=n) + index_trend + index_att\n",
    "\n",
    "        # Generate \"T\"\n",
    "        ti_nt = 0.5\n",
    "        ti_t = 0.5\n",
    "        ti = d * ti_t + (1 - d) * ti_nt\n",
    "        post = np.random.rand(n) <= ti\n",
    "\n",
    "        # Combine outcomes into panel data format\n",
    "        y = np.where(\n",
    "            d & post,\n",
    "            y11,\n",
    "            np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "        )\n",
    "\n",
    "        # Gen id\n",
    "        id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "        time = np.tile([0, 1], n)\n",
    "\n",
    "        # Put in a long data frame\n",
    "        dta_long = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": id_,\n",
    "                \"time\": time,\n",
    "                \"y\": np.tile(y, 2),\n",
    "                \"post\": np.tile(post.astype(int), 2),\n",
    "                \"d\": np.tile(d.astype(int), 2),\n",
    "                \"x1\": np.tile(z1, 2),\n",
    "                \"x2\": np.tile(z2, 2),\n",
    "                \"x3\": np.tile(z3, 2),\n",
    "                \"x4\": np.tile(z4, 2),\n",
    "            },\n",
    "        )\n",
    "        dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "        dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "        # Run the IPW-DID estimator\n",
    "        covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "        y = dta_long[\"y\"].values\n",
    "        post = dta_long[\"post\"].values\n",
    "        D = dta_long[\"d\"].values\n",
    "\n",
    "        result = drdid_rc(y, post, D, covariates)\n",
    "\n",
    "        ATTE_estimates.append(result[\"ATT\"])\n",
    "        asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "        min_training_losses.append(result[\"min_training_loss\"])\n",
    "        min_validation_losses.append(result[\"min_validation_loss\"])\n",
    "\n",
    "        # Calculate coverage indicator\n",
    "        coverage_indicator = int(result[\"lci\"] <= 0 <= result[\"uci\"])\n",
    "        coverage_indicators.append(coverage_indicator)\n",
    "\n",
    "        # Calculate individual coverage probability\n",
    "        individual_coverage_probs.append(coverage_indicator)\n",
    "        ci_bounds.append((result[\"lci\"], result[\"uci\"]))\n",
    "\n",
    "    # Calculate average bias, median bias, and RMSE\n",
    "    true_ATT = 0\n",
    "    average_bias = np.mean(ATTE_estimates) - true_ATT\n",
    "    median_bias = np.median(ATTE_estimates) - true_ATT\n",
    "    rmse = np.sqrt(np.mean((np.array(ATTE_estimates) - true_ATT) ** 2))\n",
    "\n",
    "    avg_min_training_loss = np.mean(min_training_losses)\n",
    "    avg_min_validation_loss = np.mean(min_validation_losses)\n",
    "    avg_coverage_prob = np.mean(\n",
    "        coverage_indicators,\n",
    "    )  # Calculate average coverage probability\n",
    "    avg_coverage_prob_indiv = np.mean(individual_coverage_probs)\n",
    "    # Calculate average of the variance\n",
    "    average_variance = np.mean(asymptotic_variance)\n",
    "\n",
    "    # Additional analysis for variability\n",
    "    std_dev = np.std(ATTE_estimates)\n",
    "    range_values = np.ptp(ATTE_estimates)\n",
    "    iqr = np.percentile(ATTE_estimates, 75) - np.percentile(ATTE_estimates, 25)\n",
    "\n",
    "    # Calculate average confidence interval bounds\n",
    "    avg_lci = np.mean([bounds[0] for bounds in ci_bounds])\n",
    "    avg_uci = np.mean([bounds[1] for bounds in ci_bounds])\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(\n",
    "        ATTE_estimates,\n",
    "        fill=True,\n",
    "        color=\"black\",\n",
    "        bw_adjust=0.5,\n",
    "        alpha=0.3,\n",
    "    )  # Adjust bw_adjust to control smoothness\n",
    "    plt.axvline(x=avg_lci, color=\"black\", linestyle=\"--\")\n",
    "    plt.axvline(x=avg_uci, color=\"black\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Conditional Average Treatment on the Treated Effect\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"./paper/graphs/atte_bounds.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"Average Bias\": average_bias,\n",
    "        \"Median Bias\": median_bias,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Average Variance of ATT\": average_variance,\n",
    "        \"Average Min Training Loss\": avg_min_training_loss,\n",
    "        \"Average Min Validation Loss\": avg_min_validation_loss,\n",
    "        \"Average Coverage Probability\": avg_coverage_prob,\n",
    "        \"Individual Coverage Probs\": avg_coverage_prob_indiv,\n",
    "        \"Standard Deviation\": std_dev,\n",
    "        \"Range\": range_values,\n",
    "        \"IQR\": iqr,\n",
    "        \"CI Bounds\": ci_bounds,  # Return the confidence interval bounds\n",
    "        \"Average Lower CI\": avg_lci,\n",
    "        \"Average Upper CI\": avg_uci,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run the simulation\n",
    "results = sz_dl_dgp4(dgp_type=\"4\")\n",
    "print(results)\n",
    "print(\"Confidence Interval Bounds for Each Run:\")\n",
    "for i, bounds in enumerate(results[\"CI Bounds\"], start=1):\n",
    "    print(f\"Run {i}: Lower CI = {bounds[0]}, Upper CI = {bounds[1]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
