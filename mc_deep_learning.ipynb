{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Robust DiD Estimation with Deep Learning\n",
    "\n",
    "This notebook demonstrates how to use deep learning to estimate the average treatment effect in a difference-in-differences (DiD) setting.\n",
    "\n",
    "Note:\n",
    "- The code is quite computational extensive and may take a while to run.\n",
    "- The code should be run on a GPU to speed up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the simulation and save the results\n",
    "n_reps = 10  # change to 1000 for the full simulation\n",
    "n_obs = 1000\n",
    "true_att = 0.0\n",
    "dgp_type = 4  # choose 1 to 4\n",
    "ATTE = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from doubleml import DoubleMLData, DoubleMLDID\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enable GPU acceleration if available\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=input_dim, activation=\"relu\"))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))  # Assuming binary classification\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_simulation(n_reps, n_obs, ATTE, dgp_type):\n",
    "    # Initialize arrays to store statistics\n",
    "    np.full(n_reps, np.nan)\n",
    "    np.full(n_reps, np.nan)\n",
    "    np.full(n_reps, np.nan)\n",
    "    coverage = np.full(n_reps, np.nan)\n",
    "    avg_variance = np.full(n_reps, np.nan)\n",
    "    ci_length = np.full(n_reps, np.nan)\n",
    "    ATTE_estimates = np.full(n_reps, np.nan)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor=\"loss\", patience=2, verbose=0)\n",
    "\n",
    "    for i_rep in range(n_reps):\n",
    "        x, y, d = make_did_SZ2020(\n",
    "            n_obs=n_obs,\n",
    "            dgp_type=dgp_type,\n",
    "            cross_sectional_data=False,\n",
    "            return_type=\"array\",\n",
    "        )\n",
    "        dml_data = DoubleMLData.from_arrays(x=x, y=y, d=d)\n",
    "\n",
    "        keras_classifier = KerasClassifier(\n",
    "            build_fn=lambda: create_model(x.shape[1]),\n",
    "            epochs=3,\n",
    "            batch_size=32,\n",
    "            verbose=0,\n",
    "            callbacks=[early_stopping],\n",
    "        )\n",
    "\n",
    "        ml_m = Pipeline([(\"scaler\", StandardScaler()), (\"nn\", keras_classifier)])\n",
    "        ml_g = LinearRegression()\n",
    "\n",
    "        dml_plr = DoubleMLDID(dml_data, ml_g, ml_m)\n",
    "        dml_plr.fit()\n",
    "\n",
    "        ATTE_estimates[i_rep] = dml_plr.coef.squeeze()\n",
    "        confint = dml_plr.confint(level=0.95)\n",
    "        coverage[i_rep] = (confint[\"2.5 %\"].iloc[0] <= ATTE) & (\n",
    "            confint[\"97.5 %\"].iloc[0] >= ATTE\n",
    "        )\n",
    "        ci_length[i_rep] = confint[\"97.5 %\"].iloc[0] - confint[\"2.5 %\"].iloc[0]\n",
    "\n",
    "        summary_df = dml_plr.summary\n",
    "        std_err = summary_df.loc[\"d\", \"std err\"]\n",
    "        avg_variance[i_rep] = std_err**2\n",
    "\n",
    "    avg_bias = np.mean(ATTE_estimates - ATTE)\n",
    "    med_bias = np.median(ATTE_estimates - ATTE)\n",
    "    rmse = np.sqrt(np.mean((ATTE_estimates - ATTE) ** 2))\n",
    "    avg_variance = np.mean(avg_variance)\n",
    "    coverage_probability = np.mean(coverage)\n",
    "    avg_ci_length = np.mean(ci_length)\n",
    "\n",
    "    results = {\n",
    "        \"avg_bias\": avg_bias,\n",
    "        \"med_bias\": med_bias,\n",
    "        \"rmse\": rmse,\n",
    "        \"avg_variance\": avg_variance,\n",
    "        \"coverage_probability\": coverage_probability,\n",
    "        \"avg_ci_length\": avg_ci_length,\n",
    "    }\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    latex_filename = f\"bld/tables/dr_DL_sim_results_dgp_{dgp_type}.tex\"\n",
    "\n",
    "    # Writing the results to a LaTeX file\n",
    "    with open(latex_filename, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[ht]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{|l|r|}\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"Metric & Value \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key.replace('_', ' ').title()} & {value:.4f} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\n",
    "            f\"\\\\caption{{Simulation Results for double robust deep learning with DGP Type {dgp_type}}}\\n\",\n",
    "        )\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = run_simulation(n_reps, n_obs, ATTE, dgp_type)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPW DiD Estimation with Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from doubleml import DoubleMLData\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "from scipy.stats import iqr, norm\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define std_ipw_did_rc function\n",
    "\n",
    "\n",
    "def std_ipw_did_rc(\n",
    "    y,\n",
    "    post,\n",
    "    D,\n",
    "    covariates=None,\n",
    "    i_weights=None,\n",
    "    boot=False,\n",
    "    boot_type=\"weighted\",\n",
    "    nboot=None,\n",
    "    inffunc=False,\n",
    "):\n",
    "    # Convert inputs to numpy arrays\n",
    "    D = np.asarray(D)\n",
    "    n = len(D)\n",
    "    y = np.asarray(y)\n",
    "    post = np.asarray(post)\n",
    "\n",
    "    # Add constant to covariate vector\n",
    "    if covariates is None:\n",
    "        int_cov = np.ones((n, 1))\n",
    "    else:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.column_stack((np.ones(n), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "    elif np.min(i_weights) < 0:\n",
    "        msg = \"i.weights must be non-negative\"\n",
    "        raise ValueError(msg)\n",
    "    i_weights = np.asarray(i_weights)\n",
    "\n",
    "    # Pscore estimation (neural network) and fitted values\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(int_cov.shape[1],)))\n",
    "    model.add(\n",
    "        layers.Dense(10, activation=\"relu\", kernel_regularizer=l2(0.01)),\n",
    "    )  # Added L2 regularization\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=l2(0.01)))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    model.fit(int_cov, D, epochs=3, batch_size=32, verbose=0)\n",
    "\n",
    "    ps_fit = model.predict(int_cov).flatten()\n",
    "    ps_fit = np.clip(ps_fit, 1e-16, 1 - 1e-16)\n",
    "\n",
    "    # Compute IPW estimator weights\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * y / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * y / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * y / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * y / np.mean(w_cont_post)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    # ATT estimator\n",
    "    ipw_att = (att_treat_post - att_treat_pre) - (att_cont_post - att_cont_pre)\n",
    "\n",
    "    # Influence function to compute standard error\n",
    "    score_ps = i_weights[:, None] * (D - ps_fit)[:, None] * int_cov\n",
    "    Hessian_ps = np.linalg.inv(np.dot(int_cov.T, score_ps))\n",
    "    asy_lin_rep_ps = np.dot(score_ps, Hessian_ps)\n",
    "\n",
    "    # Influence function of the \"treat\" component\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "    inf_treat = inf_treat_post - inf_treat_pre\n",
    "\n",
    "    # Influence function of the control component\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Estimation effect from gamma hat (pscore)\n",
    "    M2_pre = np.mean(\n",
    "        w_cont_pre[:, None] * (y - att_cont_pre)[:, None] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_pre)\n",
    "    M2_post = np.mean(\n",
    "        w_cont_post[:, None] * (y - att_cont_post)[:, None] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_post)\n",
    "    inf_cont_ps = np.dot(asy_lin_rep_ps, (M2_post - M2_pre))\n",
    "\n",
    "    # Influence function for the control component\n",
    "    inf_cont += inf_cont_ps\n",
    "\n",
    "    # Combine influence functions\n",
    "    att_inf_func = inf_treat - inf_cont\n",
    "\n",
    "    if not boot:\n",
    "        # Standard error\n",
    "        se_att = np.std(att_inf_func) / np.sqrt(n)\n",
    "        uci = ipw_att + 1.96 * se_att\n",
    "        lci = ipw_att - 1.96 * se_att\n",
    "        ipw_boot = None\n",
    "    else:\n",
    "        if nboot is None:\n",
    "            nboot = 999\n",
    "        if boot_type == \"multiplier\":\n",
    "            # Multiplier bootstrap\n",
    "            ipw_boot = mboot_did(att_inf_func, nboot)\n",
    "            se_att = iqr(ipw_boot) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.quantile(np.abs(ipw_boot / se_att), 0.95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "        else:\n",
    "            # Weighted bootstrap\n",
    "            ipw_boot = [\n",
    "                wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights)\n",
    "                for _ in range(nboot)\n",
    "            ]\n",
    "            ipw_boot = np.array(ipw_boot)\n",
    "            se_att = iqr(ipw_boot - ipw_att) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.quantile(np.abs((ipw_boot - ipw_att) / se_att), 0.95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "\n",
    "    if not inffunc:\n",
    "        att_inf_func = None\n",
    "\n",
    "    return {\n",
    "        \"ATT\": ipw_att,\n",
    "        \"se\": se_att,\n",
    "        \"uci\": uci,\n",
    "        \"lci\": lci,\n",
    "        \"boots\": ipw_boot,\n",
    "        \"att_inf_func\": att_inf_func,\n",
    "    }\n",
    "\n",
    "\n",
    "def mboot_did(att_inf_func, nboot):\n",
    "    n = len(att_inf_func)\n",
    "    boots = []\n",
    "    for _ in range(nboot):\n",
    "        weights = np.random.exponential(scale=1.0, size=n)\n",
    "        boot_att = np.sum(weights * att_inf_func) / np.sum(weights)\n",
    "        boots.append(boot_att)\n",
    "    return np.array(boots)\n",
    "\n",
    "\n",
    "def wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights):\n",
    "    indices = np.random.choice(n, n, replace=True)\n",
    "    y_boot = y[indices]\n",
    "    post_boot = post[indices]\n",
    "    D_boot = D[indices]\n",
    "    int_cov_boot = int_cov[indices]\n",
    "    i_weights_boot = i_weights[indices]\n",
    "    return std_ipw_did_rc(\n",
    "        y_boot,\n",
    "        post_boot,\n",
    "        D_boot,\n",
    "        int_cov_boot,\n",
    "        i_weights_boot,\n",
    "        boot=False,\n",
    "    )[\"ATT\"]\n",
    "\n",
    "\n",
    "def perform_simulation(n_reps=10, n_obs=1000, true_att=0.0, dgp_type=1):\n",
    "    att_estimates = []\n",
    "    se_estimates = []\n",
    "    uci_estimates = []\n",
    "    lci_estimates = []\n",
    "\n",
    "    for _ in range(n_reps):\n",
    "        x, y, d = make_did_SZ2020(\n",
    "            n_obs=n_obs,\n",
    "            dgp_type=dgp_type,\n",
    "            cross_sectional_data=False,\n",
    "            return_type=\"array\",\n",
    "        )\n",
    "        covariates = x\n",
    "        post = np.concatenate([np.zeros(n_obs // 2), np.ones(n_obs // 2)])\n",
    "\n",
    "        results = std_ipw_did_rc(y, post, d, covariates)\n",
    "\n",
    "        att_estimates.append(results[\"ATT\"])\n",
    "        se_estimates.append(results[\"se\"])\n",
    "        uci_estimates.append(results[\"uci\"])\n",
    "        lci_estimates.append(results[\"lci\"])\n",
    "\n",
    "    att_estimates = np.array(att_estimates)\n",
    "    se_estimates = np.array(se_estimates)\n",
    "    uci_estimates = np.array(uci_estimates)\n",
    "    lci_estimates = np.array(lci_estimates)\n",
    "\n",
    "    # Calculate measures\n",
    "    biases = att_estimates - true_att\n",
    "    avg_bias = np.mean(biases)\n",
    "    med_bias = np.median(biases)\n",
    "    rmse = np.sqrt(np.mean(biases**2))\n",
    "    var_att = np.var(att_estimates)\n",
    "    avg_var = np.mean(var_att)\n",
    "    ci_lengths = uci_estimates - lci_estimates\n",
    "    coverage = np.mean((lci_estimates <= true_att) & (uci_estimates >= true_att))\n",
    "    avg_ci_length = np.mean(ci_lengths)\n",
    "\n",
    "    return {\n",
    "        \"Average Bias\": avg_bias,\n",
    "        \"Median Bias\": med_bias,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Average Variance of ATT\": avg_var,\n",
    "        \"Coverage\": coverage,\n",
    "        \"Confidence Interval Length\": avg_ci_length,\n",
    "    }\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def save_results_to_latex(measures, dgp_type):\n",
    "    df = pd.DataFrame(list(measures.items()), columns=[\"Measure\", \"Value\"])\n",
    "    latex_filename = f\"bld/ps_estim_DL_results/dr_PS_sim_results_dgp_{dgp_type}.tex\"\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    latex_path = Path(latex_filename)\n",
    "    latex_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the DataFrame to a LaTeX file\n",
    "    df.to_latex(latex_filename, index=False)\n",
    "    print(f\"Results saved to {latex_filename}\")\n",
    "\n",
    "\n",
    "simulation_measures = perform_simulation(n_reps, n_obs, true_att, dgp_type)\n",
    "\n",
    "# Print measures\n",
    "for measure, value in simulation_measures.items():\n",
    "    print(f\"{measure}: {value}\")\n",
    "\n",
    "# Save the results to a LaTeX file\n",
    "save_results_to_latex(simulation_measures, dgp_type)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
