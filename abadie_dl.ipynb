{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header\n",
    "- Set Randomness  \n",
    "- build the tracker for losses\n",
    "- load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.special import expit as logistic_cdf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.losses.append(logs.get(\"loss\"))\n",
    "        self.val_losses.append(logs.get(\"val_loss\"))\n",
    "\n",
    "    def get_min_loss(self):\n",
    "        return min(self.losses), min(self.val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP1 - Deep Learning\n",
    "NON-RANDOMIZED EXPERIMENT WITH X-SPECIFIC TRENDS PROPENSITY SCORES CORRECTLY SPECIFIED, OUTCOME REGRESSION CORRECTLY SPECIFIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit as logistic_cdf\n",
    "from scipy.stats import iqr, norm\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def std_ipw_did_rc(\n",
    "    y,\n",
    "    post,\n",
    "    D,\n",
    "    covariates=None,\n",
    "    i_weights=None,\n",
    "    boot=False,\n",
    "    boot_type=\"weighted\",\n",
    "    nboot=None,\n",
    "    inffunc=False,\n",
    "):\n",
    "    # Convert inputs to numpy arrays\n",
    "    D = np.asarray(D).flatten()\n",
    "    n = len(D)\n",
    "    y = np.asarray(y).flatten()\n",
    "    post = np.asarray(post).flatten()\n",
    "\n",
    "    # Add constant to covariate vector\n",
    "    if covariates is None:\n",
    "        int_cov = np.ones((n, 1))\n",
    "    else:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "    elif np.min(i_weights) < 0:\n",
    "        msg = \"i.weights must be non-negative\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    def create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg):\n",
    "        np.random.seed(42)\n",
    "\n",
    "        inputs = Input(shape=(input_dim,))\n",
    "        x = Dense(units, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        for _ in range(depth - 2):\n",
    "            x = Dense(units, kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = ReLU()(x)\n",
    "\n",
    "        outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        # Compile the model with Adam optimizer and specified learning rate\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Assume `int_cov`, `D`, and `i_weights` are already defined as in your original code\n",
    "    depth = 3\n",
    "    units = 32\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.01\n",
    "    input_dim = int_cov.shape[1]\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    (\n",
    "        int_cov_train,\n",
    "        int_cov_val,\n",
    "        D_train,\n",
    "        D_val,\n",
    "        weights_train,\n",
    "        weights_val,\n",
    "    ) = train_test_split(int_cov, D, i_weights, test_size=0.2, random_state=42)\n",
    "    # Create and compile the model with the optimal hyperparameters\n",
    "    model = create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg)\n",
    "\n",
    "    # Train the model\n",
    "    history = LossHistory()\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        int_cov_train,\n",
    "        D_train,\n",
    "        sample_weight=weights_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        validation_data=(int_cov_val, D_val, weights_val),\n",
    "        callbacks=[history],\n",
    "    )\n",
    "    min_training_loss, min_validation_loss = history.get_min_loss()\n",
    "\n",
    "    # Predict the probabilities\n",
    "    ps_fit = model.predict(int_cov).flatten()\n",
    "\n",
    "    # Ensure no values are exactly 0 or 1\n",
    "    ps_fit = np.clip(ps_fit, 1e-8, 1 - 1e-8)\n",
    "\n",
    "    # Compute IPW estimator\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * y / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * y / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * y / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * y / np.mean(w_cont_post)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    # ATT estimator\n",
    "    ipw_att = (att_treat_post - att_treat_pre) - (att_cont_post - att_cont_pre)\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    score_ps = i_weights.reshape(-1, 1) * (D - ps_fit).reshape(-1, 1) * int_cov\n",
    "    hessian_ps = np.linalg.inv(np.dot(score_ps.T, score_ps) / n)\n",
    "    asy_lin_rep_ps = np.dot(score_ps, hessian_ps)\n",
    "\n",
    "    # Influence function of the \"treat\" component\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "    inf_treat = inf_treat_post - inf_treat_pre\n",
    "\n",
    "    # Influence function of the control component\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Estimation effect from gamma hat (pscore)\n",
    "    M2_pre = np.mean(\n",
    "        w_cont_pre.reshape(-1, 1)\n",
    "        * (y - att_cont_pre).reshape(-1, 1)\n",
    "        * int_cov\n",
    "        / np.mean(w_cont_pre),\n",
    "        axis=0,\n",
    "    )\n",
    "    M2_post = np.mean(\n",
    "        w_cont_post.reshape(-1, 1)\n",
    "        * (y - att_cont_post).reshape(-1, 1)\n",
    "        * int_cov\n",
    "        / np.mean(w_cont_post),\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    inf_cont_ps = np.dot(asy_lin_rep_ps, (M2_post - M2_pre))\n",
    "    inf_cont += inf_cont_ps\n",
    "\n",
    "    # Influence function of the DR estimator\n",
    "    att_inf_func = inf_treat - inf_cont\n",
    "\n",
    "    if not boot:\n",
    "        # Estimate standard error\n",
    "        se_att = np.std(att_inf_func) / np.sqrt(n)\n",
    "        uci = ipw_att + 1.96 * se_att\n",
    "        lci = ipw_att - 1.96 * se_att\n",
    "        ipw_boot = None\n",
    "    else:\n",
    "        if nboot is None:\n",
    "            nboot = 999\n",
    "        if boot_type == \"multiplier\":\n",
    "            # Multiplier bootstrap\n",
    "            multipliers = np.random.normal(size=(nboot, n))\n",
    "            ipw_boot = [np.mean(m * att_inf_func) for m in multipliers]\n",
    "            se_att = iqr(ipw_boot) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.percentile(np.abs(ipw_boot / se_att), 95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "        else:\n",
    "            # Weighted bootstrap\n",
    "            ipw_boot = [\n",
    "                wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights)\n",
    "                for _ in range(nboot)\n",
    "            ]\n",
    "            se_att = iqr(ipw_boot - ipw_att) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.percentile(np.abs((ipw_boot - ipw_att) / se_att), 95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "\n",
    "    if not inffunc:\n",
    "        att_inf_func = None\n",
    "\n",
    "    return {\n",
    "        \"ATT\": ipw_att,\n",
    "        \"se\": se_att,\n",
    "        \"uci\": uci,\n",
    "        \"lci\": lci,\n",
    "        \"boots\": ipw_boot,\n",
    "        \"att_inf_func\": att_inf_func,\n",
    "        \"min_training_loss\": min_training_loss,\n",
    "        \"min_validation_loss\": min_validation_loss,\n",
    "    }\n",
    "\n",
    "\n",
    "def wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights):\n",
    "    boot_weights = np.random.choice(np.arange(1, n + 1), size=n, replace=True)\n",
    "    return std_ipw_did_rc(y, post, D, int_cov, i_weights=boot_weights)[\"ATT\"]\n",
    "\n",
    "\n",
    "# Simulation setup\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def ipw_sim_run(dgp_type):\n",
    "    # Define parameters\n",
    "    n = 1000  # Sample size\n",
    "    Xsi_ps = 0.75  # pscore index\n",
    "    _lambda = 0.5  # Proportion in each period\n",
    "\n",
    "    # Define means and standard deviations\n",
    "    mean_z1 = np.exp(0.25 / 2)\n",
    "    sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "    mean_z2 = 10\n",
    "    sd_z2 = 0.54164\n",
    "    mean_z3 = 0.21887\n",
    "    sd_z3 = 0.04453\n",
    "    mean_z4 = 402\n",
    "    sd_z4 = 56.63891\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    ATTE_estimates = []\n",
    "    asymptotic_variance = []\n",
    "    min_training_losses = []\n",
    "    min_validation_losses = []\n",
    "\n",
    "    # Loop for 100 runs\n",
    "    for _i in range(10):\n",
    "        # Generate covariates\n",
    "        x1 = np.random.normal(0, 1, n)\n",
    "        x2 = np.random.normal(0, 1, n)\n",
    "        x3 = np.random.normal(0, 1, n)\n",
    "        x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "        z1 = np.exp(x1 / 2)\n",
    "        z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "        z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "        z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "        z1 = (z1 - mean_z1) / sd_z1\n",
    "        z2 = (z2 - mean_z2) / sd_z2\n",
    "        z3 = (z3 - mean_z3) / sd_z3\n",
    "        z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "        # Propensity score\n",
    "        pi = logistic_cdf(Xsi_ps * (-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4))\n",
    "        d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "        # Generate aux indexes for the potential outcomes\n",
    "        index_lin = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "        index_unobs_het = d * index_lin\n",
    "        index_att = 0\n",
    "        index_trend = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "        # Generate unobserved heterogeneity\n",
    "        v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "        # Generate outcomes at time 0 and time 1\n",
    "        y00 = index_lin + v + np.random.normal(size=n)\n",
    "        y10 = index_lin + v + np.random.normal(size=n)\n",
    "        y01 = index_lin + v + np.random.normal(scale=1, size=n) + index_trend\n",
    "        y11 = (\n",
    "            index_lin + v + np.random.normal(scale=1, size=n) + index_trend + index_att\n",
    "        )\n",
    "\n",
    "        # Generate \"T\"\n",
    "        ti_nt = 0.5\n",
    "        ti_t = 0.5\n",
    "        ti = d * ti_t + (1 - d) * ti_nt\n",
    "        post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "        y = np.where(\n",
    "            d & post,\n",
    "            y11,\n",
    "            np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "        )\n",
    "\n",
    "        # Generate id\n",
    "        id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "        time = np.tile([0, 1], n)\n",
    "\n",
    "        # Put in a long data frame\n",
    "        dta_long = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": id_,\n",
    "                \"time\": time,\n",
    "                \"y\": np.tile(y, 2),\n",
    "                \"post\": np.tile(post.astype(int), 2),\n",
    "                \"d\": np.tile(d.astype(int), 2),\n",
    "                \"x1\": np.tile(z1, 2),\n",
    "                \"x2\": np.tile(z2, 2),\n",
    "                \"x3\": np.tile(z3, 2),\n",
    "                \"x4\": np.tile(z4, 2),\n",
    "            },\n",
    "        )\n",
    "        dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "        dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "        # Run the IPW-DID estimator\n",
    "        covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "        y = dta_long[\"y\"].values\n",
    "        post = dta_long[\"post\"].values\n",
    "        D = dta_long[\"d\"].values\n",
    "\n",
    "        result = std_ipw_did_rc(y, post, D, covariates)\n",
    "\n",
    "        ATTE_estimates.append(result[\"ATT\"])\n",
    "        asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "        min_training_losses.append(result[\"min_training_loss\"])\n",
    "        min_validation_losses.append(result[\"min_validation_loss\"])\n",
    "\n",
    "    # Calculate average bias, median bias, and RMSE\n",
    "    true_ATT = 0\n",
    "\n",
    "    # Bias calculations\n",
    "    biases = np.array(ATTE_estimates) - true_ATT\n",
    "    average_bias = np.mean(biases)\n",
    "    median_bias = np.median(biases)\n",
    "    average_variance = np.mean(asymptotic_variance)\n",
    "    # RMSE calculation\n",
    "    rmse = np.sqrt(np.mean(biases**2))\n",
    "    avg_min_training_loss = np.mean(min_training_losses)\n",
    "    avg_min_validation_loss = np.mean(min_validation_losses)\n",
    "    results = {\n",
    "        \"avg_bias\": average_bias,\n",
    "        \"med_bias\": median_bias,\n",
    "        \"rmse\": rmse,\n",
    "        \"average_variance\": average_variance,\n",
    "        \"average_min_training_loss\": avg_min_training_loss,\n",
    "        \"average_min_validation_loss\": avg_min_validation_loss,\n",
    "    }\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(\"bld/ipw_dl_results\", exist_ok=True)\n",
    "    latex_filename = f\"bld/ipw_dl_results/ipw_dl_{dgp_type}.tex\"\n",
    "\n",
    "    # Writing the results to a LaTeX file\n",
    "    with open(latex_filename, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[ht]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{|l|r|}\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"Metric & Value \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key.replace('_', ' ').title()} & {value:.4f} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\n",
    "            f\"\\\\caption{{Simulation Results for double robust deep learning with DGP Type {dgp_type}}}\\n\",\n",
    "        )\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "    # Save ATTE estimates as a pickle file\n",
    "    os.makedirs(\"bld/ipw_dl_results\", exist_ok=True)\n",
    "    pickle_filename = f\"bld/ipw_dl_results/ipw_dl_atte_estimates_dgp_{dgp_type}.pkl\"\n",
    "    with open(pickle_filename, \"wb\") as f:\n",
    "        pickle.dump(ATTE_estimates, f)\n",
    "\n",
    "    # Display the results\n",
    "    return {\n",
    "        \"Average Bias\": average_bias,\n",
    "        \"Median Bias\": median_bias,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Average Variance of ATT\": average_variance,\n",
    "        \"average_min_training_loss\": avg_min_training_loss,\n",
    "        \"average_min_validation_loss\": avg_min_validation_loss,\n",
    "    }\n",
    "\n",
    "\n",
    "ipw_sim_run(dgp_type=\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP2 - Deep Learning\n",
    "EXPERIMENT 1C: NON-RANDOMIZED EXPERIMENT WITH X-SPECIFIC TRENDS PROPENSITY SCORES CORRECTLY SPECIFIED, OUTCOME REGRESSION NOT CORRECTLY SPECIFIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit as logistic_cdf\n",
    "from scipy.stats import iqr, norm\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def std_ipw_did_rc(\n",
    "    y,\n",
    "    post,\n",
    "    D,\n",
    "    covariates=None,\n",
    "    i_weights=None,\n",
    "    boot=False,\n",
    "    boot_type=\"weighted\",\n",
    "    nboot=None,\n",
    "    inffunc=False,\n",
    "):\n",
    "    # Convert inputs to numpy arrays\n",
    "    D = np.asarray(D).flatten()\n",
    "    n = len(D)\n",
    "    y = np.asarray(y).flatten()\n",
    "    post = np.asarray(post).flatten()\n",
    "\n",
    "    # Add constant to covariate vector\n",
    "    if covariates is None:\n",
    "        int_cov = np.ones((n, 1))\n",
    "    else:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "    elif np.min(i_weights) < 0:\n",
    "        msg = \"i.weights must be non-negative\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    def create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg):\n",
    "        np.random.seed(42)\n",
    "\n",
    "        inputs = Input(shape=(input_dim,))\n",
    "        x = Dense(units, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        for _ in range(depth - 2):\n",
    "            x = Dense(units, kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = ReLU()(x)\n",
    "\n",
    "        outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        # Compile the model with Adam optimizer and specified learning rate\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Assume `int_cov`, `D`, and `i_weights` are already defined as in your original code\n",
    "    depth = 3\n",
    "    units = 32\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.01\n",
    "    input_dim = int_cov.shape[1]\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    (\n",
    "        int_cov_train,\n",
    "        int_cov_val,\n",
    "        D_train,\n",
    "        D_val,\n",
    "        weights_train,\n",
    "        weights_val,\n",
    "    ) = train_test_split(int_cov, D, i_weights, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and compile the model with the optimal hyperparameters\n",
    "    model = create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg)\n",
    "\n",
    "    # Train the model\n",
    "    history = LossHistory()\n",
    "    # Predict the probabilities\n",
    "    ps_fit = model.predict(int_cov).flatten()\n",
    "    model.fit(\n",
    "        int_cov_train,\n",
    "        D_train,\n",
    "        sample_weight=weights_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        validation_data=(int_cov_val, D_val, weights_val),\n",
    "        callbacks=[history],\n",
    "    )\n",
    "    min_training_loss, min_validation_loss = history.get_min_loss()\n",
    "    # Ensure no values are exactly 0 or 1\n",
    "    ps_fit = np.clip(ps_fit, 1e-8, 1 - 1e-8)\n",
    "\n",
    "    # Compute IPW estimator\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * y / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * y / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * y / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * y / np.mean(w_cont_post)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    # ATT estimator\n",
    "    ipw_att = (att_treat_post - att_treat_pre) - (att_cont_post - att_cont_pre)\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    score_ps = i_weights.reshape(-1, 1) * (D - ps_fit).reshape(-1, 1) * int_cov\n",
    "    hessian_ps = np.linalg.inv(np.dot(score_ps.T, score_ps) / n)\n",
    "    asy_lin_rep_ps = np.dot(score_ps, hessian_ps)\n",
    "\n",
    "    # Influence function of the \"treat\" component\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "    inf_treat = inf_treat_post - inf_treat_pre\n",
    "\n",
    "    # Influence function of the control component\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Estimation effect from gamma hat (pscore)\n",
    "    M2_pre = np.mean(\n",
    "        w_cont_pre.reshape(-1, 1)\n",
    "        * (y - att_cont_pre).reshape(-1, 1)\n",
    "        * int_cov\n",
    "        / np.mean(w_cont_pre),\n",
    "        axis=0,\n",
    "    )\n",
    "    M2_post = np.mean(\n",
    "        w_cont_post.reshape(-1, 1)\n",
    "        * (y - att_cont_post).reshape(-1, 1)\n",
    "        * int_cov\n",
    "        / np.mean(w_cont_post),\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    inf_cont_ps = np.dot(asy_lin_rep_ps, (M2_post - M2_pre))\n",
    "    inf_cont += inf_cont_ps\n",
    "\n",
    "    # Influence function of the DR estimator\n",
    "    att_inf_func = inf_treat - inf_cont\n",
    "\n",
    "    if not boot:\n",
    "        # Estimate standard error\n",
    "        se_att = np.std(att_inf_func) / np.sqrt(n)\n",
    "        uci = ipw_att + 1.96 * se_att\n",
    "        lci = ipw_att - 1.96 * se_att\n",
    "        ipw_boot = None\n",
    "    else:\n",
    "        if nboot is None:\n",
    "            nboot = 999\n",
    "        if boot_type == \"multiplier\":\n",
    "            # Multiplier bootstrap\n",
    "            multipliers = np.random.normal(size=(nboot, n))\n",
    "            ipw_boot = [np.mean(m * att_inf_func) for m in multipliers]\n",
    "            se_att = iqr(ipw_boot) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.percentile(np.abs(ipw_boot / se_att), 95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "        else:\n",
    "            # Weighted bootstrap\n",
    "            ipw_boot = [\n",
    "                wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights)\n",
    "                for _ in range(nboot)\n",
    "            ]\n",
    "            se_att = iqr(ipw_boot - ipw_att) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.percentile(np.abs((ipw_boot - ipw_att) / se_att), 95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "\n",
    "    if not inffunc:\n",
    "        att_inf_func = None\n",
    "\n",
    "    return {\n",
    "        \"ATT\": ipw_att,\n",
    "        \"se\": se_att,\n",
    "        \"uci\": uci,\n",
    "        \"lci\": lci,\n",
    "        \"boots\": ipw_boot,\n",
    "        \"att_inf_func\": att_inf_func,\n",
    "        \"min_training_loss\": min_training_loss,\n",
    "        \"min_validation_loss\": min_validation_loss,\n",
    "    }\n",
    "\n",
    "\n",
    "def wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights):\n",
    "    boot_weights = np.random.choice(np.arange(1, n + 1), size=n, replace=True)\n",
    "    return std_ipw_did_rc(y, post, D, int_cov, i_weights=boot_weights)[\"ATT\"]\n",
    "\n",
    "\n",
    "# New Simulation setup\n",
    "\n",
    "\n",
    "def ipw_sim_run(dgp_type):\n",
    "    # Define parameters\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Sample size\n",
    "    n = 1000\n",
    "    # pscore index (strength of common support)\n",
    "    Xsi_ps = 0.75\n",
    "    # Proportion in each period\n",
    "    # Number of bootstrapped draws\n",
    "\n",
    "    # Mean and Std deviation of Z's without truncation\n",
    "    mean_z1 = np.exp(0.25 / 2)\n",
    "    sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "    mean_z2 = 10\n",
    "    sd_z2 = 0.54164\n",
    "    mean_z3 = 0.21887\n",
    "    sd_z3 = 0.04453\n",
    "    mean_z4 = 402\n",
    "    sd_z4 = 56.63891\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    ATTE_estimates = []\n",
    "    asymptotic_variance = []\n",
    "    min_training_losses = []\n",
    "    min_validation_losses = []\n",
    "\n",
    "    for _i in range(10):\n",
    "        # Generate covariates\n",
    "        x1 = np.random.normal(0, 1, n)\n",
    "        x2 = np.random.normal(0, 1, n)\n",
    "        x3 = np.random.normal(0, 1, n)\n",
    "        x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "        z1 = np.exp(x1 / 2)\n",
    "        z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "        z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "        z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "        z1 = (z1 - mean_z1) / sd_z1\n",
    "        z2 = (z2 - mean_z2) / sd_z2\n",
    "        z3 = (z3 - mean_z3) / sd_z3\n",
    "        z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "        np.column_stack((x1, x2, x3, x4))\n",
    "        np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "        # Propensity score\n",
    "        pi = logistic_cdf(Xsi_ps * (-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4))\n",
    "        d = (np.random.uniform(size=n) <= pi).astype(int)\n",
    "\n",
    "        # Generate aux indexes for the potential outcomes\n",
    "        index_lin = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "        # Create heterogeneous effects for the ATT, which is set approximately equal to zero\n",
    "        index_unobs_het = d * index_lin\n",
    "        index_att = 0\n",
    "\n",
    "        # This is the key for consistency of outcome regression\n",
    "        index_trend = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "        # v is the unobserved heterogeneity\n",
    "        v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "        # Gen realized outcome at time 0\n",
    "        y00 = index_lin + v + np.random.normal(size=n)\n",
    "        y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "        # Gen outcomes at time 1\n",
    "        y01 = (\n",
    "            index_lin + v + np.random.normal(size=n) + index_trend\n",
    "        )  # This is the baseline\n",
    "        y11 = (\n",
    "            index_lin + v + np.random.normal(size=n) + index_trend + index_att\n",
    "        )  # This is the baseline\n",
    "\n",
    "        # Generate \"T\"\n",
    "        ti_nt = 0.5\n",
    "        ti_t = 0.5\n",
    "        ti = d * ti_t + (1 - d) * ti_nt\n",
    "        post = (np.random.uniform(size=n) <= ti).astype(int)\n",
    "\n",
    "        y = np.where(\n",
    "            d & post,\n",
    "            y11,\n",
    "            np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "        )\n",
    "\n",
    "        # Gen id\n",
    "        id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "        time = np.tile([0, 1], n)\n",
    "\n",
    "        # Put in a long data frame\n",
    "        dta_long = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": id_,\n",
    "                \"time\": time,\n",
    "                \"y\": np.tile(y, 2),\n",
    "                \"post\": np.tile(post.astype(int), 2),\n",
    "                \"d\": np.tile(d.astype(int), 2),\n",
    "                \"x1\": np.tile(z1, 2),\n",
    "                \"x2\": np.tile(z2, 2),\n",
    "                \"x3\": np.tile(z3, 2),\n",
    "                \"x4\": np.tile(z4, 2),\n",
    "            },\n",
    "        )\n",
    "        dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "        dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "        # Run the IPW-DID estimator\n",
    "        covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "        y = dta_long[\"y\"].values\n",
    "        post = dta_long[\"post\"].values\n",
    "        D = dta_long[\"d\"].values\n",
    "\n",
    "        result = std_ipw_did_rc(y, post, D, covariates)\n",
    "\n",
    "        ATTE_estimates.append(result[\"ATT\"])\n",
    "        asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "        min_training_losses.append(result[\"min_training_loss\"])\n",
    "        min_validation_losses.append(result[\"min_validation_loss\"])\n",
    "\n",
    "    # Calculate average bias, median bias, and RMSE\n",
    "    true_ATT = 0\n",
    "\n",
    "    # Bias calculations\n",
    "    biases = np.array(ATTE_estimates) - true_ATT\n",
    "    average_bias = np.mean(biases)\n",
    "    median_bias = np.median(biases)\n",
    "    average_variance = np.mean(asymptotic_variance)\n",
    "    # RMSE calculation\n",
    "    rmse = np.sqrt(np.mean(biases**2))\n",
    "    avg_min_training_loss = np.mean(min_training_losses)\n",
    "    avg_min_validation_loss = np.mean(min_validation_losses)\n",
    "\n",
    "    results = {\n",
    "        \"avg_bias\": average_bias,\n",
    "        \"med_bias\": median_bias,\n",
    "        \"rmse\": rmse,\n",
    "        \"average_variance\": average_variance,\n",
    "        \"average_min_training_loss\": avg_min_training_loss,\n",
    "        \"average_min_validation_loss\": avg_min_validation_loss,\n",
    "    }\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(\"bld/ipw_dl_results\", exist_ok=True)\n",
    "    latex_filename = f\"bld/ipw_dl_results/ipw_dl_{dgp_type}.tex\"\n",
    "\n",
    "    # Writing the results to a LaTeX file\n",
    "    with open(latex_filename, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[ht]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{|l|r|}\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"Metric & Value \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key.replace('_', ' ').title()} & {value:.4f} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\n",
    "            f\"\\\\caption{{Simulation Results for double robust deep learning with DGP Type {dgp_type}}}\\n\",\n",
    "        )\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "    # Save ATTE estimates as a pickle file\n",
    "    os.makedirs(\"bld/ipw_dl_results\", exist_ok=True)\n",
    "    pickle_filename = f\"bld/ipw_dl_results/ipw_dl_atte_estimates_dgp_{dgp_type}.pkl\"\n",
    "    with open(pickle_filename, \"wb\") as f:\n",
    "        pickle.dump(ATTE_estimates, f)\n",
    "\n",
    "    # Display the results\n",
    "    return {\n",
    "        \"Average Bias\": average_bias,\n",
    "        \"Median Bias\": median_bias,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Average Variance of ATT\": average_variance,\n",
    "        \"average_min_training_loss\": avg_min_training_loss,\n",
    "        \"average_min_validation_loss\": avg_min_validation_loss,\n",
    "    }\n",
    "\n",
    "\n",
    "ipw_sim_run(dgp_type=\"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP3\n",
    "NON-RANDOMIZED EXPERIMENT WITH X-SPECIFIC TRENDS PROPENSITY SCORES NOT CORRECTLY SPECIFIED, OUTCOME REGRESSION CORRECTLY SPECIFIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit as logistic_cdf\n",
    "from scipy.stats import iqr, norm\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def std_ipw_did_rc(\n",
    "    y,\n",
    "    post,\n",
    "    D,\n",
    "    covariates=None,\n",
    "    i_weights=None,\n",
    "    boot=False,\n",
    "    boot_type=\"weighted\",\n",
    "    nboot=None,\n",
    "    inffunc=False,\n",
    "):\n",
    "    # Convert inputs to numpy arrays\n",
    "    D = np.asarray(D).flatten()\n",
    "    n = len(D)\n",
    "    y = np.asarray(y).flatten()\n",
    "    post = np.asarray(post).flatten()\n",
    "\n",
    "    # Add constant to covariate vector\n",
    "    if covariates is None:\n",
    "        int_cov = np.ones((n, 1))\n",
    "    else:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "    elif np.min(i_weights) < 0:\n",
    "        msg = \"i.weights must be non-negative\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    def create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg):\n",
    "        np.random.seed(42)\n",
    "\n",
    "        inputs = Input(shape=(input_dim,))\n",
    "        x = Dense(units, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        for _ in range(depth - 2):\n",
    "            x = Dense(units, kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = ReLU()(x)\n",
    "\n",
    "        outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        # Compile the model with Adam optimizer and specified learning rate\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Assume `int_cov`, `D`, and `i_weights` are already defined as in your original code\n",
    "    depth = 3\n",
    "    units = 32\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.01\n",
    "    input_dim = int_cov.shape[1]\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    (\n",
    "        int_cov_train,\n",
    "        int_cov_val,\n",
    "        D_train,\n",
    "        D_val,\n",
    "        weights_train,\n",
    "        weights_val,\n",
    "    ) = train_test_split(int_cov, D, i_weights, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and compile the model with the optimal hyperparameters\n",
    "    model = create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg)\n",
    "    history = LossHistory()\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        int_cov_train,\n",
    "        D_train,\n",
    "        sample_weight=weights_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        validation_data=(int_cov_val, D_val, weights_val),\n",
    "        callbacks=[history],\n",
    "    )\n",
    "    min_training_loss, min_validation_loss = history.get_min_loss()\n",
    "    # Predict the probabilities\n",
    "    ps_fit = model.predict(int_cov).flatten()\n",
    "\n",
    "    # Ensure no values are exactly 0 or 1\n",
    "    ps_fit = np.clip(ps_fit, 1e-8, 1 - 1e-8)\n",
    "\n",
    "    # Compute IPW estimator\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * y / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * y / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * y / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * y / np.mean(w_cont_post)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    # ATT estimator\n",
    "    ipw_att = (att_treat_post - att_treat_pre) - (att_cont_post - att_cont_pre)\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    score_ps = i_weights.reshape(-1, 1) * (D - ps_fit).reshape(-1, 1) * int_cov\n",
    "    hessian_ps = np.linalg.inv(np.dot(score_ps.T, score_ps) / n)\n",
    "    asy_lin_rep_ps = np.dot(score_ps, hessian_ps)\n",
    "\n",
    "    # Influence function of the \"treat\" component\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "    inf_treat = inf_treat_post - inf_treat_pre\n",
    "\n",
    "    # Influence function of the control component\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Estimation effect from gamma hat (pscore)\n",
    "    M2_pre = np.mean(\n",
    "        w_cont_pre.reshape(-1, 1)\n",
    "        * (y - att_cont_pre).reshape(-1, 1)\n",
    "        * int_cov\n",
    "        / np.mean(w_cont_pre),\n",
    "        axis=0,\n",
    "    )\n",
    "    M2_post = np.mean(\n",
    "        w_cont_post.reshape(-1, 1)\n",
    "        * (y - att_cont_post).reshape(-1, 1)\n",
    "        * int_cov\n",
    "        / np.mean(w_cont_post),\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    inf_cont_ps = np.dot(asy_lin_rep_ps, (M2_post - M2_pre))\n",
    "    inf_cont += inf_cont_ps\n",
    "\n",
    "    # Influence function of the DR estimator\n",
    "    att_inf_func = inf_treat - inf_cont\n",
    "\n",
    "    if not boot:\n",
    "        # Estimate standard error\n",
    "        se_att = np.std(att_inf_func) / np.sqrt(n)\n",
    "        uci = ipw_att + 1.96 * se_att\n",
    "        lci = ipw_att - 1.96 * se_att\n",
    "        ipw_boot = None\n",
    "    else:\n",
    "        if nboot is None:\n",
    "            nboot = 999\n",
    "        if boot_type == \"multiplier\":\n",
    "            # Multiplier bootstrap\n",
    "            multipliers = np.random.normal(size=(nboot, n))\n",
    "            ipw_boot = [np.mean(m * att_inf_func) for m in multipliers]\n",
    "            se_att = iqr(ipw_boot) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.percentile(np.abs(ipw_boot / se_att), 95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "        else:\n",
    "            # Weighted bootstrap\n",
    "            ipw_boot = [\n",
    "                wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights)\n",
    "                for _ in range(nboot)\n",
    "            ]\n",
    "            se_att = iqr(ipw_boot - ipw_att) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.percentile(np.abs((ipw_boot - ipw_att) / se_att), 95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "\n",
    "    if not inffunc:\n",
    "        att_inf_func = None\n",
    "\n",
    "    return {\n",
    "        \"ATT\": ipw_att,\n",
    "        \"se\": se_att,\n",
    "        \"uci\": uci,\n",
    "        \"lci\": lci,\n",
    "        \"boots\": ipw_boot,\n",
    "        \"att_inf_func\": att_inf_func,\n",
    "        \"min_training_loss\": min_training_loss,\n",
    "        \"min_validation_loss\": min_validation_loss,\n",
    "    }\n",
    "\n",
    "\n",
    "def wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights):\n",
    "    boot_weights = np.random.choice(np.arange(1, n + 1), size=n, replace=True)\n",
    "    return std_ipw_did_rc(y, post, D, int_cov, i_weights=boot_weights)[\"ATT\"]\n",
    "\n",
    "\n",
    "# Simulation setup\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def ipw_sim_run(dgp_type):\n",
    "    # Define parameters\n",
    "    np.random.seed(42)  # You can use any integer value as the seed\n",
    "\n",
    "    # Sample size\n",
    "    n = 1000\n",
    "    # pscore index (strength of common support)\n",
    "    Xsi_ps = 0.75\n",
    "    # Proportion in each period\n",
    "    _lambda = 0.5\n",
    "    # Number of bootstrapped draws\n",
    "\n",
    "    # Mean and Std deviation of Z's without truncation\n",
    "    mean_z1 = np.exp(0.25 / 2)\n",
    "    sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "    mean_z2 = 10\n",
    "    sd_z2 = 0.54164\n",
    "    mean_z3 = 0.21887\n",
    "    sd_z3 = 0.04453\n",
    "    mean_z4 = 402\n",
    "    sd_z4 = 56.63891\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    ATTE_estimates = []\n",
    "    asymptotic_variance = []\n",
    "    min_training_losses = []\n",
    "    min_validation_losses = []\n",
    "\n",
    "    for _i in range(10):\n",
    "        # Generate covariates\n",
    "        x1 = np.random.normal(0, 1, n)\n",
    "        x2 = np.random.normal(0, 1, n)\n",
    "        x3 = np.random.normal(0, 1, n)\n",
    "        x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "        z1 = np.exp(x1 / 2)\n",
    "        z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "        z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "        z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "        z1 = (z1 - mean_z1) / sd_z1\n",
    "        z2 = (z2 - mean_z2) / sd_z2\n",
    "        z3 = (z3 - mean_z3) / sd_z3\n",
    "        z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "        np.column_stack((x1, x2, x3, x4))\n",
    "        np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "        # Generate treatment groups\n",
    "        # Propensity score\n",
    "        pi = logistic_cdf(Xsi_ps * (-x1 + 0.5 * x2 - 0.25 * x3 - 0.1 * x4))\n",
    "        d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "        # Generate aux indexes for the potential outcomes\n",
    "        index_lin = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "        # Create heterogenenous effects for the ATT, which is set approximately equal to zero\n",
    "        index_unobs_het = d * (index_lin)\n",
    "        index_att = 0\n",
    "\n",
    "        # This is the key for consistency of outcome regression\n",
    "        index_trend = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "        # v is the unobserved heterogeneity\n",
    "        v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "        # Gen realized outcome at time 0\n",
    "        y00 = index_lin + v + np.random.normal(size=n)\n",
    "        y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "        # Gen outcomes at time 1\n",
    "        # First let's generate potential outcomes: y_1_potential\n",
    "        y01 = (\n",
    "            index_lin + v + np.random.normal(scale=1, size=n) + index_trend\n",
    "        )  # This is the baseline\n",
    "        y11 = (\n",
    "            index_lin + v + np.random.normal(scale=1, size=n) + index_trend + index_att\n",
    "        )  # This is the baseline\n",
    "\n",
    "        # Generate \"T\"\n",
    "        ti_nt = 0.5\n",
    "        ti_t = 0.5\n",
    "        ti = d * ti_t + (1 - d) * ti_nt\n",
    "        post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "        y = np.where(\n",
    "            d & post,\n",
    "            y11,\n",
    "            np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "        )\n",
    "\n",
    "        # Gen id\n",
    "        id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "        time = np.tile([0, 1], n)\n",
    "\n",
    "        # Put in a long data frame\n",
    "        dta_long = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": id_,\n",
    "                \"time\": time,\n",
    "                \"y\": np.tile(y, 2),\n",
    "                \"post\": np.tile(post.astype(int), 2),\n",
    "                \"d\": np.tile(d.astype(int), 2),\n",
    "                \"x1\": np.tile(z1, 2),\n",
    "                \"x2\": np.tile(z2, 2),\n",
    "                \"x3\": np.tile(z3, 2),\n",
    "                \"x4\": np.tile(z4, 2),\n",
    "            },\n",
    "        )\n",
    "        dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "        dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "        # Run the IPW-DID estimator\n",
    "        covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "        y = dta_long[\"y\"].values\n",
    "        post = dta_long[\"post\"].values\n",
    "        D = dta_long[\"d\"].values\n",
    "\n",
    "        result = std_ipw_did_rc(y, post, D, covariates)\n",
    "\n",
    "        ATTE_estimates.append(result[\"ATT\"])\n",
    "        asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "        min_training_losses.append(result[\"min_training_loss\"])\n",
    "        min_validation_losses.append(result[\"min_validation_loss\"])\n",
    "\n",
    "    # Calculate average bias, median bias, and RMSE\n",
    "    true_ATT = 0\n",
    "\n",
    "    # Bias calculations\n",
    "    biases = np.array(ATTE_estimates) - true_ATT\n",
    "    average_bias = np.mean(biases)\n",
    "    median_bias = np.median(biases)\n",
    "    average_variance = np.mean(asymptotic_variance)\n",
    "    # RMSE calculation\n",
    "    rmse = np.sqrt(np.mean(biases**2))\n",
    "    avg_min_training_loss = np.mean(min_training_losses)\n",
    "    avg_min_validation_loss = np.mean(min_validation_losses)\n",
    "\n",
    "    results = {\n",
    "        \"avg_bias\": average_bias,\n",
    "        \"med_bias\": median_bias,\n",
    "        \"rmse\": rmse,\n",
    "        \"average_variance\": average_variance,\n",
    "        \"average_min_training_loss\": avg_min_training_loss,\n",
    "        \"average_min_validation_loss\": avg_min_validation_loss,\n",
    "    }\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(\"bld/ipw_dl_results\", exist_ok=True)\n",
    "    latex_filename = f\"bld/ipw_dl_results/ipw_dl_{dgp_type}.tex\"\n",
    "\n",
    "    # Writing the results to a LaTeX file\n",
    "    with open(latex_filename, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[ht]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{|l|r|}\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"Metric & Value \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key.replace('_', ' ').title()} & {value:.4f} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\n",
    "            f\"\\\\caption{{Simulation Results for double robust deep learning with DGP Type {dgp_type}}}\\n\",\n",
    "        )\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "    # Save ATTE estimates as a pickle file\n",
    "    os.makedirs(\"bld/ipw_dl_results\", exist_ok=True)\n",
    "    pickle_filename = f\"bld/ipw_dl_results/ipw_dl_atte_estimates_dgp_{dgp_type}.pkl\"\n",
    "    with open(pickle_filename, \"wb\") as f:\n",
    "        pickle.dump(ATTE_estimates, f)\n",
    "\n",
    "    # Display the results\n",
    "    return {\n",
    "        \"Average Bias\": average_bias,\n",
    "        \"Median Bias\": median_bias,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Average Variance of ATT\": average_variance,\n",
    "        \"average_min_training_loss\": avg_min_training_loss,\n",
    "        \"average_min_validation_loss\": avg_min_validation_loss,\n",
    "    }\n",
    "\n",
    "\n",
    "ipw_sim_run(dgp_type=\"3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP 4\n",
    "propensity score and outcome regression models are incorrectly specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit as logistic_cdf\n",
    "from scipy.stats import iqr, norm\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def std_ipw_did_rc(\n",
    "    y,\n",
    "    post,\n",
    "    D,\n",
    "    covariates=None,\n",
    "    i_weights=None,\n",
    "    boot=False,\n",
    "    boot_type=\"weighted\",\n",
    "    nboot=None,\n",
    "    inffunc=False,\n",
    "):\n",
    "    # Convert inputs to numpy arrays\n",
    "    D = np.asarray(D).flatten()\n",
    "    n = len(D)\n",
    "    y = np.asarray(y).flatten()\n",
    "    post = np.asarray(post).flatten()\n",
    "\n",
    "    # Add constant to covariate vector\n",
    "    if covariates is None:\n",
    "        int_cov = np.ones((n, 1))\n",
    "    else:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.hstack((np.ones((n, 1)), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "    elif np.min(i_weights) < 0:\n",
    "        msg = \"i.weights must be non-negative\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    def create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg):\n",
    "        np.random.seed(42)\n",
    "\n",
    "        inputs = Input(shape=(input_dim,))\n",
    "        x = Dense(units, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        for _ in range(depth - 2):\n",
    "            x = Dense(units, kernel_regularizer=l2(l2_reg))(x)\n",
    "            x = ReLU()(x)\n",
    "\n",
    "        outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        # Compile the model with Adam optimizer and specified learning rate\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Assume `int_cov`, `D`, and `i_weights` are already defined as in your original code\n",
    "    depth = 3\n",
    "    units = 32\n",
    "    learning_rate = 0.01\n",
    "    l2_reg = 0.01\n",
    "    input_dim = int_cov.shape[1]\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    (\n",
    "        int_cov_train,\n",
    "        int_cov_val,\n",
    "        D_train,\n",
    "        D_val,\n",
    "        weights_train,\n",
    "        weights_val,\n",
    "    ) = train_test_split(int_cov, D, i_weights, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and compile the model with the optimal hyperparameters\n",
    "    model = create_deep_ffnn(input_dim, depth, units, learning_rate, l2_reg)\n",
    "    history = LossHistory()\n",
    "\n",
    "    # Train the model\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        int_cov_train,\n",
    "        D_train,\n",
    "        sample_weight=weights_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        validation_data=(int_cov_val, D_val, weights_val),\n",
    "        callbacks=[history],\n",
    "    )\n",
    "    min_training_loss, min_validation_loss = history.get_min_loss()\n",
    "\n",
    "    # Predict the probabilities\n",
    "    ps_fit = model.predict(int_cov).flatten()\n",
    "\n",
    "    # Ensure no values are exactly 0 or 1\n",
    "    ps_fit = np.clip(ps_fit, 1e-8, 1 - 1e-8)\n",
    "\n",
    "    # Compute IPW estimator\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * y / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * y / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * y / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * y / np.mean(w_cont_post)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    # ATT estimator\n",
    "    ipw_att = (att_treat_post - att_treat_pre) - (att_cont_post - att_cont_pre)\n",
    "\n",
    "    # Get the influence function to compute standard error\n",
    "    score_ps = i_weights.reshape(-1, 1) * (D - ps_fit).reshape(-1, 1) * int_cov\n",
    "    hessian_ps = np.linalg.inv(np.dot(score_ps.T, score_ps) / n)\n",
    "    asy_lin_rep_ps = np.dot(score_ps, hessian_ps)\n",
    "\n",
    "    # Influence function of the \"treat\" component\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "    inf_treat = inf_treat_post - inf_treat_pre\n",
    "\n",
    "    # Influence function of the control component\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Estimation effect from gamma hat (pscore)\n",
    "    M2_pre = np.mean(\n",
    "        w_cont_pre.reshape(-1, 1)\n",
    "        * (y - att_cont_pre).reshape(-1, 1)\n",
    "        * int_cov\n",
    "        / np.mean(w_cont_pre),\n",
    "        axis=0,\n",
    "    )\n",
    "    M2_post = np.mean(\n",
    "        w_cont_post.reshape(-1, 1)\n",
    "        * (y - att_cont_post).reshape(-1, 1)\n",
    "        * int_cov\n",
    "        / np.mean(w_cont_post),\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    inf_cont_ps = np.dot(asy_lin_rep_ps, (M2_post - M2_pre))\n",
    "    inf_cont += inf_cont_ps\n",
    "\n",
    "    # Influence function of the DR estimator\n",
    "    att_inf_func = inf_treat - inf_cont\n",
    "\n",
    "    if not boot:\n",
    "        # Estimate standard error\n",
    "        se_att = np.std(att_inf_func) / np.sqrt(n)\n",
    "        uci = ipw_att + 1.96 * se_att\n",
    "        lci = ipw_att - 1.96 * se_att\n",
    "        ipw_boot = None\n",
    "    else:\n",
    "        if nboot is None:\n",
    "            nboot = 999\n",
    "        if boot_type == \"multiplier\":\n",
    "            # Multiplier bootstrap\n",
    "            multipliers = np.random.normal(size=(nboot, n))\n",
    "            ipw_boot = [np.mean(m * att_inf_func) for m in multipliers]\n",
    "            se_att = iqr(ipw_boot) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.percentile(np.abs(ipw_boot / se_att), 95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "        else:\n",
    "            # Weighted bootstrap\n",
    "            ipw_boot = [\n",
    "                wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights)\n",
    "                for _ in range(nboot)\n",
    "            ]\n",
    "            se_att = iqr(ipw_boot - ipw_att) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.percentile(np.abs((ipw_boot - ipw_att) / se_att), 95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "\n",
    "    if not inffunc:\n",
    "        att_inf_func = None\n",
    "\n",
    "    return {\n",
    "        \"ATT\": ipw_att,\n",
    "        \"se\": se_att,\n",
    "        \"uci\": uci,\n",
    "        \"lci\": lci,\n",
    "        \"boots\": ipw_boot,\n",
    "        \"att_inf_func\": att_inf_func,\n",
    "        \"min_training_loss\": min_training_loss,\n",
    "        \"min_validation_loss\": min_validation_loss,\n",
    "    }\n",
    "\n",
    "\n",
    "def wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights):\n",
    "    boot_weights = np.random.choice(np.arange(1, n + 1), size=n, replace=True)\n",
    "    return std_ipw_did_rc(y, post, D, int_cov, i_weights=boot_weights)[\"ATT\"]\n",
    "\n",
    "\n",
    "# Simulation setup\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def ipw_sim_run(dgp_type):\n",
    "    # Define parameters\n",
    "    np.random.seed(42)  # You can use any integer value as the seed\n",
    "    # Sample size\n",
    "    n = 1000\n",
    "\n",
    "    # pscore index (strength of common support)\n",
    "    Xsi_ps = 0.75\n",
    "\n",
    "    # Proportion in each period\n",
    "\n",
    "    # Number of bootstrapped draws\n",
    "\n",
    "    # Mean and Std deviation of Z's without truncation\n",
    "    mean_z1 = np.exp(0.25 / 2)\n",
    "    sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "    mean_z2 = 10\n",
    "    sd_z2 = 0.54164\n",
    "    mean_z3 = 0.21887\n",
    "    sd_z3 = 0.04453\n",
    "    mean_z4 = 402\n",
    "    sd_z4 = 56.63891\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    ATTE_estimates = []\n",
    "    asymptotic_variance = []\n",
    "    min_training_losses = []\n",
    "    min_validation_losses = []\n",
    "    for _i in range(10):\n",
    "        # Gen covariates\n",
    "        x1 = np.random.normal(0, 1, n)\n",
    "        x2 = np.random.normal(0, 1, n)\n",
    "        x3 = np.random.normal(0, 1, n)\n",
    "        x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "        z1 = np.exp(x1 / 2)\n",
    "        z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "        z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "        z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "        z1 = (z1 - mean_z1) / sd_z1\n",
    "        z2 = (z2 - mean_z2) / sd_z2\n",
    "        z3 = (z3 - mean_z3) / sd_z3\n",
    "        z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "        np.column_stack((x1, x2, x3, x4))\n",
    "        np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "        # Gen treatment groups\n",
    "        # Propensity score\n",
    "        pi = 1 / (1 + np.exp(-Xsi_ps * (-x1 + 0.5 * x2 - 0.25 * x3 - 0.1 * x4)))\n",
    "        d = np.random.rand(n) <= pi\n",
    "\n",
    "        # Generate aux indexes for the potential outcomes\n",
    "        index_lin = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "        # Create heterogeneous effects for the ATT, which is set approximately equal to zero\n",
    "        index_unobs_het = d * index_lin\n",
    "        index_att = 0\n",
    "\n",
    "        # This is the key for consistency of outcome regression\n",
    "        index_trend = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "        # v is the unobserved heterogeneity\n",
    "        v = np.random.normal(index_unobs_het, 1, n)\n",
    "\n",
    "        # Gen realized outcome at time 0\n",
    "        y00 = index_lin + v + np.random.normal(size=n)\n",
    "        y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "        # Gen outcomes at time 1\n",
    "        # First let's generate potential outcomes: y_1_potential\n",
    "        y01 = index_lin + v + np.random.normal(size=n) + index_trend\n",
    "        y11 = index_lin + v + np.random.normal(size=n) + index_trend + index_att\n",
    "\n",
    "        # Generate \"T\"\n",
    "        ti_nt = 0.5\n",
    "        ti_t = 0.5\n",
    "        ti = d * ti_t + (1 - d) * ti_nt\n",
    "        post = np.random.rand(n) <= ti\n",
    "\n",
    "        # Combine outcomes into panel data format\n",
    "        y = np.where(\n",
    "            d & post,\n",
    "            y11,\n",
    "            np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "        )\n",
    "\n",
    "        # Gen id\n",
    "        id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "        time = np.tile([0, 1], n)\n",
    "\n",
    "        # Put in a long data frame\n",
    "        dta_long = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": id_,\n",
    "                \"time\": time,\n",
    "                \"y\": np.tile(y, 2),\n",
    "                \"post\": np.tile(post.astype(int), 2),\n",
    "                \"d\": np.tile(d.astype(int), 2),\n",
    "                \"x1\": np.tile(z1, 2),\n",
    "                \"x2\": np.tile(z2, 2),\n",
    "                \"x3\": np.tile(z3, 2),\n",
    "                \"x4\": np.tile(z4, 2),\n",
    "            },\n",
    "        )\n",
    "        dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "        dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "        # Run the IPW-DID estimator\n",
    "        covariates = dta_long[[\"x1\", \"x2\", \"x3\", \"x4\"]].values\n",
    "        y = dta_long[\"y\"].values\n",
    "        post = dta_long[\"post\"].values\n",
    "        D = dta_long[\"d\"].values\n",
    "\n",
    "        result = std_ipw_did_rc(y, post, D, covariates)\n",
    "\n",
    "        ATTE_estimates.append(result[\"ATT\"])\n",
    "        asymptotic_variance.append(result[\"se\"] ** 2)\n",
    "        min_training_losses.append(result[\"min_training_loss\"])\n",
    "        min_validation_losses.append(result[\"min_validation_loss\"])\n",
    "    # Calculate average bias, median bias, and RMSE\n",
    "    true_ATT = 0\n",
    "\n",
    "    # Bias calculations\n",
    "    biases = np.array(ATTE_estimates) - true_ATT\n",
    "    average_bias = np.mean(biases)\n",
    "    median_bias = np.median(biases)\n",
    "    average_variance = np.mean(asymptotic_variance)\n",
    "    # RMSE calculation\n",
    "    rmse = np.sqrt(np.mean(biases**2))\n",
    "    avg_min_training_loss = np.mean(min_training_losses)\n",
    "    avg_min_validation_loss = np.mean(min_validation_losses)\n",
    "    results = {\n",
    "        \"avg_bias\": average_bias,\n",
    "        \"med_bias\": median_bias,\n",
    "        \"rmse\": rmse,\n",
    "        \"average_variance\": average_variance,\n",
    "        \"average_min_training_loss\": avg_min_training_loss,\n",
    "        \"average_min_validation_loss\": avg_min_validation_loss,\n",
    "    }\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(\"bld/ipw_dl_results\", exist_ok=True)\n",
    "    latex_filename = f\"bld/ipw_dl_results/ipw_dl_{dgp_type}.tex\"\n",
    "\n",
    "    # Writing the results to a LaTeX file\n",
    "    with open(latex_filename, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[ht]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{|l|r|}\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"Metric & Value \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key.replace('_', ' ').title()} & {value:.4f} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\n",
    "            f\"\\\\caption{{Simulation Results for double robust deep learning with DGP Type {dgp_type}}}\\n\",\n",
    "        )\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "    # Save ATTE estimates as a pickle file\n",
    "    os.makedirs(\"bld/ipw_dl_results\", exist_ok=True)\n",
    "    pickle_filename = f\"bld/ipw_dl_results/ipw_dl_atte_estimates_dgp_{dgp_type}.pkl\"\n",
    "    with open(pickle_filename, \"wb\") as f:\n",
    "        pickle.dump(ATTE_estimates, f)\n",
    "\n",
    "    # Display the results\n",
    "    return {\n",
    "        \"Average Bias\": average_bias,\n",
    "        \"Median Bias\": median_bias,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Average Variance of ATT\": average_variance,\n",
    "        \"average_min_training_loss\": avg_min_training_loss,\n",
    "        \"average_min_validation_loss\": avg_min_validation_loss,\n",
    "    }\n",
    "\n",
    "\n",
    "ipw_sim_run(dgp_type=\"4\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
